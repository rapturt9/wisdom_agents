{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rapturt9/wisdom_agents/blob/sinem/run_single_agents_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "id": "a0da890d"
   },
   "outputs": [],
   "source": [
    "# ARE YOU IN COLAB?\n",
    "in_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "id": "35561f9a"
   },
   "outputs": [],
   "source": [
    "# Core Variables are now primarily in helpers.py\n",
    "# These can be overridden here if needed for specific notebook runs\n",
    "TEMP_OVERRIDE = 1\n",
    "MODELS_OVERRIDE = [\"openai/gpt-4o-mini\", \"anthropic/claude-3.5-haiku\", \"mistralai/mistral-7b-instruct\"] # Example override"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "3eb1e34f"
   },
   "source": [
    "# 1. API Definitions/Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "d2202351"
   },
   "outputs": [],
   "source": [
    "if in_colab:\n",
    "    !pip install -U \"autogen-agentchat\" \"autogen-ext[openai,azure]\"\n",
    "    !pip install python-dotenv\n",
    "    # If helpers.py and GreatestGoodBenchmark.json are not in the root, upload them or clone the repo\n",
    "    # Example: !git clone <your_repo_url> and then adjust sys.path if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "id": "c79e6ca3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "import sys\n",
    "\n",
    "# Add the parent directory to sys.path to find the helpers module if running from a subdirectory\n",
    "# Or ensure helpers.py is in the same directory or PYTHONPATH\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir))) # Adjust if your structure differs\n",
    "try:\n",
    "    from helpers import Qs, get_prompt as common_get_prompt, models as common_models, TEMP as common_TEMP, get_client as common_get_client\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from helpers: {e}\")\n",
    "    print(\"Please ensure helpers.py is accessible and all its dependencies are installed.\")\n",
    "    # Fallback or raise error if helpers are critical\n",
    "    # For now, we'll define a placeholder Qs if import fails, but this will cause issues later.\n",
    "    if 'Qs' not in globals(): Qs = None \n",
    "    if 'common_get_prompt' not in globals(): common_get_prompt = lambda: \"\" \n",
    "    if 'common_models' not in globals(): common_models = []\n",
    "    if 'common_TEMP' not in globals(): common_TEMP = 1\n",
    "    if 'common_get_client' not in globals(): common_get_client = lambda x: None\n",
    "\n",
    "# Use overridden values if they exist, otherwise use values from helpers\n",
    "TEMP = TEMP_OVERRIDE if 'TEMP_OVERRIDE' in globals() and TEMP_OVERRIDE is not None else common_TEMP\n",
    "models = MODELS_OVERRIDE if 'MODELS_OVERRIDE' in globals() and MODELS_OVERRIDE else common_models\n",
    "get_prompt = common_get_prompt # Using the one from helpers\n",
    "get_client = common_get_client # Using the one from helpers\n",
    "\n",
    "from openai import OpenAI # OpenAI might still be needed directly or is handled by get_client\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "# from autogen_ext.models.openai import OpenAIChatCompletionClient # This is now in helpers.get_client\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get(\"OPENROUTER_API_KEY\") # Ensure API_KEY is loaded for get_client in helpers\n",
    "if not API_KEY and not in_colab: # In Colab, key might be loaded differently by get_client\n",
    "    print(\"Warning: OPENROUTER_API_KEY not found in .env file for local execution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2e4fa6fe",
    "outputId": "1bb48010-f6f2-4fc0-906c-69e2774e1ba8"
   },
   "outputs": [],
   "source": [
    "# Question_Handler class is removed as we are using Qs from helpers.py\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "4oA-134-1VWL"
   },
   "outputs": [],
   "source": [
    "# @title: code for writing files and saving checkpoints\n",
    "import os\n",
    "import csv\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def get_consistent_filenames(model_name, question_range, num_runs):\n",
    "    \"\"\"Generates consistent base filename and full paths for csv, log, and checkpoint files.\"\"\"\n",
    "    safe_model_name = model_name.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "    q_start, q_end = question_range\n",
    "    base_filename = f\"single_{safe_model_name}_q{q_start}-{q_end}_n{num_runs}\"\n",
    "\n",
    "    csv_dir = 'results'\n",
    "    log_dir = 'logs'\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    csv_file = os.path.join(csv_dir, f\"{base_filename}.csv\")\n",
    "    log_file = os.path.join(log_dir, f\"{base_filename}.log\")\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, f\"{base_filename}_checkpoint.json\")\n",
    "\n",
    "    return csv_file, log_file, checkpoint_file\n",
    "\n",
    "\n",
    "def save_checkpoint(checkpoint_file, completed_runs):\n",
    "    \"\"\"Save the current progress to the specified checkpoint file.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(completed_runs, f, indent=4)\n",
    "        # print(f\"Checkpoint saved to {checkpoint_file}\") # Can be verbose\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint to {checkpoint_file}: {e}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file):\n",
    "    \"\"\"Load progress from a checkpoint file.\"\"\"\n",
    "    if not os.path.exists(checkpoint_file):\n",
    "        print(f\"Checkpoint file {checkpoint_file} not found. Starting fresh.\")\n",
    "        return {}\n",
    "    try:\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            completed_runs = json.load(f)\n",
    "        print(f\"Loaded checkpoint from {checkpoint_file}\")\n",
    "        # Optional: Add more detail about loaded data if needed\n",
    "        # Example: print(f\"... found {len(completed_runs.get(list(completed_runs.keys())[0], {}))} completed questions for the first model.\")\n",
    "        return completed_runs\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from checkpoint file {checkpoint_file}. Starting fresh.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint {checkpoint_file}: {e}. Starting fresh.\")\n",
    "        return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "id": "RtfZCW5trr3L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# OpenAI import might be redundant if get_client handles it all\n",
    "import json\n",
    "import collections\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "# OpenAIChatCompletionClient is now handled by get_client from helpers\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "import logging # Added for logger setup in run_single_agent_and_save\n",
    "\n",
    "def extract_answer_from_response(content):\n",
    "    # Extract the answer from the response. Adapt this to your exact response structure.\n",
    "    start_index = content.find(\"<ANSWER>\")\n",
    "    end_index = content.find(\"</ANSWER>\")\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        return content[start_index + len(\"<ANSWER>\"):end_index].strip()\n",
    "    return \"No answer found in the agent's response.\"\n",
    "\n",
    "def extract_confidence_from_response(content):\n",
    "  start_index = content.find(\"<CONF>\")\n",
    "  end_index = content.find(\"</CONF>\")\n",
    "  if start_index != -1 and end_index != -1:\n",
    "    return content[start_index + len(\"<CONF>\"):end_index].strip()\n",
    "  return \"No confidence found in the agent's response.\"\n",
    "\n",
    "class Single_Agent_Handler():\n",
    "  def __init__(self, model_name:str, ggb_question_handler, prompt_template = None): # Renamed to ggb_question_handler\n",
    "    self.model_name = model_name\n",
    "    self.ggb_questions = ggb_question_handler # Using GGB_Statements instance\n",
    "    self.client = get_client(model_name) # get_client is from helpers\n",
    "    if prompt_template is None:\n",
    "      self.prompt = get_prompt(group_chat=False) # get_prompt is from helpers\n",
    "    else:\n",
    "      self.prompt = prompt_template\n",
    "\n",
    "  async def run_single_agent_single_question(self, question_number=1): # question_number is 1-based\n",
    "    # returns full response (content of message), answer, confidence, question_id\n",
    "    question_data = self.ggb_questions.get_question_by_index(question_number - 1) # 0-based index\n",
    "\n",
    "    if question_data is None or 'statement' not in question_data or 'statement_id' not in question_data:\n",
    "      print(f\"Question data for index {question_number-1} (number {question_number}) not found or malformed!\")\n",
    "      return None, None, None, None\n",
    "    question_text = question_data['statement']\n",
    "    question_id = question_data['statement_id'] # This is the GGB statement_id\n",
    "\n",
    "    agent = AssistantAgent(\n",
    "        name=\"assistant_agent\",\n",
    "        model_client=self.client,\n",
    "        system_message=self.prompt\n",
    "    )\n",
    "\n",
    "    team = RoundRobinGroupChat([agent], termination_condition=MaxMessageTermination(2))\n",
    "    result = await Console(team.run_stream(task=question_text))\n",
    "\n",
    "    response_content = result.messages[-1].content\n",
    "    answer = extract_answer_from_response(response_content)\n",
    "    confidence = extract_confidence_from_response(response_content)\n",
    "\n",
    "    return answer, confidence, response_content, question_id\n",
    "\n",
    "  async def run_single_agent_multiple_times(self, question_number=1, num_runs=10):\n",
    "    results = []\n",
    "    for _ in range(num_runs):\n",
    "        run_output = await self.run_single_agent_single_question(question_number)\n",
    "        if run_output and run_output[0] is not None: # Check if answer is not None\n",
    "            results.append(run_output) # (answer, confidence, response_content, question_id)\n",
    "        else:\n",
    "            print(f\"Task returned None or malformed data for question {question_number}\")\n",
    "            # Append a placeholder if necessary, or handle error\n",
    "            results.append((None, None, None, self.ggb_questions.get_question_by_index(question_number - 1).get('statement_id', 'unknown_id_error')))\n",
    "\n",
    "    answers = [res[0] for res in results]\n",
    "    confidences = [res[1] for res in results]\n",
    "    responses = [res[2] for res in results]\n",
    "    question_ids = [res[3] for res in results] # All should be the same for a given question_number\n",
    "\n",
    "    return answers, confidences, responses, question_ids[0] if question_ids else None\n",
    "\n",
    "  async def run_single_agent_and_save(self, question_range=(1, 88), num_runs=1):\n",
    "    model_name = self.model_name\n",
    "    q_start, q_end = question_range\n",
    "    csv_file, log_file, checkpoint_file = get_consistent_filenames(model_name, question_range, num_runs)\n",
    "    completed_runs = load_checkpoint(checkpoint_file)\n",
    "    all_results_this_session = []\n",
    "    question_numbers_to_process = list(range(q_start, q_end + 1))\n",
    "\n",
    "    logger_name = os.path.basename(log_file).replace('.log', '')\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    if not logger.handlers:\n",
    "        logger.setLevel(logging.INFO)\n",
    "        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')\n",
    "        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "\n",
    "    print(f\"Starting/Resuming run for model {model_name} using GGB questions\")\n",
    "    logger.info(f\"--- Starting/Resuming Run (GGB) --- Model: {model_name}, Questions: {question_range}, Runs: {num_runs} ---\")\n",
    "\n",
    "    model_checkpoint_key = str(model_name) \n",
    "    if model_checkpoint_key not in completed_runs:\n",
    "        completed_runs[model_checkpoint_key] = {}\n",
    "\n",
    "    for question_num in question_numbers_to_process:\n",
    "        q_checkpoint_key = str(question_num)\n",
    "        if completed_runs[model_checkpoint_key].get(q_checkpoint_key, False):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            print(f\"Processing GGB question number {question_num} (index {question_num-1})...\")\n",
    "            logger.info(f\"Processing GGB question number {question_num} (index {question_num-1})\")\n",
    "\n",
    "            # Fetch GGB question_data to log statement_id and text\n",
    "            question_data = self.ggb_questions.get_question_by_index(question_num - 1)\n",
    "            if not question_data or 'statement_id' not in question_data:\n",
    "                logger.warning(f\"GGB Question for index {question_num-1} not found or malformed! Skipping.\")\n",
    "                continue\n",
    "            current_question_id = question_data['statement_id'] # This is GGB statement_id\n",
    "            logger.info(f\"GGB Stmt ID: {current_question_id}, Text: {question_data['statement'][:100]}...\")\n",
    "\n",
    "            answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(\n",
    "                question_number=question_num,\n",
    "                num_runs=num_runs\n",
    "            )\n",
    "            if q_id_from_run != current_question_id and q_id_from_run is not None:\n",
    "                 logger.warning(f\"Mismatch in question ID for Q_num {question_num}. Expected {current_question_id}, got {q_id_from_run}\")\n",
    "            # Use current_question_id as the definitive ID for this loop iteration\n",
    "\n",
    "            question_results_for_csv = []\n",
    "            for i in range(len(answers)):\n",
    "                result_obj = {\n",
    "                    \"model_name\": model_name,\n",
    "                    \"question_num\": question_num, # This is the sequential number from range\n",
    "                    \"question_id\": current_question_id, # This is GGB statement_id\n",
    "                    \"run_index\": i + 1,\n",
    "                    \"answer\": answers[i],\n",
    "                    \"confidence\": confidences[i],\n",
    "                    \"full_response\": responses[i]\n",
    "                }\n",
    "                question_results_for_csv.append(result_obj)\n",
    "\n",
    "            self._write_to_csv(question_results_for_csv, csv_file)\n",
    "            all_results_this_session.extend(question_results_for_csv)\n",
    "            completed_runs[model_checkpoint_key][q_checkpoint_key] = True\n",
    "            save_checkpoint(checkpoint_file, completed_runs)\n",
    "            print(f\"  GGB Question number {question_num} (Stmt ID: {current_question_id}) completed and saved.\")\n",
    "            logger.info(f\"GGB Question number {question_num} (Stmt ID: {current_question_id}) completed.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing GGB question number {question_num}: {str(e)}\")\n",
    "            logger.error(f\"Error processing GGB question number {question_num}: {str(e)}\", exc_info=True)\n",
    "\n",
    "    processed_count = len(all_results_this_session)\n",
    "    print(f\"Run finished for model {model_name}. Added {processed_count} new GGB results this session.\")\n",
    "    logger.info(f\"--- Run Finished (GGB) --- Model: {model_name}. Added {processed_count} new results. ---\")\n",
    "    return all_results_this_session, csv_file, log_file\n",
    "\n",
    "  def _write_to_csv(self, results, csv_file):\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    is_empty = not file_exists or os.path.getsize(csv_file) == 0\n",
    "    os.makedirs(os.path.dirname(csv_file) if os.path.dirname(csv_file) else '.', exist_ok=True)\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        if results:\n",
    "            # Ensure question_id is part of fieldnames\n",
    "            fieldnames = ['model_name', 'question_num', 'question_id', 'run_index', 'answer', 'confidence', 'full_response']\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "            if is_empty:\n",
    "                writer.writeheader()\n",
    "            writer.writerows(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "id": "ESauHUjlN2IK"
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "QUESTION_RANGE = (1, Qs.get_total_questions() if Qs else 1)  # Use total questions from GGB\n",
    "NUM_RUNS = 10             # Define the number of runs per question\n",
    "# Use models list from helpers, potentially overridden by MODELS_OVERRIDE\n",
    "MODELS_TO_RUN = models[:3] # Select which models to run (e.g., first 3 from resolved 'models' list)\n",
    "\n",
    "# --- Execution Loop ---\n",
    "async def run_all_models():\n",
    "    if Qs is None:\n",
    "        print(\"Error: Qs (GGB_Statements handler from helpers.py) is not initialized. Cannot run.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total GGB questions available: {Qs.get_total_questions()}\")\n",
    "    # Adjust QUESTION_RANGE if it exceeds available questions\n",
    "    global QUESTION_RANGE\n",
    "    if QUESTION_RANGE[1] > Qs.get_total_questions():\n",
    "        print(f\"Warning: Requested upper question range {QUESTION_RANGE[1]} exceeds available GGB questions {Qs.get_total_questions()}.\")\n",
    "        print(f\"Adjusting upper range to {Qs.get_total_questions()}.\")\n",
    "        QUESTION_RANGE = (QUESTION_RANGE[0], Qs.get_total_questions())\n",
    "\n",
    "    for this_model in MODELS_TO_RUN:\n",
    "        print(f\"\\n--- Initializing handler for model: {this_model} with GGB Questions ---\")\n",
    "        # Pass the imported Qs (GGB_Statements instance) to the handler\n",
    "        handler = Single_Agent_Handler(this_model, Qs)\n",
    "\n",
    "        results_session, csv_file_path, log_file_path = await handler.run_single_agent_and_save(\n",
    "            question_range=QUESTION_RANGE,\n",
    "            num_runs=NUM_RUNS\n",
    "        )\n",
    "\n",
    "        print(f\"Run session completed for {this_model}. Results appended to {csv_file_path}\")\n",
    "        print(f\"Full logs appended to {log_file_path}\")\n",
    "\n",
    "        del handler\n",
    "        del results_session\n",
    "        print(f\"--- Finished handler for model: {this_model} ---\\n\")\n",
    "\n",
    "# --- Start Execution ---\n",
    "async def main():\n",
    "    await run_all_models()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Standard way to run asyncio main in a script/notebook\n",
    "    # In Jupyter, top-level await might work, but this is more robust.\n",
    "    if 'get_ipython' in globals() and get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        # Running in Jupyter, ensure nest_asyncio if needed or handle event loop\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        asyncio.run(main())\n",
    "    else:\n",
    "        asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sitewiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and Setup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Define directories\n",
    "RESULTS_DIR = 'results' # Directory containing single-agent CSV results\n",
    "RESULTS_DIR_SINGLE = 'results'\n",
    "RESULTS_DIR_MULTI = 'results_multi'\n",
    "PLOT_DIR = 'plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Define the datasets (categories) to include\n",
    "# These should match the category names returned by Question_Handler\n",
    "INCLUDED_DATASETS = ['MFQ_30', '6_concepts']\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Add MoralBench repo to path to import Question_Handler\n",
    "MORAL_BENCH_REPO_DIR = '../MoralBench_AgentEnsembles' # Adjust if needed\n",
    "moral_bench_path = os.path.abspath(MORAL_BENCH_REPO_DIR)\n",
    "if moral_bench_path not in sys.path:\n",
    "    sys.path.insert(0, moral_bench_path)\n",
    "print(f\"Using MoralBench repository at: {moral_bench_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated mapping based on standard MFT + Liberty\n",
    "dict_map = {\n",
    "   'authority': 'Authority',\n",
    "   'fairness': 'Fairness',\n",
    "   'harm': 'Harm', # Care/Harm\n",
    "   'ingroup': 'Loyalty', # Loyalty/Betrayal\n",
    "   'purity': 'Sanctity', # Sanctity/Degradation\n",
    "   'liberty': 'Liberty'\n",
    "}\n",
    "\n",
    "# Define the order for plotting categories\n",
    "PLOT_CATEGORIES = ['Harm', 'Fairness', 'Loyalty', 'Authority', 'Sanctity', 'Liberty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Question Handler Definition (Copied for self-containment)\n",
    "# Note: Ideally, this would be imported from a shared module.\n",
    "class Question_Handler():\n",
    "  def __init__(self, repo_dir):\n",
    "    self.repo_dir = os.path.abspath(repo_dir) # Use absolute path\n",
    "    self.questions_dir = os.path.join(self.repo_dir, 'questions')\n",
    "    self.answers_dir = os.path.join(self.repo_dir, 'answers')\n",
    "    self.categories = self.list_categories()\n",
    "    self._build_question_map()\n",
    "\n",
    "  def _build_question_map(self):\n",
    "      \"\"\"Builds a map from question number to (category, index).\"\"\"\n",
    "      self.question_map = {}\n",
    "      current_question_num = 1\n",
    "      for category in self.categories:\n",
    "          count = self.get_question_count(category)\n",
    "          for i in range(count):\n",
    "              self.question_map[current_question_num] = {'category': category, 'index': i}\n",
    "              current_question_num += 1\n",
    "      self.total_questions = current_question_num - 1\n",
    "\n",
    "  def get_question_category_and_index(self, question_number):\n",
    "      \"\"\"Gets the category and index for a given question number.\"\"\"\n",
    "      return self.question_map.get(question_number)\n",
    "\n",
    "  def get_question_category(self, question_number):\n",
    "      \"\"\"Gets the category for a given question number.\"\"\"\n",
    "      mapping = self.question_map.get(question_number)\n",
    "      return mapping['category'] if mapping else None\n",
    "\n",
    "  def get_question_count(self, category_folder):\n",
    "      \"\"\"\n",
    "      Get the number of questions in a specific category folder.\n",
    "      \"\"\"\n",
    "      questions_path = os.path.join(self.questions_dir, category_folder)\n",
    "      if not os.path.exists(questions_path):\n",
    "          # print(f\"Warning: Category folder {questions_path} does not exist!\")\n",
    "          return 0\n",
    "      try:\n",
    "          question_files = [f for f in os.listdir(questions_path) if f.endswith('.txt')]\n",
    "          return len(question_files)\n",
    "      except FileNotFoundError:\n",
    "          # print(f\"Warning: Error accessing category folder {questions_path}.\")\n",
    "          return 0\n",
    "\n",
    "  def list_categories(self):\n",
    "      \"\"\"\n",
    "      List all available question categories.\n",
    "      \"\"\"\n",
    "      if not os.path.exists(self.questions_dir):\n",
    "          print(f\"Warning: Questions directory {self.questions_dir} not found!\")\n",
    "          return []\n",
    "      try:\n",
    "          categories = sorted([d for d in os.listdir(self.questions_dir) if os.path.isdir(os.path.join(self.questions_dir, d))])\n",
    "          return categories\n",
    "      except FileNotFoundError:\n",
    "           print(f\"Warning: Error listing categories in {self.questions_dir}.\")\n",
    "           return []\n",
    "\n",
    "  def load_question_answer(self, category_folder, index):\n",
    "      \"\"\"\n",
    "      Load a question and its possible answers using an index.\n",
    "      \"\"\"\n",
    "      questions_path = os.path.join(self.questions_dir, category_folder)\n",
    "      if not os.path.exists(questions_path):\n",
    "          # print(f\"Warning: Category folder {questions_path} does not exist!\")\n",
    "          return None\n",
    "\n",
    "      try:\n",
    "          # Get all question files and sort them\n",
    "          question_files = sorted([f for f in os.listdir(questions_path) if f.endswith('.txt')])\n",
    "\n",
    "          if index < 0 or index >= len(question_files):\n",
    "              # print(f\"Warning: Index {index} is out of range for category {category_folder}! Valid range: 0-{len(question_files)-1}\")\n",
    "              return None\n",
    "\n",
    "          # Get question filename and ID\n",
    "          question_file = question_files[index]\n",
    "          question_id = os.path.splitext(question_file)[0]\n",
    "\n",
    "          # Read question content\n",
    "          question_path = os.path.join(questions_path, question_file)\n",
    "          with open(question_path, 'r', encoding='utf-8') as f:\n",
    "              question_text = f.read()\n",
    "\n",
    "          # Load answers from JSON\n",
    "          answers_path = os.path.join(self.repo_dir, 'answers', f\"{category_folder}.json\") # Corrected path\n",
    "          question_answers = None\n",
    "          if os.path.exists(answers_path):\n",
    "              try:\n",
    "                  with open(answers_path, 'r', encoding='utf-8') as f:\n",
    "                      all_answers = json.load(f)\n",
    "                  question_answers = all_answers.get(question_id, {})\n",
    "              except json.JSONDecodeError:\n",
    "                  print(f\"Warning: Error decoding JSON from {answers_path}\")\n",
    "              except Exception as e:\n",
    "                  print(f\"Warning: Error reading answers file {answers_path}: {e}\")\n",
    "          # else:\n",
    "              # print(f\"Warning: Answers file {answers_path} for {category_folder} does not exist!\")\n",
    "\n",
    "          return {\n",
    "              'question_id': question_id,\n",
    "              'question_text': question_text,\n",
    "              'answers': question_answers\n",
    "          }\n",
    "      except FileNotFoundError:\n",
    "          # print(f\"Warning: Error accessing files in {questions_path}.\")\n",
    "          return None\n",
    "      except Exception as e:\n",
    "          print(f\"Warning: Unexpected error loading question {category_folder}/{index}: {e}\")\n",
    "          return None\n",
    "\n",
    "  def get_question(self, number):\n",
    "      \"\"\"Gets question data by absolute number.\"\"\"\n",
    "      mapping = self.get_question_category_and_index(number)\n",
    "      if mapping:\n",
    "          return self.load_question_answer(mapping['category'], mapping['index'])\n",
    "      else:\n",
    "          # print(f\"Warning: Question number {number} not found in map.\")\n",
    "          return None\n",
    "\n",
    "  def get_total_question_count(self):\n",
    "      \"\"\"Returns the total number of questions across all categories.\"\"\"\n",
    "      return self.total_questions\n",
    "\n",
    "# --- Initialize Question Handler ---\n",
    "try:\n",
    "    Qs = Question_Handler(MORAL_BENCH_REPO_DIR)\n",
    "    print(f\"Question Handler initialized. Found {Qs.get_total_question_count()} questions in {len(Qs.categories)} categories.\")\n",
    "    print(f\"Available categories: {Qs.categories}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Question_Handler: {e}\")\n",
    "    Qs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs.get_question_category_and_index(88)\n",
    "\n",
    "# Qs.load_question_answer('MFQ_30', 88) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure they all have question_id\n",
    "if Qs:\n",
    "    for i in range(1,89):\n",
    "        q_info = Qs.get_question(i)  # Test the Question_Handler\n",
    "        print(f'{q_info.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def extract_category_from_id(question_id):\n",
    "    \"\"\"Extracts the category name from the question_id (e.g., 'fairness_3' -> 'Fairness').\"\"\"\n",
    "    if not isinstance(question_id, str):\n",
    "        return 'Unknown'\n",
    "    match = re.match(r\"([a-zA-Z_]+)_?\\d*\", question_id)\n",
    "    if match:\n",
    "        category_name = match.group(1).replace('_', ' ').title()\n",
    "        # Handle specific known prefixes if needed\n",
    "        if category_name.startswith('Mfq '):\n",
    "             category_name = 'MFQ_30' # Keep original dataset name if preferred\n",
    "        elif category_name.startswith('6 Concepts'):\n",
    "             category_name = '6_concepts' # Keep original dataset name if preferred\n",
    "        return category_name.strip().lower()\n",
    "    return 'Unknown'\n",
    "\n",
    "def get_category_from_qnum(q_num): # this gets out the dataset category (eg MFQ_30)\n",
    "    \"\"\"Gets the category name using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        return Qs.get_question_category(q_num)\n",
    "    return 'Unknown' # Fallback if Qs is not initialized\n",
    "\n",
    "def get_moralbench_scores(question_number, answer):\n",
    "    if Qs:\n",
    "        if answer in Qs.get_question(question_number)['answers']:\n",
    "            return Qs.get_question(question_number)['answers'][answer]\n",
    "    return None # Fallback if Qs is not initialized or answer not found\n",
    "\n",
    "def get_question_id_from_qnum(q_num): # this gets out the question_id (eg fairness_3)\n",
    "    \"\"\"Gets the question ID using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        q_info = Qs.get_question(q_num)\n",
    "        if q_info and 'question_id' in q_info:\n",
    "            return q_info['question_id']\n",
    "    return 'Unknown' # Fallback if Qs is not initialized or question not found  \n",
    "\n",
    "def get_moral_category_from_qnum(q_num): # this gets out the moral category (eg Harm)\n",
    "    \"\"\"Gets the moral category name using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        q_info = Qs.get_question(q_num)\n",
    "        if q_info and 'question_id' in q_info:\n",
    "            return extract_category_from_id(q_info['question_id'])\n",
    "    return 'Unknown' # Fallback if Qs is not initialized or question not found\n",
    "\n",
    "def load_and_preprocess_data(results_dir):\n",
    "    \"\"\"Loads all CSV files from a directory and preprocesses them.\"\"\"\n",
    "    all_data = []\n",
    "    print(f\"Checking directory: {results_dir}\")\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(f\"Warning: Directory not found: {results_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found directory: {results_dir}. Searching for CSV files...\")\n",
    "    found_csv = False\n",
    "    for filename in os.listdir(results_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            found_csv = True\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            print(f\"  Loading file: {filename}\")\n",
    "            try:\n",
    "                df = pd.read_csv(filepath)\n",
    "                if df.empty:\n",
    "                    print(f\"    Warning: File is empty: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # --- Add Category ---\n",
    "                if 'question_id' in df.columns:\n",
    "                    # Use question_id if available (likely multi-agent)\n",
    "                    df['category'] = df['question_id'].apply(extract_category_from_id)\n",
    "                    print(f\"    Extracted categories from 'question_id'. Unique values: {df['category'].unique()[:5]}...\")\n",
    "                elif 'question_num' in df.columns and Qs:\n",
    "                    # Use question_num if question_id is missing (likely single-agent)\n",
    "                    df['category'] = df['question_num'].apply(get_category_from_qnum) # get the dataset category\n",
    "                    print(f\"    Extracted categories from 'question_num'. Unique values: {df['category'].unique()[:5]}...\")\n",
    "                    \n",
    "                    df['question_id'] = df['question_num'].apply(get_question_id_from_qnum) # get the question_id\n",
    "                    print(f\"    Extracted question_id from 'question_num'. Unique values: {df['question_id'].unique()[:5]}...\")                    \n",
    "\n",
    "                    df['moral_category'] = df['question_num'].apply(get_moral_category_from_qnum) # get the moral category\n",
    "                    print(f\"    Extracted moral categories from 'question_num'. Unique values: {df['moral_category'].unique()[:5]}...\")\n",
    "                else:\n",
    "                    df['category'] = 'Unknown' # Fallback\n",
    "                    print(\"    Warning: Could not determine category ('question_id' or 'question_num' missing, or Qs handler failed).\")\n",
    "\n",
    "                # --- Filter by Dataset ---\n",
    "                initial_rows = len(df)\n",
    "                df = df[df['category'].isin(INCLUDED_DATASETS)]\n",
    "                filtered_rows = len(df)\n",
    "                print(f\"    Filtered by INCLUDED_DATASETS ({INCLUDED_DATASETS}). Kept {filtered_rows}/{initial_rows} rows.\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    all_data.append(df)\n",
    "                else:\n",
    "                    print(f\"    Info: No rows remaining after filtering for datasets {INCLUDED_DATASETS}.\")\n",
    "\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"    Warning: Skipping empty file: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading or processing file {filename}: {e}\")\n",
    "\n",
    "    if not found_csv:\n",
    "        print(f\"Warning: No CSV files found in directory: {results_dir}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(f\"No data loaded or retained from {results_dir} after processing and filtering. Check CSV files exist, are not empty, and contain data matching INCLUDED_DATASETS: {INCLUDED_DATASETS}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Concatenating data from {len(all_data)} files/dataframes.\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # --- Data Cleaning ---\n",
    "    # Convert confidence to numeric, coercing errors\n",
    "    if 'extracted_confidence' in combined_df.columns:\n",
    "        combined_df['confidence_numeric'] = pd.to_numeric(combined_df['extracted_confidence'], errors='coerce')\n",
    "    elif 'confidence' in combined_df.columns:\n",
    "         combined_df['confidence_numeric'] = pd.to_numeric(combined_df['confidence'], errors='coerce')\n",
    "    else:\n",
    "        print(\"Warning: No 'confidence' or 'extracted_confidence' column found for numeric conversion.\")\n",
    "        combined_df['confidence_numeric'] = np.nan # Add column as NaN\n",
    "\n",
    "    # Clean up answer strings (remove leading/trailing spaces, periods)\n",
    "    if 'extracted_answer' in combined_df.columns:\n",
    "        combined_df['answer_clean'] = combined_df['extracted_answer'].astype(str).str.strip().str.rstrip('.')\n",
    "    elif 'answer' in combined_df.columns:\n",
    "         combined_df['answer_clean'] = combined_df['answer'].astype(str).str.strip().str.rstrip('.')\n",
    "    else:\n",
    "        print(\"Warning: No 'answer' or 'extracted_answer' column found for cleaning.\")\n",
    "        combined_df['answer_clean'] = 'Unknown'\n",
    "\n",
    "    # Identify run type (single vs multi) based on columns\n",
    "    if 'agent_name' in combined_df.columns and 'message_index' in combined_df.columns:\n",
    "        combined_df['run_type'] = 'multi'\n",
    "    elif 'model_name' in combined_df.columns and 'run_index' in combined_df.columns:\n",
    "         combined_df['run_type'] = 'single'\n",
    "         if Qs and 'question_num' in combined_df.columns:\n",
    "            combined_df['score'] = combined_df.apply(lambda row: get_moralbench_scores(row['question_num'], row['answer']), axis=1)\n",
    "\n",
    "    else:\n",
    "         combined_df['run_type'] = 'unknown'\n",
    "         print(\"Warning: Could not determine run_type based on typical columns.\")\n",
    "\n",
    "    print(f\"Finished loading and preprocessing for {results_dir}. Resulting dataframe shape: {combined_df.shape}\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the category extraction\n",
    "qinfo = Qs.get_question(40)\n",
    "get_category_from_qnum(40)\n",
    "get_moral_category_from_qnum(40)\n",
    "print(f\"Question ID: {qinfo['question_id']}, Category: {get_category_from_qnum(40)}, Moral Category: {get_moral_category_from_qnum(40)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloading\n",
    "single_agent_df = load_and_preprocess_data(RESULTS_DIR_SINGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_agent_df['category'].unique()\n",
    "single_agent_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_agent_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Functions ---\n",
    "\n",
    "def plot_answer_distribution(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of answers (A vs B) across all relevant runs.\"\"\"\n",
    "    if df.empty or 'answer_clean' not in df.columns:\n",
    "        print(\"Cannot plot answer distribution: DataFrame is empty or 'answer_clean' column missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Filter for only 'A' and 'B' answers for clarity\n",
    "    plot_data = df[df['answer_clean'].isin(['A', 'B'])]\n",
    "    if plot_data.empty:\n",
    "        print(\"Cannot plot answer distribution: No 'A' or 'B' answers found after cleaning.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    sns.countplot(data=plot_data, x='answer_clean', order=['A', 'B'])\n",
    "    plt.title('Overall Distribution of Answers (A vs B)')\n",
    "    plt.xlabel('Answer')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved answer distribution plot to {plot_filename}\")\n",
    "\n",
    "def plot_confidence_distribution(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of confidence scores.\"\"\"\n",
    "    if df.empty or 'confidence_numeric' not in df.columns:\n",
    "        print(\"Cannot plot confidence distribution: DataFrame is empty or 'confidence_numeric' column missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Filter out NaN values before plotting\n",
    "    plot_data = df.dropna(subset=['confidence_numeric'])\n",
    "    if plot_data.empty:\n",
    "        print(\"Cannot plot confidence distribution: No valid numeric confidence scores found.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    sns.histplot(data=plot_data, x='confidence_numeric', bins=np.arange(-0.5, 6.5, 1), kde=False)\n",
    "    plt.title('Distribution of Confidence Scores')\n",
    "    plt.xlabel('Confidence Score (0-5)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(6)) # Ensure ticks are 0, 1, 2, 3, 4, 5\n",
    "    plt.xlim(-0.5, 5.5)\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved confidence distribution plot to {plot_filename}\")\n",
    "\n",
    "def plot_answer_by_category(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of answers (A vs B) for each category.\"\"\"\n",
    "    if df.empty or 'answer_clean' not in df.columns or 'category' not in df.columns:\n",
    "        print(\"Cannot plot answer by category: DataFrame empty or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plot_data = df[df['answer_clean'].isin(['A', 'B']) & df['category'].isin(INCLUDED_DATASETS)]\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot answer by category: No 'A' or 'B' answers found for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    category_order = sorted([cat for cat in plot_data['category'].unique() if cat in INCLUDED_DATASETS])\n",
    "    if not category_order:\n",
    "        print(f\"Cannot plot answer by category: No data found for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    sns.countplot(data=plot_data, x='category', hue='answer_clean', order=category_order, hue_order=['A', 'B'])\n",
    "    plt.title('Answer Distribution (A vs B) by Category')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Answer')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved answer by category plot to {plot_filename}\")\n",
    "\n",
    "def plot_confidence_by_category(df, plot_filename):\n",
    "    \"\"\"Plots the average confidence score for each category.\"\"\"\n",
    "    if df.empty or 'confidence_numeric' not in df.columns or 'category' not in df.columns:\n",
    "        print(\"Cannot plot confidence by category: DataFrame empty or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plot_data = df.dropna(subset=['confidence_numeric'])\n",
    "    plot_data = plot_data[plot_data['category'].isin(INCLUDED_DATASETS)]\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot confidence by category: No valid numeric confidence scores found for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    category_order = sorted([cat for cat in plot_data['category'].unique() if cat in INCLUDED_DATASETS])\n",
    "    if not category_order:\n",
    "         print(f\"Cannot plot confidence by category: No data found for included datasets {INCLUDED_DATASETS}.\")\n",
    "         plt.close()\n",
    "         return\n",
    "    sns.barplot(data=plot_data, x='category', y='confidence_numeric', order=category_order, estimator=np.mean, errorbar='sd') # Show mean and std dev\n",
    "    plt.title('Average Confidence Score by Category')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Average Confidence Score (0-5)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 5) # Set y-axis limits\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved confidence by category plot to {plot_filename}\")\n",
    "\n",
    "def plot_answer_by_model(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of answers (A vs B) for each model (single agent runs).\"\"\"\n",
    "    df_single = df[(df['run_type'] == 'single') & (df['category'].isin(INCLUDED_DATASETS))]\n",
    "    if df_single.empty or 'answer_clean' not in df_single.columns or 'model_name' not in df_single.columns:\n",
    "        print(f\"Cannot plot answer by model: No single-agent data for included datasets {INCLUDED_DATASETS} or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plot_data = df_single[df_single['answer_clean'].isin(['A', 'B'])]\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot answer by model: No 'A' or 'B' answers found in single-agent data for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    model_order = sorted(plot_data['model_name'].unique())\n",
    "    sns.countplot(data=plot_data, y='model_name', hue='answer_clean', order=model_order, hue_order=['A', 'B'])\n",
    "    plt.title('Answer Distribution (A vs B) by Model (Single Agent Runs)')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Model Name')\n",
    "    plt.legend(title='Answer')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved answer by model plot to {plot_filename}\")\n",
    "\n",
    "def plot_confidence_by_model(df, plot_filename):\n",
    "    \"\"\"Plots the average confidence score for each model (single agent runs).\"\"\"\n",
    "    df_single = df[(df['run_type'] == 'single') & (df['category'].isin(INCLUDED_DATASETS))]\n",
    "    if df_single.empty or 'confidence_numeric' not in df_single.columns or 'model_name' not in df_single.columns:\n",
    "        print(f\"Cannot plot confidence by model: No single-agent data for included datasets {INCLUDED_DATASETS} or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plot_data = df_single.dropna(subset=['confidence_numeric'])\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot confidence by model: No valid numeric confidence scores found in single-agent data for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    model_order = sorted(plot_data['model_name'].unique())\n",
    "    sns.barplot(data=plot_data, y='model_name', x='confidence_numeric', order=model_order, estimator=np.mean, errorbar='sd')\n",
    "    plt.title('Average Confidence Score by Model (Single Agent Runs)')\n",
    "    plt.xlabel('Average Confidence Score (0-5)')\n",
    "    plt.ylabel('Model Name')\n",
    "    plt.xlim(0, 5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved confidence by model plot to {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# single angent moral bench scores\n",
    "'''\n",
    "For independent random variables, when summing values, the variance (square of standard deviation) adds up. So the correct way to calculate the SEM for the summed scores would be:\n",
    "\n",
    "Calculate nanmean for each question, model, category, and moral_category\n",
    "Calculate SEM for each question's mean\n",
    "Sum the means across questions for each model, category, moral_category\n",
    "Calculate the propagated SEM using the square root of the sum of squared SEMs\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def set_plot_style(title_fontsize=16, label_fontsize=14, tick_fontsize=12, \n",
    "                   legend_fontsize=14, line_width=2.5):\n",
    "    \"\"\"Set global matplotlib parameters for radar plots\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': label_fontsize,\n",
    "        'axes.titlesize': title_fontsize,\n",
    "        'axes.labelsize': label_fontsize,\n",
    "        'xtick.labelsize': tick_fontsize,\n",
    "        'ytick.labelsize': tick_fontsize,\n",
    "        'legend.fontsize': legend_fontsize,\n",
    "        'lines.linewidth': line_width,\n",
    "        'figure.titlesize': title_fontsize + 2\n",
    "    })\n",
    "\n",
    "def plot_moral_radar(df, metric='score', show_sem=False, figsize=(18, 9), save_path=None):\n",
    "    \"\"\"\n",
    "    Create radar plots for moral foundations by model and category.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame with required columns\n",
    "    metric : str, default='score'\n",
    "        Column to plot ('score' or 'confidence')\n",
    "    show_sem : bool, default=False\n",
    "        Whether to show standard error as shaded region\n",
    "    figsize : tuple, default=(18, 9)\n",
    "        Size of the figure (width, height)\n",
    "    save_path : str, optional\n",
    "        Path to save the plots, if None, plots will be displayed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib figure\n",
    "        The created figure with two subplots\n",
    "    \"\"\"\n",
    "    # Dictionary to map moral_category to display names\n",
    "    dict_map = {\n",
    "        'authority': 'Authority',\n",
    "        'fairness': 'Fairness',\n",
    "        'harm': 'Care',  # Care/Harm\n",
    "        'ingroup': 'Loyalty',  # Loyalty/Betrayal\n",
    "        'purity': 'Sanctity',  # Sanctity/Degradation\n",
    "        'liberty': 'Liberty'\n",
    "    }\n",
    "    \n",
    "    # First, calculate mean and SEM for each question, by model, category, moral_category\n",
    "    question_stats = df.groupby(['model_name', 'category', 'moral_category', 'question_num']).agg({\n",
    "        metric: [\n",
    "            ('mean', lambda x: np.nanmean(x)),\n",
    "            ('sem', lambda x: np.nanstd(x, ddof=1) / np.sqrt(np.sum(~np.isnan(x))))\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Flatten the MultiIndex columns\n",
    "    question_stats.columns = ['mean', 'sem']\n",
    "    question_stats = question_stats.reset_index()\n",
    "    \n",
    "    # Now, sum the means across questions for each model, category, moral_category\n",
    "    summed_means = question_stats.groupby(['model_name', 'category', 'moral_category'])['mean'].sum().reset_index()\n",
    "    \n",
    "    # Calculate the propagated SEM for the sums (sqrt of sum of squared SEMs)\n",
    "    summed_sems = question_stats.groupby(['model_name', 'category', 'moral_category'])['sem'].apply(\n",
    "        lambda x: np.sqrt(np.sum(x**2))\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Merge the means and SEMs\n",
    "    final_data = pd.merge(summed_means, summed_sems, on=['model_name', 'category', 'moral_category'])\n",
    "    \n",
    "    # Get unique categories and models\n",
    "    categories = df['category'].unique()\n",
    "    models = df['model_name'].unique()\n",
    "    \n",
    "    # Create a colormap for the models\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(models)))\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, len(categories), figsize=figsize, subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # If there's only one category, make axes an array\n",
    "    if len(categories) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Process each category\n",
    "    for i, category in enumerate(categories):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Filter data for this category\n",
    "        cat_data = final_data[final_data['category'] == category]\n",
    "        \n",
    "        # Get moral categories for this category\n",
    "        moral_cats = sorted(cat_data['moral_category'].unique())\n",
    "        \n",
    "        # Number of moral categories\n",
    "        N = len(moral_cats)\n",
    "        \n",
    "        # Create angle values (in radians)\n",
    "        angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "        \n",
    "        # Make the plot circular by appending the first angle again\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        # Set up the axis\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        \n",
    "        # Map moral_category to display names and set as labels\n",
    "        labels = [dict_map.get(cat, cat) for cat in moral_cats]\n",
    "        ax.set_xticklabels(labels)\n",
    "        \n",
    "        # Set title for the subplot with padding to avoid overlap\n",
    "        ax.set_title(f\"Category: {category}\", pad=20)\n",
    "        \n",
    "        # Find max value for scaling\n",
    "        max_val = cat_data['mean'].max()\n",
    "        if show_sem:\n",
    "            max_val = max(max_val, (cat_data['mean'] + cat_data['sem']).max())\n",
    "            \n",
    "        # Set y-axis limits with some margin\n",
    "        ax.set_ylim(0, max_val * 1.2)\n",
    "        \n",
    "        # Add grid lines with improved labels\n",
    "        rticks = [max_val/5, 2*max_val/5, 3*max_val/5, 4*max_val/5, max_val]\n",
    "        ax.set_rticks(rticks)\n",
    "        # Format tick labels with proper precision\n",
    "        ax.set_yticklabels([f\"{tick:.3f}\" for tick in rticks])\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Plot each model\n",
    "        for j, model in enumerate(models):\n",
    "            # Filter data for this model\n",
    "            model_data = cat_data[cat_data['model_name'] == model]\n",
    "            \n",
    "            if len(model_data) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Create ordered arrays of means and SEMs based on moral_categories\n",
    "            means = []\n",
    "            sems = []\n",
    "            for mc in moral_cats:\n",
    "                model_mc_data = model_data[model_data['moral_category'] == mc]\n",
    "                if len(model_mc_data) > 0:\n",
    "                    means.append(model_mc_data['mean'].values[0])\n",
    "                    sems.append(model_mc_data['sem'].values[0])\n",
    "                else:\n",
    "                    means.append(0)\n",
    "                    sems.append(0)\n",
    "            \n",
    "            # Make means circular for plotting\n",
    "            means_circular = np.append(means, means[0])\n",
    "            \n",
    "            # Plot the mean line\n",
    "            ax.plot(angles, means_circular, color=colors[j], label=model)\n",
    "            \n",
    "            # Add SEM shading if requested\n",
    "            if show_sem:\n",
    "                upper_bound = np.array(means) + np.array(sems)\n",
    "                lower_bound = np.array(means) - np.array(sems)\n",
    "                lower_bound = np.maximum(lower_bound, 0)  # Ensure no negative values\n",
    "                \n",
    "                # Make bounds circular\n",
    "                upper_bound_circular = np.append(upper_bound, upper_bound[0])\n",
    "                lower_bound_circular = np.append(lower_bound, lower_bound[0])\n",
    "                \n",
    "                # Create shaded region\n",
    "                ax.fill_between(angles, lower_bound_circular, upper_bound_circular, \n",
    "                                alpha=0.2, color=colors[j])\n",
    "    \n",
    "    # Add a legend to the figure\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.02), \n",
    "               ncol=len(models))\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "    \n",
    "    # Add more space between subplots\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "# set_plot_style(title_fontsize=16, label_fontsize=14, tick_fontsize=12, \n",
    "#                legend_fontsize=14, line_width=2.5)\n",
    "# fig = plot_moral_radar(df, metric='score', show_sem=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_plot_style(title_fontsize=16, label_fontsize=12, tick_fontsize=16, \n",
    "               legend_fontsize=14, line_width=2.5)\n",
    "fig = plot_moral_radar(single_agent_df, metric='score', show_sem=True, save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_score.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_plot_style(title_fontsize=16, label_fontsize=12, tick_fontsize=16, \n",
    "               legend_fontsize=14, line_width=2.5)\n",
    "fig = plot_moral_radar(single_agent_df, metric='confidence_numeric', show_sem=True, save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_confidence.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "print(\"Loading and preprocessing data...\")\n",
    "df_single = load_and_preprocess_data(RESULTS_DIR_SINGLE)\n",
    "df_multi = load_and_preprocess_data(RESULTS_DIR_MULTI)\n",
    "\n",
    "# Combine data if both exist\n",
    "df_combined = pd.DataFrame() # Initialize empty\n",
    "if not df_single.empty and not df_multi.empty:\n",
    "    print(\"Combining single-agent and multi-agent data.\")\n",
    "    # Ensure columns align before concat, fill missing with NaN or appropriate value\n",
    "    # This is a basic alignment, might need more sophisticated merging based on specific analysis needs\n",
    "    cols = list(set(df_single.columns) | set(df_multi.columns))\n",
    "    df_single = df_single.reindex(columns=cols)\n",
    "    df_multi = df_multi.reindex(columns=cols)\n",
    "    df_combined = pd.concat([df_single, df_multi], ignore_index=True)\n",
    "elif not df_single.empty:\n",
    "    print(\"Using only single-agent data.\")\n",
    "    df_combined = df_single\n",
    "elif not df_multi.empty:\n",
    "    print(\"Using only multi-agent data.\")\n",
    "    df_combined = df_multi\n",
    "else:\n",
    "    print(\"No data loaded from either single-agent or multi-agent results directories.\")\n",
    "    # df_combined remains empty\n",
    "\n",
    "if not df_combined.empty:\n",
    "    print(f\"\\nLoaded {len(df_combined)} total records for analysis.\")\n",
    "    print(f\"Included datasets: {INCLUDED_DATASETS}\")\n",
    "    print(f\"Unique categories found in loaded data: {df_combined['category'].unique()}\")\n",
    "    print(f\"Data shape after combining/filtering: {df_combined.shape}\")\n",
    "    print(f\"Value counts for 'category':\\n{df_combined['category'].value_counts()}\")\n",
    "    print(f\"Value counts for 'run_type':\\n{df_combined['run_type'].value_counts()}\")\n",
    "\n",
    "    # --- Generate Individual Plots ---\n",
    "    print(\"\\nGenerating individual plots...\")\n",
    "    plot_answer_distribution(df_combined, os.path.join(PLOT_DIR, 'overall_answer_distribution.png'))\n",
    "    plot_confidence_distribution(df_combined, os.path.join(PLOT_DIR, 'overall_confidence_distribution.png'))\n",
    "    plot_answer_by_category(df_combined, os.path.join(PLOT_DIR, 'answer_by_category.png'))\n",
    "    plot_confidence_by_category(df_combined, os.path.join(PLOT_DIR, 'confidence_by_category.png'))\n",
    "    plot_answer_by_model(df_combined, os.path.join(PLOT_DIR, 'answer_by_model_single.png'))\n",
    "    plot_confidence_by_model(df_combined, os.path.join(PLOT_DIR, 'confidence_by_model_single.png'))\n",
    "\n",
    "    # --- Generate 3x2 Grid Plot ---\n",
    "    print(\"\\nGenerating combined grid plot...\")\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(18, 24)) # Adjusted figsize\n",
    "    fig.suptitle(f'MoralBench Analysis ({', '.join(INCLUDED_DATASETS)})', fontsize=16, y=1.02) # Add main title\n",
    "\n",
    "    # Plot 1: Overall Answer Distribution\n",
    "    if 'answer_clean' in df_combined.columns:\n",
    "        plot_data_ans = df_combined[df_combined['answer_clean'].isin(['A', 'B'])]\n",
    "        if not plot_data_ans.empty:\n",
    "            sns.countplot(ax=axes[0, 0], data=plot_data_ans, x='answer_clean', order=['A', 'B'])\n",
    "            axes[0, 0].set_title('Overall Answer Distribution (A vs B)')\n",
    "            axes[0, 0].set_xlabel('Answer')\n",
    "            axes[0, 0].set_ylabel('Count')\n",
    "        else:\n",
    "            axes[0, 0].set_title('Overall Answer Distribution (No A/B Data)')\n",
    "    else:\n",
    "         axes[0, 0].set_title('Overall Answer Distribution (No Data)')\n",
    "\n",
    "    # Plot 2: Overall Confidence Distribution\n",
    "    if 'confidence_numeric' in df_combined.columns:\n",
    "        plot_data_conf = df_combined.dropna(subset=['confidence_numeric'])\n",
    "        if not plot_data_conf.empty:\n",
    "            sns.histplot(ax=axes[0, 1], data=plot_data_conf, x='confidence_numeric', bins=np.arange(-0.5, 6.5, 1), kde=False)\n",
    "            axes[0, 1].set_title('Overall Confidence Distribution')\n",
    "            axes[0, 1].set_xlabel('Confidence Score (0-5)')\n",
    "            axes[0, 1].set_ylabel('Count')\n",
    "            axes[0, 1].set_xticks(range(6))\n",
    "            axes[0, 1].set_xlim(-0.5, 5.5)\n",
    "        else:\n",
    "            axes[0, 1].set_title('Overall Confidence Distribution (No Numeric Data)')\n",
    "    else:\n",
    "         axes[0, 1].set_title('Overall Confidence Distribution (No Data)')\n",
    "\n",
    "    # Plot 3: Answer Distribution by Category\n",
    "    if 'answer_clean' in df_combined.columns and 'category' in df_combined.columns:\n",
    "        plot_data_cat_ans = df_combined[df_combined['answer_clean'].isin(['A', 'B']) & df_combined['category'].isin(INCLUDED_DATASETS)]\n",
    "        if not plot_data_cat_ans.empty:\n",
    "            category_order = sorted([cat for cat in plot_data_cat_ans['category'].unique() if cat in INCLUDED_DATASETS])\n",
    "            if category_order:\n",
    "                sns.countplot(ax=axes[1, 0], data=plot_data_cat_ans, x='category', hue='answer_clean', order=category_order, hue_order=['A', 'B'])\n",
    "                axes[1, 0].set_title('Answer Distribution by Category')\n",
    "                axes[1, 0].set_xlabel('Category')\n",
    "                axes[1, 0].set_ylabel('Count')\n",
    "                axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "                axes[1, 0].legend(title='Answer')\n",
    "            else:\n",
    "                axes[1, 0].set_title('Answer Distribution by Category (No Included Dataset Data)')\n",
    "        else:\n",
    "            axes[1, 0].set_title('Answer Distribution by Category (No A/B Data)')\n",
    "    else:\n",
    "         axes[1, 0].set_title('Answer Distribution by Category (No Data)')\n",
    "\n",
    "    # Plot 4: Confidence Distribution by Category\n",
    "    if 'confidence_numeric' in df_combined.columns and 'category' in df_combined.columns:\n",
    "        plot_data_cat_conf = df_combined.dropna(subset=['confidence_numeric'])\n",
    "        plot_data_cat_conf = plot_data_cat_conf[plot_data_cat_conf['category'].isin(INCLUDED_DATASETS)]\n",
    "        if not plot_data_cat_conf.empty:\n",
    "            category_order = sorted([cat for cat in plot_data_cat_conf['category'].unique() if cat in INCLUDED_DATASETS])\n",
    "            if category_order:\n",
    "                sns.barplot(ax=axes[1, 1], data=plot_data_cat_conf, x='category', y='confidence_numeric', order=category_order, estimator=np.mean, errorbar='sd')\n",
    "                axes[1, 1].set_title('Average Confidence by Category')\n",
    "                axes[1, 1].set_xlabel('Category')\n",
    "                axes[1, 1].set_ylabel('Average Confidence Score (0-5)')\n",
    "                axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "                axes[1, 1].set_ylim(0, 5)\n",
    "            else:\n",
    "                axes[1, 1].set_title('Average Confidence by Category (No Included Dataset Data)')\n",
    "        else:\n",
    "            axes[1, 1].set_title('Average Confidence by Category (No Numeric Data)')\n",
    "    else:\n",
    "         axes[1, 1].set_title('Average Confidence by Category (No Data)')\n",
    "\n",
    "    # Plot 5: Answer Distribution by Model (Single Agent)\n",
    "    df_single_plot = df_combined[(df_combined['run_type'] == 'single') & (df_combined['category'].isin(INCLUDED_DATASETS))]\n",
    "    if not df_single_plot.empty and 'answer_clean' in df_single_plot.columns and 'model_name' in df_single_plot.columns:\n",
    "        plot_data_mod_ans = df_single_plot[df_single_plot['answer_clean'].isin(['A', 'B'])]\n",
    "        if not plot_data_mod_ans.empty:\n",
    "            model_order = sorted(plot_data_mod_ans['model_name'].unique())\n",
    "            sns.countplot(ax=axes[2, 0], data=plot_data_mod_ans, y='model_name', hue='answer_clean', order=model_order, hue_order=['A', 'B'])\n",
    "            axes[2, 0].set_title('Answer Distribution by Model (Single)')\n",
    "            axes[2, 0].set_xlabel('Count')\n",
    "            axes[2, 0].set_ylabel('Model Name')\n",
    "            axes[2, 0].legend(title='Answer')\n",
    "        else:\n",
    "            axes[2, 0].set_title('Answer Distribution by Model (No A/B Single Data)')\n",
    "    else:\n",
    "         axes[2, 0].set_title('Answer Distribution by Model (No Single Data)')\n",
    "\n",
    "    # Plot 6: Confidence Distribution by Model (Single Agent)\n",
    "    if not df_single_plot.empty and 'confidence_numeric' in df_single_plot.columns and 'model_name' in df_single_plot.columns:\n",
    "        plot_data_mod_conf = df_single_plot.dropna(subset=['confidence_numeric'])\n",
    "        if not plot_data_mod_conf.empty:\n",
    "            model_order = sorted(plot_data_mod_conf['model_name'].unique())\n",
    "            sns.barplot(ax=axes[2, 1], data=plot_data_mod_conf, y='model_name', x='confidence_numeric', order=model_order, estimator=np.mean, errorbar='sd')\n",
    "            axes[2, 1].set_title('Average Confidence by Model (Single)')\n",
    "            axes[2, 1].set_xlabel('Average Confidence Score (0-5)')\n",
    "            axes[2, 1].set_ylabel('Model Name')\n",
    "            axes[2, 1].set_xlim(0, 5)\n",
    "        else:\n",
    "            axes[2, 1].set_title('Average Confidence by Model (No Numeric Single Data)')\n",
    "    else:\n",
    "         axes[2, 1].set_title('Average Confidence by Model (No Single Data)')\n",
    "\n",
    "    # Adjust layout and save\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust rect to make space for suptitle\n",
    "    grid_plot_filename = os.path.join(PLOT_DIR, 'combined_analysis_grid.png')\n",
    "    plt.savefig(grid_plot_filename)\n",
    "    plt.close(fig) # Close the figure to free memory\n",
    "    print(f\"\\nSaved combined analysis grid plot to {grid_plot_filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo data available for plotting after loading and filtering.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

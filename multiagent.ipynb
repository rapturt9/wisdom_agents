{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Multi-Agent Experiment Runner\n",
    "\n",
    "This notebook contains the code for running various multi-agent chat configurations (Round Robin, Star, Ring Convergence) using the **GreatestGoodBenchmark** questions via `helpers.Qs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Variables are now primarily in helpers.py\n",
    "# These can be overridden here if needed for specific notebook runs\n",
    "TEMP_OVERRIDE = 1\n",
    "MODELS_OVERRIDE = [\"openai/gpt-4o-mini\", \"anthropic/claude-3.5-haiku\", \"mistralai/mixtral-8x7b-instruct\"] # Example override"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. API Definitions/Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U \"autogen-agentchat\" \"autogen-ext[openai,azure]\"\n",
    "# !pip install python-dotenv matplotlib numpy pandas seaborn\n",
    "# install for colab or local if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import collections\n",
    "import sys\n",
    "import re # Added re\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir))) # Adjust if your structure differs\n",
    "try:\n",
    "    from helpers import Qs, get_prompt as common_get_prompt, models as common_models, TEMP as common_TEMP, get_client as common_get_client\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing from helpers: {e}\")\n",
    "    print(\"Please ensure helpers.py is accessible and all its dependencies are installed.\")\n",
    "    if 'Qs' not in globals(): Qs = None \n",
    "    if 'common_get_prompt' not in globals(): common_get_prompt = lambda persona=None, group_chat=True, reasoning=True, confidence=True, answer=True: \"\" \n",
    "    if 'common_models' not in globals(): common_models = []\n",
    "    if 'common_TEMP' not in globals(): common_TEMP = 1\n",
    "    if 'common_get_client' not in globals(): common_get_client = lambda x: None\n",
    "TEMP = TEMP_OVERRIDE if 'TEMP_OVERRIDE' in globals() and TEMP_OVERRIDE is not None else common_TEMP\n",
    "models = MODELS_OVERRIDE if 'MODELS_OVERRIDE' in globals() and MODELS_OVERRIDE else common_models\n",
    "get_prompt = common_get_prompt\n",
    "get_client = common_get_client\n",
    "model = models[0] if models else None # Default model if needed\n",
    "\n",
    "from openai import OpenAI # OpenAI might still be needed directly or is handled by get_client\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")\n",
    "if not API_KEY:\n",
    "    print(\"Warning: OPENROUTER_API_KEY not found in .env file.\")\n",
    "\n",
    "client = get_client(model) if model else None # Initialize a default client if a model is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 2. Question Handler Setup (Using Qs from helpers.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Question_Handler class definition is removed.\n",
    "# We will use the 'Qs' instance (GGB_Statements) imported from helpers.py\n",
    "\n",
    "if Qs:\n",
    "    print(f\"Successfully imported Qs (GGB_Statements handler) from helpers.py.\")\n",
    "    print(f\"Total GGB questions available: {Qs.get_total_questions()}\")\n",
    "    # Example: Get GGB question by 1-based number (internally calls get_question_by_index)\n",
    "    # Note: The old Question_Handler had a custom map. Qs.get_question_by_index(0) gets the first GGB question.\n",
    "    example_q_num = 1\n",
    "    question_data_example = Qs.get_question_by_index(example_q_num -1)\n",
    "    if question_data_example:\n",
    "        print(f\"Example GGB Question {example_q_num} (ID: {question_data_example.get('statement_id')}): {question_data_example.get('statement', '')[:100]}...\")\n",
    "    else:\n",
    "        print(f\"Could not fetch example GGB question {example_q_num}.\")\n",
    "else:\n",
    "    print(\"Error: Qs (GGB_Statements handler) could not be imported or initialized from helpers.py.\")\n",
    "    print(\"Multi-agent runs will likely fail.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Helper Functions (Saving, Logging, Checkpointing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import hashlib\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def create_config_hash(config_details):\n",
    "    \"\"\"Creates a short hash from a configuration dictionary or list.\"\"\"\n",
    "    if isinstance(config_details, dict):\n",
    "        config_string = json.dumps(config_details, sort_keys=True)\n",
    "    elif isinstance(config_details, list):\n",
    "        try:\n",
    "            # Attempt to sort if list of dicts with 'model' key\n",
    "            sorted_list = sorted(config_details, key=lambda x: x.get('model', str(x)))\n",
    "            config_string = json.dumps(sorted_list)\n",
    "        except TypeError:\n",
    "            config_string = json.dumps(sorted(map(str, config_details))) # Sort by string representation\n",
    "    else:\n",
    "        config_string = str(config_details)\n",
    "\n",
    "    return hashlib.md5(config_string.encode('utf-8')).hexdigest()[:8]\n",
    "\n",
    "def get_multi_agent_filenames(chat_type, config_details, question_range, num_iterations, model_identifier=\"ggb\"): # Added model_identifier\n",
    "    \"\"\"Generates consistent filenames for multi-agent runs.\"\"\"\n",
    "    config_hash = create_config_hash(config_details)\n",
    "    q_start, q_end = question_range\n",
    "    safe_model_id = model_identifier.replace(\"/\", \"_\").replace(\":\", \"_\")\n",
    "\n",
    "    # Ensure filenames clearly indicate GGB source and distinguish from old MoralBench runs\n",
    "    base_filename_core = f\"{chat_type}_{safe_model_id}_{config_hash}_q{q_start}-{q_end}_n{num_iterations}\"\n",
    "\n",
    "    csv_dir = 'results_multi'\n",
    "    log_dir = 'logs'\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(csv_dir, exist_ok=True)\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    csv_file = os.path.join(csv_dir, f\"{base_filename_core}.csv\")\n",
    "    log_file = os.path.join(log_dir, f\"{base_filename_core}.log\")\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, f\"{base_filename_core}_checkpoint.json\")\n",
    "\n",
    "    return csv_file, log_file, checkpoint_file\n",
    "\n",
    "def save_checkpoint_multi(checkpoint_file, completed_data):\n",
    "    \"\"\"Save the current progress (structured without top-level hash) for multi-agent runs.\"\"\"\n",
    "    try:\n",
    "        with open(checkpoint_file, 'w') as f:\n",
    "            json.dump(completed_data, f, indent=4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving checkpoint to {checkpoint_file}: {e}\")\n",
    "\n",
    "def load_checkpoint_multi(checkpoint_file):\n",
    "    \"\"\"Load progress for multi-agent runs (structured without top-level hash).\"\"\"\n",
    "    if not os.path.exists(checkpoint_file):\n",
    "        print(f\"Checkpoint file {checkpoint_file} not found. Starting fresh.\")\n",
    "        return {}\n",
    "    try:\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            completed_data = json.load(f)\n",
    "        if isinstance(completed_data, dict):\n",
    "            print(f\"Loaded checkpoint from {checkpoint_file}\")\n",
    "            return completed_data\n",
    "        else:\n",
    "            print(f\"Invalid checkpoint format in {checkpoint_file}. Starting fresh.\")\n",
    "            return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error decoding JSON from {checkpoint_file}. Starting fresh.\")\n",
    "        return {}\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint {checkpoint_file}: {e}. Starting fresh.\")\n",
    "        return {}\n",
    "\n",
    "def setup_logger_multi(log_file):\n",
    "    \"\"\"Sets up a logger for multi-agent runs.\"\"\"\n",
    "    logger_name = os.path.basename(log_file).replace('.log', '')\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "    if not logger.handlers:\n",
    "        log_dir = os.path.dirname(log_file)\n",
    "        if log_dir:\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "        file_handler = logging.FileHandler(log_file, mode='a', encoding='utf-8')\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def write_to_csv_multi(run_result, csv_file):\n",
    "    \"\"\"Appends a single run's results (as a dictionary) to a CSV file.\"\"\"\n",
    "    if not run_result:\n",
    "        return\n",
    "    file_exists = os.path.exists(csv_file)\n",
    "    is_empty = not file_exists or os.path.getsize(csv_file) == 0\n",
    "    os.makedirs(os.path.dirname(csv_file) if os.path.dirname(csv_file) else '.', exist_ok=True)\n",
    "\n",
    "    fieldnames = [\n",
    "        'question_num', 'question_id', 'run_index', 'chat_type', 'config_details',\n",
    "        'conversation_history', 'agent_responses', 'timestamp'\n",
    "    ]\n",
    "\n",
    "    with open(csv_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')\n",
    "        if is_empty:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(run_result)\n",
    "\n",
    "def extract_answer_from_response(content):\n",
    "    \"\"\"Extracts the answer (e.g., A, B) from <ANSWER> tags.\"\"\"\n",
    "    match = re.search(r\"<ANSWER>(.*?)</ANSWER>\", content, re.IGNORECASE | re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"No answer found\"\n",
    "\n",
    "def extract_confidence_from_response(content):\n",
    "    \"\"\"Extracts the confidence number from <CONF> tags.\"\"\"\n",
    "    match = re.search(r\"<CONF>(.*?)</CONF>\", content, re.IGNORECASE | re.DOTALL)\n",
    "    return match.group(1).strip() if match else \"No confidence found\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Ring/Chain with Convergence Pressure (Using GGB Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # Keep for potential inline plotting\n",
    "import pandas as pd # Keep for potential inline analysis\n",
    "import seaborn as sns # Keep for potential inline plotting\n",
    "import json # Ensure json is imported\n",
    "from datetime import datetime # Ensure datetime is imported\n",
    "import gc # Import gc for garbage collection\n",
    "import os # Ensure os is imported for path confirmation\n",
    "\n",
    "# --- Configuration ---\n",
    "CHAT_TYPE = \"round_robin_ggb\" # Indicate GGB usage\n",
    "QUESTION_RANGE = (1, Qs.get_total_questions() if Qs else 1) # Use total GGB questions\n",
    "N_ITERATIONS_PER_QUESTION = 1\n",
    "N_CONVERGENCE_LOOPS = 3\n",
    "SHUFFLE_AGENTS = False\n",
    "\n",
    "MODEL_ENSEMBLE_CONFIG = [\n",
    "    {\"model\": models[0], \"number\": 1},\n",
    "    {\"model\": models[1], \"number\": 1},\n",
    "    {\"model\": models[2], \"number\": 1},\n",
    "] if len(models) >=3 else [] # Ensure enough models are defined\n",
    "\n",
    "# --- Generate Filenames and Load Checkpoint ---\n",
    "config_details_for_filename = {'ensemble': MODEL_ENSEMBLE_CONFIG, 'loops': N_CONVERGENCE_LOOPS, 'shuffle': SHUFFLE_AGENTS}\n",
    "CONFIG_HASH = create_config_hash(config_details_for_filename)\n",
    "csv_file, log_file, checkpoint_file = get_multi_agent_filenames(CHAT_TYPE, config_details_for_filename, QUESTION_RANGE, N_ITERATIONS_PER_QUESTION, model_identifier=\"ggb_ring\")\n",
    "logger = setup_logger_multi(log_file)\n",
    "completed_runs = load_checkpoint_multi(checkpoint_file)\n",
    "\n",
    "async def run_single_ring_iteration(model_ensemble, task, max_loops, config_details, question_num, question_id, iteration_idx, shuffle=False):\n",
    "    \"\"\"Runs one iteration of the round-robin chat, returning aggregated results.\"\"\"\n",
    "    agents = []\n",
    "    agent_map = {}\n",
    "    config_details_str = json.dumps(config_details, sort_keys=True)\n",
    "\n",
    "    agent_index = 0\n",
    "    for i, model_data in enumerate(model_ensemble):\n",
    "        for j in range(model_data['number']):\n",
    "            model_name = model_data['model']\n",
    "            system_message = get_prompt(group_chat=True) # get_prompt from helpers\n",
    "            model_text_safe = re.sub(r'\\W+','_', model_name)\n",
    "            agent_name = f\"agent_{model_text_safe}_{i}_{j}\"\n",
    "            agent = AssistantAgent(\n",
    "                name=agent_name,\n",
    "                model_client=get_client(model_name), # get_client from helpers\n",
    "                system_message=system_message,\n",
    "            )\n",
    "            agent_map[agent_name] = model_name\n",
    "            agents.append(agent)\n",
    "            agent_index += 1\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(agents)\n",
    "\n",
    "    num_agents = len(agents)\n",
    "    if num_agents == 0:\n",
    "        logger.warning(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx}: No agents created, skipping.\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx}: Starting chat with {num_agents} agents.\")\n",
    "\n",
    "    termination_condition = MaxMessageTermination((max_loops * num_agents) + 1)\n",
    "    team = RoundRobinGroupChat(agents, termination_condition=termination_condition)\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = await Console(team.run_stream(task=task))\n",
    "    duration = time.time() - start_time\n",
    "    logger.info(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx}: Chat finished in {duration:.2f} seconds.\")\n",
    "\n",
    "    conversation_history = []\n",
    "    agent_responses = []\n",
    "\n",
    "    for msg_idx, message in enumerate(result.messages):\n",
    "        msg_timestamp_iso = None\n",
    "        if hasattr(message, 'timestamp') and message.timestamp:\n",
    "            try:\n",
    "                msg_timestamp_iso = message.timestamp.isoformat()\n",
    "            except AttributeError:\n",
    "                 msg_timestamp_iso = str(message.timestamp)\n",
    "\n",
    "        conversation_history.append({\n",
    "            'index': msg_idx,\n",
    "            'source': message.source,\n",
    "            'content': message.content,\n",
    "            'timestamp': msg_timestamp_iso\n",
    "        })\n",
    "\n",
    "        if message.source != \"user\":\n",
    "            agent_name = message.source\n",
    "            model_name = agent_map.get(agent_name, \"unknown_model\")\n",
    "            answer = extract_answer_from_response(message.content)\n",
    "            conf = extract_confidence_from_response(message.content)\n",
    "\n",
    "            agent_responses.append({\n",
    "                'agent_name': agent_name,\n",
    "                'agent_model': model_name,\n",
    "                'message_index': msg_idx,\n",
    "                'extracted_answer': answer,\n",
    "                'extracted_confidence': conf,\n",
    "                'message_content': message.content\n",
    "            })\n",
    "            logger.info(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx+1} Msg{msg_idx} Agent {agent_name}: Ans={answer}, Conf={conf}\")\n",
    "\n",
    "    conversation_history_json = json.dumps(conversation_history)\n",
    "    agent_responses_json = json.dumps(agent_responses)\n",
    "\n",
    "    run_result_dict = {\n",
    "        'question_num': question_num, # Sequential number from range\n",
    "        'question_id': question_id,   # GGB statement_id\n",
    "        'run_index': iteration_idx + 1,\n",
    "        'chat_type': CHAT_TYPE,\n",
    "        'config_details': config_details_str,\n",
    "        'conversation_history': conversation_history_json,\n",
    "        'agent_responses': agent_responses_json,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "    del agents, team, result\n",
    "    gc.collect()\n",
    "\n",
    "    return run_result_dict\n",
    "\n",
    "def plot_summary(csv_file):\n",
    "    # ... existing code ...\n",
    "\n",
    "async def main_ring_convergence():\n",
    "    if not Qs:\n",
    "        print(\"Qs (GGB Question Handler) not available. Aborting.\")\n",
    "        return\n",
    "    if not MODEL_ENSEMBLE_CONFIG:\n",
    "        print(\"MODEL_ENSEMBLE_CONFIG is empty. Aborting ring convergence run.\")\n",
    "        return\n",
    "\n",
    "    global QUESTION_RANGE\n",
    "    if QUESTION_RANGE[1] > Qs.get_total_questions():\n",
    "        print(f\"Warning: Requested upper question range {QUESTION_RANGE[1]} exceeds available GGB questions {Qs.get_total_questions()}.\")\n",
    "        QUESTION_RANGE = (QUESTION_RANGE[0], Qs.get_total_questions())\n",
    "        print(f\"Adjusted upper range to {QUESTION_RANGE[1]}.\")\n",
    "\n",
    "    print(f\"Starting {CHAT_TYPE} run with GGB questions.\")\n",
    "    logger.info(f\"--- Starting New Run (GGB) --- CONFIG HASH: {CONFIG_HASH} --- Chat Type: {CHAT_TYPE} ---\")\n",
    "\n",
    "    for q_num_iter in range(QUESTION_RANGE[0], QUESTION_RANGE[1] + 1): # q_num_iter is 1-based\n",
    "        q_checkpoint_key = str(q_num_iter)\n",
    "        if q_checkpoint_key not in completed_runs:\n",
    "            completed_runs[q_checkpoint_key] = {}\n",
    "\n",
    "        # Fetch GGB question data using 0-based index\n",
    "        question_data = Qs.get_question_by_index(q_num_iter - 1)\n",
    "        if not question_data or 'statement' not in question_data or 'statement_id' not in question_data:\n",
    "            logger.error(f\"GGB Question for index {q_num_iter-1} (number {q_num_iter}) not found or malformed. Skipping.\")\n",
    "            continue\n",
    "        task_text = question_data['statement']\n",
    "        current_ggb_question_id = question_data['statement_id']\n",
    "\n",
    "        for iter_idx in range(N_ITERATIONS_PER_QUESTION):\n",
    "            iter_checkpoint_key = str(iter_idx)\n",
    "            if completed_runs.get(q_checkpoint_key, {}).get(iter_checkpoint_key, False):\n",
    "                print(f\"Skipping GGB Question num {q_num_iter} (ID {current_ggb_question_id}), Iteration {iter_idx+1} (already completed).\")\n",
    "                logger.info(f\"Skipping GGB Q_num{q_num_iter} (ID {current_ggb_question_id}) Iter{iter_idx+1} (already completed).\")\n",
    "                continue\n",
    "\n",
    "            print(f\"--- Running GGB Q_num {q_num_iter} (ID {current_ggb_question_id}), Iteration {iter_idx+1}/{N_ITERATIONS_PER_QUESTION} ---\")\n",
    "            logger.info(f\"--- Running GGB Q_num{q_num_iter} (ID {current_ggb_question_id}) Iter{iter_idx+1}/{N_ITERATIONS_PER_QUESTION} ---\")\n",
    "            logger.info(f\"Task: {task_text[:100]}...\")\n",
    "\n",
    "            try:\n",
    "                iteration_result_data = await run_single_ring_iteration(\n",
    "                    model_ensemble=MODEL_ENSEMBLE_CONFIG,\n",
    "                    task=task_text,\n",
    "                    max_loops=N_CONVERGENCE_LOOPS,\n",
    "                    config_details=config_details_for_filename,\n",
    "                    question_num=q_num_iter, # Pass the 1-based number for record keeping\n",
    "                    question_id=current_ggb_question_id, # Pass GGB statement_id\n",
    "                    iteration_idx=iter_idx,\n",
    "                    shuffle=SHUFFLE_AGENTS\n",
    "                )\n",
    "\n",
    "                if iteration_result_data:\n",
    "                    write_to_csv_multi(iteration_result_data, csv_file)\n",
    "                    completed_runs[q_checkpoint_key][iter_checkpoint_key] = True\n",
    "                    save_checkpoint_multi(checkpoint_file, completed_runs)\n",
    "                    print(f\"--- Finished GGB Q_num {q_num_iter} (ID {current_ggb_question_id}), Iteration {iter_idx+1}. Results saved. ---\")\n",
    "                    logger.info(f\"--- Finished GGB Q_num{q_num_iter} (ID {current_ggb_question_id}) Iter{iter_idx+1}. Results saved. ---\")\n",
    "                else:\n",
    "                    print(f\"--- GGB Q_num {q_num_iter} (ID {current_ggb_question_id}), Iteration {iter_idx+1} produced no results. ---\")\n",
    "                    logger.warning(f\"--- GGB Q_num{q_num_iter} (ID {current_ggb_question_id}) Iter{iter_idx+1} produced no results. ---\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during GGB Q_num {q_num_iter} (ID {current_ggb_question_id}), Iteration {iter_idx+1}: {e}\")\n",
    "                logger.error(f\"Error during GGB Q_num{q_num_iter} (ID {current_ggb_question_id}) Iter{iter_idx+1}: {e}\", exc_info=True)\n",
    "            finally:\n",
    "                gc.collect()\n",
    "\n",
    "    print(f\"--- Run Finished (GGB) --- CONFIG HASH: {CONFIG_HASH} ---\")\n",
    "    logger.info(f\"--- Run Finished (GGB) --- CONFIG HASH: {CONFIG_HASH} ---\")\n",
    "\n",
    "async def run_main_ring(): # Renamed to avoid conflict if running star chat later\n",
    "    await main_ring_convergence()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Standard way to run asyncio main in a script/notebook\n",
    "    if 'get_ipython' in globals() and get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        asyncio.run(run_main_ring())\n",
    "    else:\n",
    "        asyncio.run(run_main_ring())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ring convergence results\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "# Ensure csv_file is defined from the previous cell's execution context\n",
    "if 'csv_file' in locals():\n",
    "    print(f\"Plotting data from: {csv_file}\")\n",
    "    # plot_all_agent_responses(csv_file) # Call the plotting function if needed\n",
    "else:\n",
    "    print(\"csv_file variable not found. Cannot plot. Make sure the ring convergence cell has run.\")\n",
    "\n",
    "# ... existing code for plot_all_agent_responses ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Star with Convergence Pressure (Using GGB Questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Sequence\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from typing_extensions import Self\n",
    "\n",
    "from autogen_core._component_config import Component\n",
    "from autogen_core.models import FunctionExecutionResultMessage, LLMMessage\n",
    "from autogen_core.model_context._chat_completion_context import ChatCompletionContext\n",
    "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage # Added BaseChatMessage\n",
    "\n",
    "class BufferedChatCompletionContextConfig(BaseModel):\n",
    "    buffer_size: int\n",
    "    initial_messages: List[LLMMessage] | None = None\n",
    "\n",
    "class BufferedChatCompletionContext(ChatCompletionContext, Component[BufferedChatCompletionContextConfig]):\n",
    "    component_config_schema = BufferedChatCompletionContextConfig\n",
    "    component_provider_override = \"autogen_core.model_context.BufferedChatCompletionContext\"\n",
    "    def __init__(self, buffer_size: int, initial_messages: List[LLMMessage] | None = None) -> None:\n",
    "        super().__init__(initial_messages)\n",
    "        if buffer_size <= 0:\n",
    "            raise ValueError(\"buffer_size must be greater than 0.\")\n",
    "        self._buffer_size = buffer_size\n",
    "    async def get_messages(self) -> List[LLMMessage]:\n",
    "        # Ensure we don't go out of bounds if self._messages is short\n",
    "        # Keep the system message (if any, typically self._messages[0]) and the last (buffer_size-1) messages\n",
    "        if not self._messages: return []\n",
    "        if len(self._messages) <= self._buffer_size:\n",
    "            messages_to_return = list(self._messages)\n",
    "        else:\n",
    "            messages_to_return = [self._messages[0]] + self._messages[-(self._buffer_size -1):]\n",
    "\n",
    "        if messages_to_return and isinstance(messages_to_return[0], FunctionExecutionResultMessage):\n",
    "            messages_to_return = messages_to_return[1:]\n",
    "        return messages_to_return\n",
    "    def _to_config(self) -> BufferedChatCompletionContextConfig:\n",
    "        return BufferedChatCompletionContextConfig(\n",
    "            buffer_size=self._buffer_size, initial_messages=self._initial_messages\n",
    "        )\n",
    "    @classmethod\n",
    "    def _from_config(cls, config: BufferedChatCompletionContextConfig) -> Self:\n",
    "        return cls(**config.model_dump())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.teams import SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_agentchat.conditions import MaxMessageTermination\n",
    "from autogen_agentchat.messages import BaseAgentEvent, BaseChatMessage\n",
    "\n",
    "import asyncio\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import json\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import os\n",
    "from typing import Sequence, List, Dict, Any\n",
    "import re\n",
    "\n",
    "# --- Configuration ---\n",
    "STAR_CHAT_TYPE = \"star_ggb\" # Specific type for star runs with GGB\n",
    "STAR_QUESTION_RANGE = (1, Qs.get_total_questions() if Qs else 1) # Use total GGB questions\n",
    "STAR_N_ITERATIONS_PER_QUESTION = 1\n",
    "STAR_N_CONVERGENCE_LOOPS = 3\n",
    "\n",
    "if len(models) < 1:\n",
    "    print(\"Warning: 'models' list is empty. Star chat requires at least one model.\")\n",
    "    STAR_CENTRAL_MODEL = None\n",
    "    STAR_PERIPHERAL_MODELS = []\n",
    "else:\n",
    "    STAR_MODEL_NUM_FOR_ID = 0 # Identifier for filenames if needed, can be a string too\n",
    "    STAR_CENTRAL_MODEL = models[0]\n",
    "    STAR_PERIPHERAL_MODELS = models[:3] # Example: use first 3 models as peripherals\n",
    "    if not STAR_PERIPHERAL_MODELS:\n",
    "        print(\"Warning: Not enough models for peripherals based on current 'models' list.\")\n",
    "\n",
    "star_config_details_for_filename = {\n",
    "    'central_model': STAR_CENTRAL_MODEL,\n",
    "    'peripheral_models': STAR_PERIPHERAL_MODELS,\n",
    "    'loops': STAR_N_CONVERGENCE_LOOPS\n",
    "}\n",
    "STAR_CONFIG_HASH = create_config_hash(star_config_details_for_filename)\n",
    "star_csv_file, star_log_file, star_checkpoint_file = get_multi_agent_filenames(\n",
    "    STAR_CHAT_TYPE,\n",
    "    star_config_details_for_filename,\n",
    "    STAR_QUESTION_RANGE,\n",
    "    STAR_N_ITERATIONS_PER_QUESTION,\n",
    "    model_identifier=f\"central_{str(STAR_MODEL_NUM_FOR_ID).replace('/','_')}\" # More descriptive identifier\n",
    ")\n",
    "star_logger = setup_logger_multi(star_log_file)\n",
    "star_completed_runs = load_checkpoint_multi(star_checkpoint_file)\n",
    "\n",
    "async def run_single_star_iteration(central_model_name, peripheral_model_names, task, max_loops, config_details, question_num, question_id, iteration_idx):\n",
    "    agents = []\n",
    "    agent_map = {}\n",
    "    config_details_str = json.dumps(config_details, sort_keys=True)\n",
    "\n",
    "    central_agent_name = \"central_supervisor\"\n",
    "    central_system_message = \"\"\"You are a supervisor agent. Your role is to manage the conversation flow with peripheral agents to answer a given question. You will ask each peripheral agent for their response and rationale. You should relay the question and any relevant context (like previous responses if the strategy involves sharing them, though current peripherals have short buffers). You cannot form your own opinion on the question itself but must ensure the process is followed. After each peripheral agent responds, you will select the next one or conclude if the loops are done. The final output should be a summary of the discussion if applicable, or simply ensuring all peripherals have had their say for the required number of loops.\"\"\"\n",
    "    central_agent = AssistantAgent(\n",
    "        name=central_agent_name,\n",
    "        model_client=get_client(central_model_name),\n",
    "        system_message=central_system_message,\n",
    "    )\n",
    "    agents.append(central_agent)\n",
    "    agent_map[central_agent_name] = central_model_name\n",
    "\n",
    "    peripheral_agent_names_list = []\n",
    "    for i, p_model_name in enumerate(peripheral_model_names):\n",
    "        system_message = get_prompt(group_chat=True) # Peripheral agents get the group chat prompt\n",
    "        model_text_safe = re.sub(r'\\W+','_', p_model_name)\n",
    "        p_agent_name = f\"peripheral_{model_text_safe}_{i}\"\n",
    "        p_agent = AssistantAgent(\n",
    "            name=p_agent_name,\n",
    "            model_client=get_client(p_model_name),\n",
    "            system_message=system_message,\n",
    "            model_context=BufferedChatCompletionContext(buffer_size=2) # Small buffer for peripherals\n",
    "        )\n",
    "        agents.append(p_agent)\n",
    "        agent_map[p_agent_name] = p_model_name\n",
    "        peripheral_agent_names_list.append(p_agent_name)\n",
    "\n",
    "    num_peripherals = len(peripheral_agent_names_list)\n",
    "    if num_peripherals == 0:\n",
    "        star_logger.warning(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx}: No peripheral agents, skipping.\")\n",
    "        return None\n",
    "\n",
    "    star_logger.info(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx}: Starting star chat. Central: {central_model_name}, Peripherals: {peripheral_model_names}\")\n",
    "\n",
    "    current_peripheral_idx = 0\n",
    "    peripheral_turns_taken = [0] * num_peripherals\n",
    "\n",
    "    def star_selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n",
    "        nonlocal current_peripheral_idx, peripheral_turns_taken\n",
    "        last_message = messages[-1]\n",
    "\n",
    "        if len(messages) == 1: return central_agent_name # Initial task to central\n",
    "\n",
    "        if last_message.source == central_agent_name:\n",
    "            # Central just spoke, pick next peripheral that hasn't completed its loops\n",
    "            for i in range(num_peripherals):\n",
    "                p_idx_to_check = (current_peripheral_idx + i) % num_peripherals\n",
    "                if peripheral_turns_taken[p_idx_to_check] < max_loops:\n",
    "                    current_peripheral_idx = p_idx_to_check\n",
    "                    return peripheral_agent_names_list[current_peripheral_idx]\n",
    "            return None # All peripherals done\n",
    "        elif last_message.source in peripheral_agent_names_list:\n",
    "            # Peripheral just spoke, increment its turn count\n",
    "            p_idx_that_spoke = peripheral_agent_names_list.index(last_message.source)\n",
    "            peripheral_turns_taken[p_idx_that_spoke] += 1\n",
    "            # Always return to central agent to decide next step or summarize\n",
    "            return central_agent_name\n",
    "        return None # Should not happen\n",
    "\n",
    "    # Max messages: 1 (user) + N_loops * num_peripherals (for peripheral responses) + N_loops * num_peripherals (for central agent to prompt each peripheral)\n",
    "    # Potentially one final message from central if it summarizes.\n",
    "    max_total_messages = 1 + (max_loops * num_peripherals * 2) + 1\n",
    "    termination_condition = MaxMessageTermination(max_total_messages)\n",
    "\n",
    "    team = SelectorGroupChat(\n",
    "        agents,\n",
    "        selector_func=star_selector_func,\n",
    "        termination_condition=termination_condition,\n",
    "        model_client=get_client(central_model_name), # Selector group chat needs a client\n",
    "    )\n",
    "\n",
    "    start_time = time.time()\n",
    "    result = await Console(team.run_stream(task=task))\n",
    "    duration = time.time() - start_time\n",
    "    star_logger.info(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx}: Chat finished in {duration:.2f}s. Msgs: {len(result.messages)}\")\n",
    "\n",
    "    conversation_history_list = []\n",
    "    agent_responses_list = []\n",
    "    for msg_idx, message_obj in enumerate(result.messages):\n",
    "        # ... (timestamp formatting as in ring convergence) ...\n",
    "        msg_timestamp_iso = datetime.now().isoformat() # Placeholder if not available\n",
    "        if hasattr(message_obj, 'timestamp') and message_obj.timestamp:\n",
    "            try: msg_timestamp_iso = message_obj.timestamp.isoformat()\n",
    "            except: msg_timestamp_iso = str(message_obj.timestamp)\n",
    "\n",
    "        conversation_history_list.append({\n",
    "            'index': msg_idx, 'source': message_obj.source, 'content': message_obj.content, 'timestamp': msg_timestamp_iso\n",
    "        })\n",
    "        if message_obj.source in peripheral_agent_names_list:\n",
    "            p_agent_name = message_obj.source\n",
    "            p_model_name = agent_map.get(p_agent_name, \"unknown_peripheral\")\n",
    "            answer_ext = extract_answer_from_response(message_obj.content)\n",
    "            conf_ext = extract_confidence_from_response(message_obj.content)\n",
    "            agent_responses_list.append({\n",
    "                'agent_name': p_agent_name, 'agent_model': p_model_name, 'message_index': msg_idx,\n",
    "                'extracted_answer': answer_ext, 'extracted_confidence': conf_ext, 'message_content': message_obj.content\n",
    "            })\n",
    "            star_logger.info(f\"Q_num{question_num} (GGB ID {question_id}) Iter{iteration_idx+1} Msg{msg_idx} Agent {p_agent_name}: Ans={answer_ext}, Conf={conf_ext}\")\n",
    "\n",
    "    run_result_data_dict = {\n",
    "        'question_num': question_num, 'question_id': question_id, 'run_index': iteration_idx + 1,\n",
    "        'chat_type': STAR_CHAT_TYPE, 'config_details': config_details_str,\n",
    "        'conversation_history': json.dumps(conversation_history_list),\n",
    "        'agent_responses': json.dumps(agent_responses_list), # Contains only peripheral responses\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    del agents, team, result\n",
    "    gc.collect()\n",
    "    return run_result_data_dict\n",
    "\n",
    "async def main_star_convergence():\n",
    "    if not Qs or STAR_CENTRAL_MODEL is None or not STAR_PERIPHERAL_MODELS:\n",
    "        print(\"Qs, STAR_CENTRAL_MODEL, or STAR_PERIPHERAL_MODELS not available. Aborting star run.\")\n",
    "        star_logger.error(\"Qs, STAR_CENTRAL_MODEL, or STAR_PERIPHERAL_MODELS not available. Aborting star run.\")\n",
    "        return\n",
    "\n",
    "    global STAR_QUESTION_RANGE\n",
    "    if STAR_QUESTION_RANGE[1] > Qs.get_total_questions():\n",
    "        STAR_QUESTION_RANGE = (STAR_QUESTION_RANGE[0], Qs.get_total_questions())\n",
    "        print(f\"Adjusted star question upper range to {STAR_QUESTION_RANGE[1]}.\")\n",
    "\n",
    "    print(f\"Starting {STAR_CHAT_TYPE} run with GGB questions.\")\n",
    "    star_logger.info(f\"--- Starting New Star Run (GGB) --- CONFIG HASH: {STAR_CONFIG_HASH} ---\")\n",
    "\n",
    "    for q_num_iter_star in range(STAR_QUESTION_RANGE[0], STAR_QUESTION_RANGE[1] + 1):\n",
    "        q_star_checkpoint_key = str(q_num_iter_star)\n",
    "        if q_star_checkpoint_key not in star_completed_runs:\n",
    "            star_completed_runs[q_star_checkpoint_key] = {}\n",
    "\n",
    "        ggb_question_data = Qs.get_question_by_index(q_num_iter_star - 1)\n",
    "        if not ggb_question_data or 'statement' not in ggb_question_data or 'statement_id' not in ggb_question_data:\n",
    "            star_logger.error(f\"GGB Question for index {q_num_iter_star-1} (num {q_num_iter_star}) malformed. Skipping.\")\n",
    "            continue\n",
    "        current_task_text = ggb_question_data['statement']\n",
    "        current_ggb_id = ggb_question_data['statement_id']\n",
    "\n",
    "        for star_iter_idx in range(STAR_N_ITERATIONS_PER_QUESTION):\n",
    "            star_iter_checkpoint_key = str(star_iter_idx)\n",
    "            if star_completed_runs.get(q_star_checkpoint_key, {}).get(star_iter_checkpoint_key, False):\n",
    "                print(f\"Skipping GGB Q_num {q_num_iter_star} (ID {current_ggb_id}), Star Iter {star_iter_idx+1} (completed).\")\n",
    "                star_logger.info(f\"Skipping GGB Q_num{q_num_iter_star} (ID {current_ggb_id}) Star Iter{star_iter_idx+1} (completed).\")\n",
    "                continue\n",
    "\n",
    "            print(f\"--- Running GGB Q_num {q_num_iter_star} (ID {current_ggb_id}), Star Iter {star_iter_idx+1}/{STAR_N_ITERATIONS_PER_QUESTION} ---\")\n",
    "            star_logger.info(f\"--- Running GGB Q_num{q_num_iter_star} (ID {current_ggb_id}) Star Iter{star_iter_idx+1} ---\")\n",
    "\n",
    "            try:\n",
    "                star_iteration_result = await run_single_star_iteration(\n",
    "                    central_model_name=STAR_CENTRAL_MODEL,\n",
    "                    peripheral_model_names=STAR_PERIPHERAL_MODELS,\n",
    "                    task=current_task_text,\n",
    "                    max_loops=STAR_N_CONVERGENCE_LOOPS,\n",
    "                    config_details=star_config_details_for_filename,\n",
    "                    question_num=q_num_iter_star,\n",
    "                    question_id=current_ggb_id,\n",
    "                    iteration_idx=star_iter_idx\n",
    "                )\n",
    "                if star_iteration_result:\n",
    "                    write_to_csv_multi(star_iteration_result, star_csv_file)\n",
    "                    star_completed_runs[q_star_checkpoint_key][star_iter_checkpoint_key] = True\n",
    "                    save_checkpoint_multi(star_checkpoint_file, star_completed_runs)\n",
    "                    star_logger.info(f\"--- Finished GGB Q_num{q_num_iter_star} (ID {current_ggb_id}) Star Iter{star_iter_idx+1}. Saved. ---\")\n",
    "                else:\n",
    "                    star_logger.warning(f\"--- GGB Q_num{q_num_iter_star} (ID {current_ggb_id}) Star Iter{star_iter_idx+1} no results. ---\")\n",
    "            except Exception as e_star:\n",
    "                star_logger.error(f\"Error GGB Q_num{q_num_iter_star} (ID {current_ggb_id}) Star Iter{star_iter_idx+1}: {e_star}\", exc_info=True)\n",
    "            finally: gc.collect()\n",
    "\n",
    "    star_logger.info(f\"--- Star Run Finished (GGB) --- CONFIG HASH: {STAR_CONFIG_HASH} ---\")\n",
    "\n",
    "async def run_star_main_async(): # Renamed\n",
    "    await main_star_convergence()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if 'get_ipython' in globals() and get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        # asyncio.run(run_star_main_async()) # Uncomment to run star chat\n",
    "        print(\"Star chat execution commented out. Uncomment the line above in __main__ to run.\")\n",
    "    else:\n",
    "        # asyncio.run(run_star_main_async()) # Uncomment to run star chat\n",
    "        print(\"Star chat execution commented out. Uncomment the line above in __main__ to run.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

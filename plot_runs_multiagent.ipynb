{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and Setup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import ast  # For safely evaluating string representations of lists/dicts\n",
    "\n",
    "# Define directories\n",
    "RESULTS_DIR = 'results' # Directory containing single-agent CSV results\n",
    "RESULTS_DIR_SINGLE = 'results'\n",
    "RESULTS_DIR_MULTI = 'results_multi'\n",
    "PLOT_DIR = 'plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Define the datasets (categories) to include\n",
    "# These should match the category names returned by Question_Handler\n",
    "INCLUDED_DATASETS = ['MFQ_30', '6_concepts']\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Add MoralBench repo to path to import Question_Handler\n",
    "MORAL_BENCH_REPO_DIR = '../MoralBench_AgentEnsembles' # Adjust if needed\n",
    "moral_bench_path = os.path.abspath(MORAL_BENCH_REPO_DIR)\n",
    "if moral_bench_path not in sys.path:\n",
    "    sys.path.insert(0, moral_bench_path)\n",
    "print(f\"Using MoralBench repository at: {moral_bench_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated mapping based on standard MFT + Liberty\n",
    "dict_map = {\n",
    "   'authority': 'Authority',\n",
    "   'fairness': 'Fairness',\n",
    "   'harm': 'Harm', # Care/Harm\n",
    "   'ingroup': 'Loyalty', # Loyalty/Betrayal\n",
    "   'purity': 'Sanctity', # Sanctity/Degradation\n",
    "   'liberty': 'Liberty'\n",
    "}\n",
    "\n",
    "# Define the order for plotting categories\n",
    "PLOT_CATEGORIES = ['Harm', 'Fairness', 'Loyalty', 'Authority', 'Sanctity', 'Liberty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Question Handler Definition (Copied for self-containment)\n",
    "# Note: Ideally, this would be imported from a shared module.\n",
    "class Question_Handler():\n",
    "    def __init__(self, repo_dir):\n",
    "        self.repo_dir = os.path.abspath(repo_dir) # Use absolute path\n",
    "        self.questions_dir = os.path.join(self.repo_dir, 'questions')\n",
    "        self.answers_dir = os.path.join(self.repo_dir, 'answers')\n",
    "        self.categories = self.list_categories()\n",
    "        self._build_question_map()\n",
    "\n",
    "    def _build_question_map(self):\n",
    "        \"\"\"Builds a map from question number to (category, index).\"\"\"\n",
    "        self.question_map = {}\n",
    "        current_question_num = 1\n",
    "        for category in self.categories:\n",
    "            count = self.get_question_count(category)\n",
    "            for i in range(count):\n",
    "                self.question_map[current_question_num] = {'category': category, 'index': i}\n",
    "                current_question_num += 1\n",
    "        self.total_questions = current_question_num - 1\n",
    "\n",
    "    def get_question_category_and_index(self, question_number):\n",
    "        \"\"\"Gets the category and index for a given question number.\"\"\"\n",
    "        return self.question_map.get(question_number)\n",
    "\n",
    "    def get_question_category(self, question_number):\n",
    "        \"\"\"Gets the category for a given question number.\"\"\"\n",
    "        mapping = self.question_map.get(question_number)\n",
    "        return mapping['category'] if mapping else None\n",
    "\n",
    "    def get_question_count(self, category_folder):\n",
    "        \"\"\"\n",
    "        Get the number of questions in a specific category folder.\n",
    "        \"\"\"\n",
    "        questions_path = os.path.join(self.questions_dir, category_folder)\n",
    "        if not os.path.exists(questions_path):\n",
    "            # print(f\"Warning: Category folder {questions_path} does not exist!\")\n",
    "            return 0\n",
    "        try:\n",
    "            question_files = [f for f in os.listdir(questions_path) if f.endswith('.txt')]\n",
    "            return len(question_files)\n",
    "        except FileNotFoundError:\n",
    "            # print(f\"Warning: Error accessing category folder {questions_path}.\")\n",
    "            return 0\n",
    "\n",
    "    def list_categories(self):\n",
    "        \"\"\"\n",
    "        List all available question categories.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.questions_dir):\n",
    "            print(f\"Warning: Questions directory {self.questions_dir} not found!\")\n",
    "            return []\n",
    "        try:\n",
    "            categories = sorted([d for d in os.listdir(self.questions_dir) if os.path.isdir(os.path.join(self.questions_dir, d))])\n",
    "            return categories\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: Error listing categories in {self.questions_dir}.\")\n",
    "            return []\n",
    "\n",
    "    def load_question_answer(self, category_folder, index):\n",
    "        \"\"\"\n",
    "        Load a question and its possible answers using an index.\n",
    "        \"\"\"\n",
    "        questions_path = os.path.join(self.questions_dir, category_folder)\n",
    "        if not os.path.exists(questions_path):\n",
    "            # print(f\"Warning: Category folder {questions_path} does not exist!\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            # Get all question files and sort them\n",
    "            question_files = sorted([f for f in os.listdir(questions_path) if f.endswith('.txt')])\n",
    "\n",
    "            if index < 0 or index >= len(question_files):\n",
    "                # print(f\"Warning: Index {index} is out of range for category {category_folder}! Valid range: 0-{len(question_files)-1}\")\n",
    "                return None\n",
    "\n",
    "            # Get question filename and ID\n",
    "            question_file = question_files[index]\n",
    "            question_id = os.path.splitext(question_file)[0]\n",
    "\n",
    "            # Read question content\n",
    "            question_path = os.path.join(questions_path, question_file)\n",
    "            with open(question_path, 'r', encoding='utf-8') as f:\n",
    "                question_text = f.read()\n",
    "\n",
    "            # Load answers from JSON\n",
    "            answers_path = os.path.join(self.repo_dir, 'answers', f\"{category_folder}.json\") # Corrected path\n",
    "            question_answers = None\n",
    "            if os.path.exists(answers_path):\n",
    "                try:\n",
    "                    with open(answers_path, 'r', encoding='utf-8') as f:\n",
    "                        all_answers = json.load(f)\n",
    "                    question_answers = all_answers.get(question_id, {})\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Error decoding JSON from {answers_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Error reading answers file {answers_path}: {e}\")\n",
    "            # else:\n",
    "                # print(f\"Warning: Answers file {answers_path} for {category_folder} does not exist!\")\n",
    "\n",
    "            return {\n",
    "                'question_id': question_id,\n",
    "                'question_text': question_text,\n",
    "                'answers': question_answers\n",
    "            }\n",
    "        except FileNotFoundError:\n",
    "            # print(f\"Warning: Error accessing files in {questions_path}.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Unexpected error loading question {category_folder}/{index}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_question(self, number):\n",
    "        \"\"\"Gets question data by absolute number.\"\"\"\n",
    "        mapping = self.get_question_category_and_index(number)\n",
    "        if mapping:\n",
    "            return self.load_question_answer(mapping['category'], mapping['index'])\n",
    "        else:\n",
    "            # print(f\"Warning: Question number {number} not found in map.\")\n",
    "            return None\n",
    "        \n",
    "    def get_question_by_category_and_id(self, category, id):\n",
    "        \"\"\"Gets question data by category and ID (eg MFQ_30 and harm_1).\"\"\"\n",
    "        # it should iterate over the questions in the category and find the one with the matching id\n",
    "        questions_path = os.path.join(self.questions_dir, category)\n",
    "        if not os.path.exists(questions_path):\n",
    "            # print(f\"Warning: Category folder {questions_path} does not exist!\")\n",
    "            return None\n",
    "        try:\n",
    "            # Get all question files and sort them\n",
    "            question_files = sorted([f for f in os.listdir(questions_path) if f.endswith('.txt')])\n",
    "\n",
    "            for question_file in question_files:\n",
    "                question_id = os.path.splitext(question_file)[0]\n",
    "                if question_id == id:\n",
    "                    # Read question content\n",
    "                    question_path = os.path.join(questions_path, question_file)\n",
    "                    with open(question_path, 'r', encoding='utf-8') as f:\n",
    "                        question_text = f.read()\n",
    "\n",
    "                    # Load answers from JSON\n",
    "                    answers_path = os.path.join(self.repo_dir, 'answers', f\"{category}.json\") # Corrected path\n",
    "                    question_answers = None\n",
    "                    if os.path.exists(answers_path):\n",
    "                        try:\n",
    "                            with open(answers_path, 'r', encoding='utf-8') as f:\n",
    "                                all_answers = json.load(f)\n",
    "                            question_answers = all_answers.get(question_id, {})\n",
    "                        except json.JSONDecodeError:\n",
    "                            print(f\"Warning: Error decoding JSON from {answers_path}\")\n",
    "                        except Exception as e:\n",
    "                            print(f\"Warning: Error reading answers file {answers_path}: {e}\")\n",
    "\n",
    "                    return {\n",
    "                        'question_id': question_id,\n",
    "                        'question_text': question_text,\n",
    "                        'answers': question_answers\n",
    "                    }\n",
    "            # print(f\"Warning: Question ID {id} not found in category {category}.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Unexpected error loading question {category}/{id}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "    def get_total_question_count(self):\n",
    "        \"\"\"Returns the total number of questions across all categories.\"\"\"\n",
    "        return self.total_questions\n",
    "\n",
    "# --- Initialize Question Handler ---\n",
    "try:\n",
    "    Qs = Question_Handler(MORAL_BENCH_REPO_DIR)\n",
    "    print(f\"Question Handler initialized. Found {Qs.get_total_question_count()} questions in {len(Qs.categories)} categories.\")\n",
    "    print(f\"Available categories: {Qs.categories}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Question_Handler: {e}\")\n",
    "    Qs = None\n",
    "\n",
    "Qs.get_question_by_category_and_id('MFQ_30', 'harm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs.get_question_category_and_index(88)\n",
    "\n",
    "# Qs.load_question_answer('MFQ_30', 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure they all have question_id\n",
    "if Qs:\n",
    "    for i in range(1,89):\n",
    "        q_info = Qs.get_question(i)  # Test the Question_Handler\n",
    "        print(f'{q_info.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def extract_category_from_id(question_id):\n",
    "    \"\"\"Extracts the category name from the question_id (e.g., 'fairness_3' -> 'Fairness').\"\"\"\n",
    "    if not isinstance(question_id, str):\n",
    "        return 'Unknown'\n",
    "    match = re.match(r\"([a-zA-Z_]+)_?\\d*\", question_id)\n",
    "    if match:\n",
    "        category_name = match.group(1).replace('_', ' ').title()\n",
    "        # Handle specific known prefixes if needed\n",
    "        if category_name.startswith('Mfq '):\n",
    "             category_name = 'MFQ_30' # Keep original dataset name if preferred\n",
    "        elif category_name.startswith('6 Concepts'):\n",
    "             category_name = '6_concepts' # Keep original dataset name if preferred\n",
    "        return category_name.strip().lower()\n",
    "    return 'Unknown'\n",
    "\n",
    "def get_category_from_qnum(q_num): # this gets out the dataset category (eg MFQ_30)\n",
    "    \"\"\"Gets the category name using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        return Qs.get_question_category(q_num)\n",
    "    return 'Unknown' # Fallback if Qs is not initialized\n",
    "\n",
    "def get_moralbench_scores(question_number, answer):\n",
    "    \"\"\"Gets the moral score for a given question number and answer.\"\"\"\n",
    "    if Qs:\n",
    "        q_data = Qs.get_question(question_number)\n",
    "        if q_data and 'answers' in q_data and q_data['answers'] and answer in q_data['answers']:\n",
    "            return q_data['answers'][answer]\n",
    "        # Handle cases where answers might be missing or empty\n",
    "        # print(f\"Warning: No score found for Q{question_number}, Answer '{answer}'. Q_data: {q_data}\")\n",
    "    return None # Fallback if Qs is not initialized, answer not found, or answers missing\n",
    "\n",
    "def get_question_id_from_qnum(q_num): # this gets out the question_id (eg fairness_3)\n",
    "    \"\"\"Gets the question ID using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        q_info = Qs.get_question(q_num)\n",
    "        if q_info and 'question_id' in q_info:\n",
    "            return q_info['question_id']\n",
    "    return 'Unknown' # Fallback if Qs is not initialized or question not found\n",
    "\n",
    "def get_moral_category_from_qnum(q_num): # this gets out the moral category (eg Harm)\n",
    "    \"\"\"Gets the moral category name using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        q_info = Qs.get_question(q_num)\n",
    "        if q_info and 'question_id' in q_info:\n",
    "            return extract_category_from_id(q_info['question_id'])\n",
    "    return 'Unknown' # Fallback if Qs is not initialized or question not found\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    \"\"\"Safely evaluate a string literal (list, dict). Returns None on error.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError, TypeError):\n",
    "        # print(f\"Warning: Could not parse value: {val}\")\n",
    "        return None\n",
    "\n",
    "def load_and_preprocess_data(results_dir):\n",
    "    \"\"\"Loads all CSV files from a directory and preprocesses them.\"\"\"\n",
    "    all_data_rows = []\n",
    "    print(f\"Checking directory: {results_dir}\")\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(f\"Warning: Directory not found: {results_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found directory: {results_dir}. Searching for CSV files...\")\n",
    "    found_csv = False\n",
    "    for filename in os.listdir(results_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            found_csv = True\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            print(f\"  Loading file: {filename}\")\n",
    "            try:\n",
    "                df_raw = pd.read_csv(filepath)\n",
    "                if df_raw.empty:\n",
    "                    print(f\"    Warning: File is empty: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Determine run type early based on columns\n",
    "                is_multi_agent = 'agent_responses' in df_raw.columns\n",
    "                is_single_agent = 'model_name' in df_raw.columns and 'run_index' in df_raw.columns and not is_multi_agent\n",
    "\n",
    "                # --- Process based on run type ---\n",
    "                if is_multi_agent:\n",
    "                    print(f\"    Processing as multi-agent data...\")\n",
    "                    df_raw['run_type'] = 'multi'\n",
    "                    # Explode the agent_responses column\n",
    "                    df_raw['agent_responses_parsed'] = df_raw['agent_responses'].apply(safe_literal_eval)\n",
    "                    df_exploded = df_raw.explode('agent_responses_parsed')\n",
    "                    df_exploded = df_exploded.dropna(subset=['agent_responses_parsed']) # Drop rows where parsing failed or was empty\n",
    "\n",
    "                    # Expand the dictionary into columns\n",
    "                    agent_data = pd.json_normalize(df_exploded['agent_responses_parsed'])\n",
    "                    df = pd.concat([df_exploded.drop(columns=['agent_responses', 'agent_responses_parsed']).reset_index(drop=True),\n",
    "                                    agent_data.reset_index(drop=True)], axis=1)\n",
    "                    print(f\"    Exploded agent responses. Shape after explode: {df.shape}\")\n",
    "\n",
    "                elif is_single_agent:\n",
    "                    print(f\"    Processing as single-agent data...\")\n",
    "                    df = df_raw.copy() # Use the raw df directly\n",
    "                    df['run_type'] = 'single'\n",
    "                else:\n",
    "                    print(f\"    Warning: Could not determine run type for {filename}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Add Category and Moral Category (Common Logic) ---\n",
    "                if 'question_num' in df.columns and Qs:\n",
    "                    df['category'] = df['question_num'].apply(get_category_from_qnum) # get the dataset category\n",
    "                    df['question_id'] = df['question_num'].apply(get_question_id_from_qnum) # get the question_id\n",
    "                    df['moral_category'] = df['question_num'].apply(get_moral_category_from_qnum) # get the moral category\n",
    "                    print(f\"    Extracted categories from 'question_num'. Unique values: {df['category'].unique()[:5]}...\")\n",
    "                elif 'question_id' in df.columns: # Fallback if question_num missing but question_id exists\n",
    "                     df['category'] = df['question_id'].apply(extract_category_from_id) # Attempt to get moral category\n",
    "                     df['moral_category'] = df['question_id'].apply(extract_category_from_id) # Use same logic for moral category\n",
    "                     # Try to infer dataset category if possible (might be less reliable)\n",
    "                     if Qs:\n",
    "                         # This requires reversing the map, might be slow/complex. Stick to moral category for now.\n",
    "                         print(\"    Warning: 'question_num' missing. Using 'question_id' for moral category. Dataset category might be inaccurate.\")\n",
    "                     else:\n",
    "                         print(\"    Warning: 'question_num' missing and Qs handler failed. Using 'question_id' for moral category.\")\n",
    "                else:\n",
    "                    df['category'] = 'Unknown'\n",
    "                    df['moral_category'] = 'Unknown'\n",
    "                    df['question_id'] = 'Unknown'\n",
    "                    print(\"    Warning: Could not determine category ('question_num' or 'question_id' missing, or Qs handler failed).\")\n",
    "\n",
    "                # --- Filter by Dataset ---\n",
    "                initial_rows = len(df)\n",
    "                df = df[df['category'].isin(INCLUDED_DATASETS)]\n",
    "                filtered_rows = len(df)\n",
    "                print(f\"    Filtered by INCLUDED_DATASETS ({INCLUDED_DATASETS}). Kept {filtered_rows}/{initial_rows} rows.\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    all_data_rows.append(df)\n",
    "                else:\n",
    "                    print(f\"    Info: No rows remaining after filtering for datasets {INCLUDED_DATASETS}.\")\n",
    "\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"    Warning: Skipping empty file: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading or processing file {filename}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc() # Print full traceback for debugging\n",
    "\n",
    "    if not found_csv:\n",
    "        print(f\"Warning: No CSV files found in directory: {results_dir}\")\n",
    "\n",
    "    if not all_data_rows:\n",
    "        print(f\"No data loaded or retained from {results_dir} after processing and filtering. Check CSV files exist, are not empty, and contain data matching INCLUDED_DATASETS: {INCLUDED_DATASETS}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Concatenating data from {len(all_data_rows)} files/dataframes.\")\n",
    "    combined_df = pd.concat(all_data_rows, ignore_index=True)\n",
    "\n",
    "    # --- Data Cleaning (Common Logic) ---\n",
    "    # Convert confidence to numeric, coercing errors\n",
    "    if 'extracted_confidence' in combined_df.columns:\n",
    "        combined_df['confidence_numeric'] = pd.to_numeric(combined_df['extracted_confidence'], errors='coerce')\n",
    "    elif 'confidence' in combined_df.columns:\n",
    "         combined_df['confidence_numeric'] = pd.to_numeric(combined_df['confidence'], errors='coerce')\n",
    "    else:\n",
    "        print(\"Warning: No 'confidence' or 'extracted_confidence' column found for numeric conversion.\")\n",
    "        combined_df['confidence_numeric'] = np.nan # Add column as NaN\n",
    "\n",
    "    # Clean up answer strings (remove leading/trailing spaces, periods)\n",
    "    if 'extracted_answer' in combined_df.columns:\n",
    "        combined_df['answer_clean'] = combined_df['extracted_answer'].astype(str).str.strip().str.rstrip('.')\n",
    "    elif 'answer' in combined_df.columns:\n",
    "         combined_df['answer_clean'] = combined_df['answer'].astype(str).str.strip().str.rstrip('.')\n",
    "    else:\n",
    "        print(\"Warning: No 'answer' or 'extracted_answer' column found for cleaning.\")\n",
    "        combined_df['answer_clean'] = 'Unknown'\n",
    "\n",
    "    # --- Calculate Score (Common Logic, requires 'question_num' and 'answer_clean') ---\n",
    "    if Qs and 'question_num' in combined_df.columns and 'answer_clean' in combined_df.columns:\n",
    "        print(\"    Calculating MoralBench scores...\")\n",
    "        combined_df['score'] = combined_df.apply(lambda row: get_moralbench_scores(row['question_num'], row['answer_clean']), axis=1)\n",
    "        print(f\"    Score calculation done. NaN count: {combined_df['score'].isna().sum()}\")\n",
    "    else:\n",
    "        print(\"    Warning: Could not calculate scores ('question_num' or 'answer_clean' missing, or Qs handler failed).\")\n",
    "        combined_df['score'] = np.nan\n",
    "\n",
    "    print(f\"Finished loading and preprocessing for {results_dir}. Resulting dataframe shape: {combined_df.shape}\")\n",
    "    print(f\"Columns: {combined_df.columns.tolist()}\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi_agent_file = 'star_1_0.csv'\n",
    "print(f\"\\n--- Testing loading for specific file: {test_multi_agent_file} ---\")\n",
    "\n",
    "multi_agent_json_path = os.path.join(RESULTS_DIR_MULTI, test_multi_agent_file)\n",
    "multi_agent_df = pd.read_csv(multi_agent_json_path)\n",
    "multi_agent_df\n",
    "# map question_id to category\n",
    "def get_moral_category_from_id(question_id):\n",
    "    \"\"\"Gets the moral category name using the Question_Handler based on question ID.\"\"\"\n",
    "    return dict_map.get(extract_category_from_id(question_id), 'Unknown')\n",
    "\n",
    "multi_agent_df['category'] = multi_agent_df['question_id'].apply(get_moral_category_from_id)\n",
    "# if question num mod 9 is 1 - 5, database is MFQ_30, else 6_concepts\n",
    "multi_agent_df['database'] = multi_agent_df['question_num'].apply(lambda x: '6_concepts' if x % 11 in [1, 2, 3, 4, 5, 6] else 'MFQ_30')\n",
    "\n",
    "multi_agent_df.head()\n",
    "\n",
    "# get first agent responses\n",
    "\n",
    "all_scores = []\n",
    "for j, row in multi_agent_df.iterrows():\n",
    "    x = j\n",
    "    agent_responses = multi_agent_df['agent_responses'].iloc[x]\n",
    "    # Parse the agent responses\n",
    "    agent_responses_parsed = safe_literal_eval(agent_responses)\n",
    "    agent_responses_parsed\n",
    "\n",
    "    # run Qs.get_question_by_category_and_id('MFQ_30', 'harm_1') to get each score\n",
    "    question_id = multi_agent_df['question_id'].iloc[x]\n",
    "    database = multi_agent_df['database'].iloc[x]\n",
    "\n",
    "    score_answer = Qs.get_question_by_category_and_id(database, question_id)\n",
    "    if score_answer is None:\n",
    "        pass\n",
    "\n",
    "    scores = {}\n",
    "    confidences = {}\n",
    "    iteration = 0\n",
    "    scores_in_iteration = []\n",
    "    confidences_in_iteration = []\n",
    "    first_agent = agent_responses_parsed[0]['agent_model']\n",
    "    for i, obj in enumerate(agent_responses_parsed):\n",
    "        # get the answer\n",
    "        if agent_responses_parsed[i]['agent_model'] == first_agent and i > 0:\n",
    "            iteration += 1\n",
    "            average_score = np.mean(scores_in_iteration)\n",
    "            sem = np.std(scores_in_iteration) / np.sqrt(len(scores_in_iteration))\n",
    "            scores[iteration] = {\n",
    "                \"score\": average_score,\n",
    "                \"sem\": sem\n",
    "            }\n",
    "            average_confidence = np.mean(confidences_in_iteration)\n",
    "            confidences[iteration] = {\n",
    "                \"score\": average_confidence,\n",
    "                \"sem\": np.std(confidences_in_iteration) / np.sqrt(len(confidences_in_iteration))\n",
    "            }\n",
    "            scores_in_iteration = []\n",
    "            confidences_in_iteration = []\n",
    "        \n",
    "        answer = obj['extracted_answer']\n",
    "        if score_answer is None:\n",
    "            print(database, question_id)\n",
    "        if 'answers' in score_answer and answer in score_answer['answers']:\n",
    "            score = score_answer['answers'][answer]\n",
    "            scores_in_iteration.append(score)\n",
    "\n",
    "            try:\n",
    "                confidence = float(obj['extracted_confidence'])\n",
    "                confidences_in_iteration.append(confidence)\n",
    "            except (ValueError, TypeError):\n",
    "                # Handle cases where confidence is not a valid float\n",
    "                print(f\"Warning: Invalid confidence value '{obj['extracted_confidence']}' for answer '{answer}'. Skipping.\")\n",
    "    scores[iteration + 1] = {\n",
    "        \"score\": np.mean(scores_in_iteration), # Add the last iteration's\n",
    "        \"sem\": np.std(scores_in_iteration) / np.sqrt(len(scores_in_iteration))\n",
    "    }\n",
    "    confidences[iteration + 1] = {\n",
    "        \"score\": np.mean(confidences_in_iteration), # Add the last iteration's\n",
    "        \"sem\": np.std(confidences_in_iteration) / np.sqrt(len(confidences_in_iteration))\n",
    "    }\n",
    "        \n",
    "    all_scores.append({\n",
    "        'scores': scores,\n",
    "        'confidences': confidences,\n",
    "        'database': database,\n",
    "        'category': multi_agent_df['category'].iloc[x],\n",
    "    })\n",
    "all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Test loading and preprocessing for a specific multi-agent file from scratch, don't use any functions ---\n",
    "test_multi_agent_file = 'ring_1.csv'\n",
    "print(f\"\\n--- Testing loading for specific file: {test_multi_agent_file} ---\")\n",
    "\n",
    "multi_agent_json_path = os.path.join(RESULTS_DIR_MULTI, test_multi_agent_file)\n",
    "multi_agent_df = pd.read_csv(multi_agent_json_path)\n",
    "multi_agent_df\n",
    "# pop the first 2 rows\n",
    "multi_agent_df = multi_agent_df.iloc[2:]\n",
    "multi_agent_df\n",
    "# map question_id to category\n",
    "def get_moral_category_from_id(question_id):\n",
    "    \"\"\"Gets the moral category name using the Question_Handler based on question ID.\"\"\"\n",
    "    return dict_map.get(extract_category_from_id(question_id), 'Unknown')\n",
    "\n",
    "multi_agent_df['category'] = multi_agent_df['question_id'].apply(get_moral_category_from_id)\n",
    "# if question num mod 9 is 1 - 5, database is MFQ_30, else 6_concepts\n",
    "multi_agent_df['database'] = multi_agent_df['question_num'].apply(lambda x: '6_concepts' if x % 11 in [1, 2, 3, 4, 5, 6] else 'MFQ_30')\n",
    "\n",
    "multi_agent_df.head()\n",
    "\n",
    "# get first agent responses\n",
    "\n",
    "all_scores = []\n",
    "for j, row in multi_agent_df.iterrows():\n",
    "    x = j-2\n",
    "    agent_responses = multi_agent_df['agent_responses'].iloc[x]\n",
    "    # Parse the agent responses\n",
    "    agent_responses_parsed = safe_literal_eval(agent_responses)\n",
    "    agent_responses_parsed\n",
    "\n",
    "    # run Qs.get_question_by_category_and_id('MFQ_30', 'harm_1') to get each score\n",
    "    question_id = multi_agent_df['question_id'].iloc[x]\n",
    "    database = multi_agent_df['database'].iloc[x]\n",
    "\n",
    "    score_answer = Qs.get_question_by_category_and_id(database, question_id)\n",
    "    if score_answer is None:\n",
    "        pass\n",
    "\n",
    "    scores = {}\n",
    "    confidences = {}\n",
    "    iteration = 0\n",
    "    scores_in_iteration = []\n",
    "    confidences_in_iteration = []\n",
    "    first_agent = agent_responses_parsed[0]['agent_model']\n",
    "    for i, obj in enumerate(agent_responses_parsed):\n",
    "        # get the answer\n",
    "        if agent_responses_parsed[i]['agent_model'] == first_agent and i > 0:\n",
    "            iteration += 1\n",
    "            average_score = np.mean(scores_in_iteration)\n",
    "            sem = np.std(scores_in_iteration) / np.sqrt(len(scores_in_iteration))\n",
    "            scores[iteration] = {\n",
    "                \"score\": average_score,\n",
    "                \"sem\": sem\n",
    "            }\n",
    "            average_confidence = np.mean(confidences_in_iteration)\n",
    "            confidences[iteration] = {\n",
    "                \"score\": average_confidence,\n",
    "                \"sem\": np.std(confidences_in_iteration) / np.sqrt(len(confidences_in_iteration))\n",
    "            }\n",
    "            scores_in_iteration = []\n",
    "            confidences_in_iteration = []\n",
    "        \n",
    "        answer = obj['extracted_answer']\n",
    "        if score_answer is None:\n",
    "            print(database, question_id)\n",
    "        if 'answers' in score_answer and answer in score_answer['answers']:\n",
    "            score = score_answer['answers'][answer]\n",
    "            scores_in_iteration.append(score)\n",
    "\n",
    "            try:\n",
    "                confidence = float(obj['extracted_confidence'])\n",
    "                confidences_in_iteration.append(confidence)\n",
    "            except (ValueError, TypeError):\n",
    "                # Handle cases where confidence is not a valid float\n",
    "                print(f\"Warning: Invalid confidence value '{obj['extracted_confidence']}' for answer '{answer}'. Skipping.\")\n",
    "    scores[iteration + 1] = {\n",
    "        \"score\": np.mean(scores_in_iteration), # Add the last iteration's\n",
    "        \"sem\": np.std(scores_in_iteration) / np.sqrt(len(scores_in_iteration))\n",
    "    }\n",
    "    confidences[iteration + 1] = {\n",
    "        \"score\": np.mean(confidences_in_iteration), # Add the last iteration's\n",
    "        \"sem\": np.std(confidences_in_iteration) / np.sqrt(len(confidences_in_iteration))\n",
    "    }\n",
    "        \n",
    "    all_scores.append({\n",
    "        'scores': scores,\n",
    "        'confidences': confidences,\n",
    "        'database': database,\n",
    "        'category': multi_agent_df['category'].iloc[x],\n",
    "    })\n",
    "all_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "variable = 'confidences'\n",
    "\n",
    "figsize = (15, 8)\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 16,\n",
    "    'axes.labelsize': 14,\n",
    "    'xtick.labelsize': 12,\n",
    "    'ytick.labelsize': 12,\n",
    "    'legend.fontsize': 14,\n",
    "    'lines.linewidth': 2.5,\n",
    "    'figure.titlesize': 18\n",
    "})\n",
    "\n",
    "# Group data by database\n",
    "databases = {}\n",
    "for item in all_scores:\n",
    "    db = item['database']\n",
    "    if db not in databases:\n",
    "        databases[db] = {}\n",
    "    if item['category'] not in databases[db]:\n",
    "        databases[db][item['category']] = {\n",
    "            1: [],\n",
    "            2: [],\n",
    "            3: [],\n",
    "        }\n",
    "    for k, v in item[variable].items():\n",
    "        if k not in databases[db][item['category']]:\n",
    "            databases[db][item['category']][k] = []\n",
    "        databases[db][item['category']][k].append(v)\n",
    "# Calculate the mean and std for each category\n",
    "for db, categories in databases.items():\n",
    "    for category, scores in categories.items():\n",
    "        for k, v in scores.items():\n",
    "            if len(v) > 0:\n",
    "                print(f\"Database: {db}, Category: {category}, Iteration: {k}, Scores: {v}\") \n",
    "                sum = np.sum([x['score'] for x in v])\n",
    "                sem = np.sum([x['sem'] for x in v])\n",
    "                databases[db][category][k] = (sum, sem)\n",
    "            else:\n",
    "                databases[db][category][k] = (0, 0)\n",
    "\n",
    "\n",
    "def set_plot_style(title_fontsize=16, label_fontsize=14, tick_fontsize=12, \n",
    "                   legend_fontsize=14, line_width=2.5):\n",
    "    \"\"\"Set global matplotlib parameters for radar plots\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': label_fontsize,\n",
    "        'axes.titlesize': title_fontsize,\n",
    "        'axes.labelsize': label_fontsize,\n",
    "        'xtick.labelsize': tick_fontsize,\n",
    "        'ytick.labelsize': tick_fontsize,\n",
    "        'legend.fontsize': legend_fontsize,\n",
    "        'lines.linewidth': line_width,\n",
    "        'figure.titlesize': title_fontsize + 2\n",
    "    })\n",
    "\n",
    "def get_consistent_colors(iterations):\n",
    "    \"\"\"Get consistent colors for iterations across different plots\"\"\"\n",
    "    # Define a fixed set of colors\n",
    "    fixed_colors = {\n",
    "        1: '#1f77b4',  # blue\n",
    "        2: '#ff7f0e',  # orange\n",
    "        3: '#2ca02c',  # green\n",
    "        4: '#d62728',  # red\n",
    "        5: '#9467bd',  # purple\n",
    "        6: '#8c564b',  # brown\n",
    "    }\n",
    "    \n",
    "    # Create a mapping from iterations to colors\n",
    "    color_map = {}\n",
    "    for iteration in iterations:\n",
    "        color_map[iteration] = fixed_colors.get(iteration, f'C{iteration}')\n",
    "    \n",
    "    return color_map\n",
    "\n",
    "def plot_moral_radar_by_iteration(databases_data, figsize=(18, 9), show_sem=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Create radar plots for moral foundations by database and iteration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    databases_data : dict\n",
    "        Dictionary with database names as keys and category data as values\n",
    "    figsize : tuple, default=(18, 9)\n",
    "        Size of the figure (width, height)\n",
    "    show_sem : bool, default=True\n",
    "        Whether to show standard error as shaded region\n",
    "    save_path : str, optional\n",
    "        Path to save the plots, if None, plots will be displayed\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib figure\n",
    "        The created figure with subplots\n",
    "    \"\"\"\n",
    "    # Dictionary to map moral_category to display names\n",
    "    dict_map = {\n",
    "        'authority': 'Authority',\n",
    "        'fairness': 'Fairness',\n",
    "        'harm': 'Care',  # Care/Harm\n",
    "        'ingroup': 'Loyalty',  # Loyalty/Betrayal\n",
    "        'purity': 'Sanctity',  # Sanctity/Degradation\n",
    "        'liberty': 'Liberty'\n",
    "    }\n",
    "    \n",
    "    # Get list of databases\n",
    "    databases = list(databases_data.keys())\n",
    "    \n",
    "    # Get list of all iterations\n",
    "    all_iterations = set()\n",
    "    for db in databases:\n",
    "        for category in databases_data[db]:\n",
    "            all_iterations.update(databases_data[db][category].keys())\n",
    "    iterations = sorted(list(all_iterations))\n",
    "    \n",
    "    # Get consistent colors for iterations\n",
    "    color_map = get_consistent_colors(iterations)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(1, len(databases), figsize=figsize, subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # If there's only one database, make axes an array\n",
    "    if len(databases) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    # Process each database\n",
    "    for i, database in enumerate(databases):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Get data for this database\n",
    "        db_data = databases_data[database]\n",
    "        \n",
    "        # Get moral categories for this database\n",
    "        moral_cats = sorted(db_data.keys())\n",
    "        \n",
    "        # Number of moral categories\n",
    "        N = len(moral_cats)\n",
    "        \n",
    "        # Create angle values (in radians)\n",
    "        angles = np.linspace(0, 2*np.pi, N, endpoint=False).tolist()\n",
    "        \n",
    "        # Make the plot circular by appending the first angle again\n",
    "        angles += angles[:1]\n",
    "        \n",
    "        # Set up the axis\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        \n",
    "        # Map moral_category to display names and set as labels\n",
    "        labels = [dict_map.get(cat.lower(), cat) for cat in moral_cats]\n",
    "        ax.set_xticklabels(labels)\n",
    "        \n",
    "        # Set title for the subplot with padding to avoid overlap\n",
    "        ax.set_title(f\"Database: {database}\", pad=20)\n",
    "        \n",
    "        # Find max value for scaling\n",
    "        max_val = 0\n",
    "        for cat in moral_cats:\n",
    "            for it in iterations:\n",
    "                if it in db_data[cat]:\n",
    "                    if db_data[cat][it][0] + db_data[cat][it][1] > max_val:\n",
    "                        max_val = db_data[cat][it][0] + db_data[cat][it][1]\n",
    "            \n",
    "        # Set y-axis limits with some margin\n",
    "        ax.set_ylim(0, max_val * 1.2)\n",
    "        \n",
    "        # Add grid lines with improved labels\n",
    "        rticks = [max_val/5, 2*max_val/5, 3*max_val/5, 4*max_val/5, max_val]\n",
    "        ax.set_rticks(rticks)\n",
    "        # Format tick labels with proper precision\n",
    "        ax.set_yticklabels([f\"{tick:.3f}\" for tick in rticks])\n",
    "        ax.grid(True)\n",
    "        \n",
    "        # Plot each iteration\n",
    "        for iteration in iterations:\n",
    "            # Create arrays for means and SEMs\n",
    "            means = []\n",
    "            sems = []\n",
    "            \n",
    "            # Collect data for each category\n",
    "            for cat in moral_cats:\n",
    "                if iteration in db_data[cat]:\n",
    "                    mean, sem = db_data[cat][iteration]\n",
    "                    means.append(mean)\n",
    "                    sems.append(sem)\n",
    "                else:\n",
    "                    means.append(0)\n",
    "                    sems.append(0)\n",
    "            \n",
    "            # Make means circular for plotting\n",
    "            means_circular = np.append(means, means[0])\n",
    "            \n",
    "            # Plot the mean line with consistent color\n",
    "            ax.plot(angles, means_circular, color=color_map[iteration], \n",
    "                    label=f\"Iteration {iteration}\")\n",
    "            \n",
    "            # Add SEM shading if requested\n",
    "            if show_sem:\n",
    "                upper_bound = np.array(means) + np.array(sems)\n",
    "                lower_bound = np.array(means) - np.array(sems)\n",
    "                lower_bound = np.maximum(lower_bound, 0)  # Ensure no negative values\n",
    "                \n",
    "                # Make bounds circular\n",
    "                upper_bound_circular = np.append(upper_bound, upper_bound[0])\n",
    "                lower_bound_circular = np.append(lower_bound, lower_bound[0])\n",
    "                \n",
    "                # Create shaded region\n",
    "                ax.fill_between(angles, lower_bound_circular, upper_bound_circular, \n",
    "                                alpha=0.2, color=color_map[iteration])\n",
    "    \n",
    "    # Add a legend to the figure\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.02), \n",
    "               ncol=len(iterations))\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.08, 1, 0.95])\n",
    "    \n",
    "    # Add more space between subplots\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "    return fig\n",
    "\n",
    "# Example usage:\n",
    "set_plot_style(title_fontsize=16, label_fontsize=14, tick_fontsize=12, \n",
    "               legend_fontsize=14, line_width=2.5)\n",
    "fig = plot_moral_radar_by_iteration(databases, show_sem=True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sitewiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Imports and Setup\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "from collections import Counter, defaultdict\n",
    "import ast  # For safely evaluating string representations of lists/dicts\n",
    "\n",
    "# Define directories\n",
    "RESULTS_DIR = 'results' # Directory containing single-agent CSV results\n",
    "RESULTS_DIR_SINGLE = 'results'\n",
    "RESULTS_DIR_MULTI = 'results_multi'\n",
    "PLOT_DIR = 'plots'\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Define the datasets (categories) to include\n",
    "# These should match the category names returned by Question_Handler\n",
    "INCLUDED_DATASETS = ['MFQ_30', '6_concepts']\n",
    "\n",
    "# Create plots directory if it doesn't exist\n",
    "os.makedirs(PLOT_DIR, exist_ok=True)\n",
    "\n",
    "# Add MoralBench repo to path to import Question_Handler\n",
    "MORAL_BENCH_REPO_DIR = '../MoralBench_AgentEnsembles' # Adjust if needed\n",
    "moral_bench_path = os.path.abspath(MORAL_BENCH_REPO_DIR)\n",
    "if moral_bench_path not in sys.path:\n",
    "    sys.path.insert(0, moral_bench_path)\n",
    "print(f\"Using MoralBench repository at: {moral_bench_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated mapping based on standard MFT + Liberty\n",
    "dict_map = {\n",
    "   'authority': 'Authority',\n",
    "   'fairness': 'Fairness',\n",
    "   'harm': 'Harm', # Care/Harm\n",
    "   'ingroup': 'Loyalty', # Loyalty/Betrayal\n",
    "   'purity': 'Sanctity', # Sanctity/Degradation\n",
    "   'liberty': 'Liberty'\n",
    "}\n",
    "\n",
    "# Define the order for plotting categories\n",
    "PLOT_CATEGORIES = ['Harm', 'Fairness', 'Loyalty', 'Authority', 'Sanctity', 'Liberty']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Question Handler Definition (Copied for self-containment)\n",
    "# Note: Ideally, this would be imported from a shared module.\n",
    "class Question_Handler():\n",
    "  def __init__(self, repo_dir):\n",
    "    self.repo_dir = os.path.abspath(repo_dir) # Use absolute path\n",
    "    self.questions_dir = os.path.join(self.repo_dir, 'questions')\n",
    "    self.answers_dir = os.path.join(self.repo_dir, 'answers')\n",
    "    self.categories = self.list_categories()\n",
    "    self._build_question_map()\n",
    "\n",
    "  def _build_question_map(self):\n",
    "      \"\"\"Builds a map from question number to (category, index).\"\"\"\n",
    "      self.question_map = {}\n",
    "      current_question_num = 1\n",
    "      for category in self.categories:\n",
    "          count = self.get_question_count(category)\n",
    "          for i in range(count):\n",
    "              self.question_map[current_question_num] = {'category': category, 'index': i}\n",
    "              current_question_num += 1\n",
    "      self.total_questions = current_question_num - 1\n",
    "\n",
    "  def get_question_category_and_index(self, question_number):\n",
    "      \"\"\"Gets the category and index for a given question number.\"\"\"\n",
    "      return self.question_map.get(question_number)\n",
    "\n",
    "  def get_question_category(self, question_number):\n",
    "      \"\"\"Gets the category for a given question number.\"\"\"\n",
    "      mapping = self.question_map.get(question_number)\n",
    "      return mapping['category'] if mapping else None\n",
    "\n",
    "  def get_question_count(self, category_folder):\n",
    "      \"\"\"\n",
    "      Get the number of questions in a specific category folder.\n",
    "      \"\"\"\n",
    "      questions_path = os.path.join(self.questions_dir, category_folder)\n",
    "      if not os.path.exists(questions_path):\n",
    "          # print(f\"Warning: Category folder {questions_path} does not exist!\")\n",
    "          return 0\n",
    "      try:\n",
    "          question_files = [f for f in os.listdir(questions_path) if f.endswith('.txt')]\n",
    "          return len(question_files)\n",
    "      except FileNotFoundError:\n",
    "          # print(f\"Warning: Error accessing category folder {questions_path}.\")\n",
    "          return 0\n",
    "\n",
    "  def list_categories(self):\n",
    "      \"\"\"\n",
    "      List all available question categories.\n",
    "      \"\"\"\n",
    "      if not os.path.exists(self.questions_dir):\n",
    "          print(f\"Warning: Questions directory {self.questions_dir} not found!\")\n",
    "          return []\n",
    "      try:\n",
    "          categories = sorted([d for d in os.listdir(self.questions_dir) if os.path.isdir(os.path.join(self.questions_dir, d))])\n",
    "          return categories\n",
    "      except FileNotFoundError:\n",
    "           print(f\"Warning: Error listing categories in {self.questions_dir}.\")\n",
    "           return []\n",
    "\n",
    "  def load_question_answer(self, category_folder, index):\n",
    "      \"\"\"\n",
    "      Load a question and its possible answers using an index.\n",
    "      \"\"\"\n",
    "      questions_path = os.path.join(self.questions_dir, category_folder)\n",
    "      if not os.path.exists(questions_path):\n",
    "          # print(f\"Warning: Category folder {questions_path} does not exist!\")\n",
    "          return None\n",
    "\n",
    "      try:\n",
    "          # Get all question files and sort them\n",
    "          question_files = sorted([f for f in os.listdir(questions_path) if f.endswith('.txt')])\n",
    "\n",
    "          if index < 0 or index >= len(question_files):\n",
    "              # print(f\"Warning: Index {index} is out of range for category {category_folder}! Valid range: 0-{len(question_files)-1}\")\n",
    "              return None\n",
    "\n",
    "          # Get question filename and ID\n",
    "          question_file = question_files[index]\n",
    "          question_id = os.path.splitext(question_file)[0]\n",
    "\n",
    "          # Read question content\n",
    "          question_path = os.path.join(questions_path, question_file)\n",
    "          with open(question_path, 'r', encoding='utf-8') as f:\n",
    "              question_text = f.read()\n",
    "\n",
    "          # Load answers from JSON\n",
    "          answers_path = os.path.join(self.repo_dir, 'answers', f\"{category_folder}.json\") # Corrected path\n",
    "          question_answers = None\n",
    "          if os.path.exists(answers_path):\n",
    "              try:\n",
    "                  with open(answers_path, 'r', encoding='utf-8') as f:\n",
    "                      all_answers = json.load(f)\n",
    "                  question_answers = all_answers.get(question_id, {})\n",
    "              except json.JSONDecodeError:\n",
    "                  print(f\"Warning: Error decoding JSON from {answers_path}\")\n",
    "              except Exception as e:\n",
    "                  print(f\"Warning: Error reading answers file {answers_path}: {e}\")\n",
    "          # else:\n",
    "              # print(f\"Warning: Answers file {answers_path} for {category_folder} does not exist!\")\n",
    "\n",
    "          return {\n",
    "              'question_id': question_id,\n",
    "              'question_text': question_text,\n",
    "              'answers': question_answers\n",
    "          }\n",
    "      except FileNotFoundError:\n",
    "          # print(f\"Warning: Error accessing files in {questions_path}.\")\n",
    "          return None\n",
    "      except Exception as e:\n",
    "          print(f\"Warning: Unexpected error loading question {category_folder}/{index}: {e}\")\n",
    "          return None\n",
    "\n",
    "  def get_question(self, number):\n",
    "      \"\"\"Gets question data by absolute number.\"\"\"\n",
    "      mapping = self.get_question_category_and_index(number)\n",
    "      if mapping:\n",
    "          return self.load_question_answer(mapping['category'], mapping['index'])\n",
    "      else:\n",
    "          # print(f\"Warning: Question number {number} not found in map.\")\n",
    "          return None\n",
    "\n",
    "  def get_total_question_count(self):\n",
    "      \"\"\"Returns the total number of questions across all categories.\"\"\"\n",
    "      return self.total_questions\n",
    "\n",
    "# --- Initialize Question Handler ---\n",
    "try:\n",
    "    Qs = Question_Handler(MORAL_BENCH_REPO_DIR)\n",
    "    print(f\"Question Handler initialized. Found {Qs.get_total_question_count()} questions in {len(Qs.categories)} categories.\")\n",
    "    print(f\"Available categories: {Qs.categories}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing Question_Handler: {e}\")\n",
    "    Qs = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qs.get_question_category_and_index(88)\n",
    "\n",
    "# Qs.load_question_answer('MFQ_30', 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure they all have question_id\n",
    "if Qs:\n",
    "    for i in range(1,89):\n",
    "        q_info = Qs.get_question(i)  # Test the Question_Handler\n",
    "        print(f'{q_info.keys()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Helper Functions ---\n",
    "\n",
    "def extract_category_from_id(question_id):\n",
    "    \"\"\"Extracts the category name from the question_id (e.g., 'fairness_3' -> 'Fairness').\"\"\"\n",
    "    if not isinstance(question_id, str):\n",
    "        return 'Unknown'\n",
    "    match = re.match(r\"([a-zA-Z_]+)_?\\d*\", question_id)\n",
    "    if match:\n",
    "        category_name = match.group(1).replace('_', ' ').title()\n",
    "        # Handle specific known prefixes if needed\n",
    "        if category_name.startswith('Mfq '):\n",
    "             category_name = 'MFQ_30' # Keep original dataset name if preferred\n",
    "        elif category_name.startswith('6 Concepts'):\n",
    "             category_name = '6_concepts' # Keep original dataset name if preferred\n",
    "        return category_name.strip().lower()\n",
    "    return 'Unknown'\n",
    "\n",
    "def get_category_from_qnum(q_num): # this gets out the dataset category (eg MFQ_30)\n",
    "    \"\"\"Gets the category name using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        return Qs.get_question_category(q_num)\n",
    "    return 'Unknown' # Fallback if Qs is not initialized\n",
    "\n",
    "def get_moralbench_scores(question_number, answer):\n",
    "    \"\"\"Gets the moral score for a given question number and answer.\"\"\"\n",
    "    if Qs:\n",
    "        q_data = Qs.get_question(question_number)\n",
    "        if q_data and 'answers' in q_data and q_data['answers'] and answer in q_data['answers']:\n",
    "            return q_data['answers'][answer]\n",
    "        # Handle cases where answers might be missing or empty\n",
    "        # print(f\"Warning: No score found for Q{question_number}, Answer '{answer}'. Q_data: {q_data}\")\n",
    "    return None # Fallback if Qs is not initialized, answer not found, or answers missing\n",
    "\n",
    "def get_question_id_from_qnum(q_num): # this gets out the question_id (eg fairness_3)\n",
    "    \"\"\"Gets the question ID using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        q_info = Qs.get_question(q_num)\n",
    "        if q_info and 'question_id' in q_info:\n",
    "            return q_info['question_id']\n",
    "    return 'Unknown' # Fallback if Qs is not initialized or question not found\n",
    "\n",
    "def get_moral_category_from_qnum(q_num): # this gets out the moral category (eg Harm)\n",
    "    \"\"\"Gets the moral category name using the Question_Handler based on question number.\"\"\"\n",
    "    if Qs:\n",
    "        q_info = Qs.get_question(q_num)\n",
    "        if q_info and 'question_id' in q_info:\n",
    "            return extract_category_from_id(q_info['question_id'])\n",
    "    return 'Unknown' # Fallback if Qs is not initialized or question not found\n",
    "\n",
    "def safe_literal_eval(val):\n",
    "    \"\"\"Safely evaluate a string literal (list, dict). Returns None on error.\"\"\"\n",
    "    try:\n",
    "        return ast.literal_eval(val)\n",
    "    except (ValueError, SyntaxError, TypeError):\n",
    "        # print(f\"Warning: Could not parse value: {val}\")\n",
    "        return None\n",
    "\n",
    "def load_and_preprocess_data(results_dir):\n",
    "    \"\"\"Loads all CSV files from a directory and preprocesses them.\"\"\"\n",
    "    all_data_rows = []\n",
    "    print(f\"Checking directory: {results_dir}\")\n",
    "    if not os.path.exists(results_dir):\n",
    "        print(f\"Warning: Directory not found: {results_dir}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Found directory: {results_dir}. Searching for CSV files...\")\n",
    "    found_csv = False\n",
    "    for filename in os.listdir(results_dir):\n",
    "        if filename.endswith(\".csv\"):\n",
    "            found_csv = True\n",
    "            filepath = os.path.join(results_dir, filename)\n",
    "            print(f\"  Loading file: {filename}\")\n",
    "            try:\n",
    "                df_raw = pd.read_csv(filepath)\n",
    "                if df_raw.empty:\n",
    "                    print(f\"    Warning: File is empty: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Determine run type early based on columns\n",
    "                is_multi_agent = 'agent_responses' in df_raw.columns\n",
    "                is_single_agent = 'model_name' in df_raw.columns and 'run_index' in df_raw.columns and not is_multi_agent\n",
    "\n",
    "                # --- Process based on run type ---\n",
    "                if is_multi_agent:\n",
    "                    print(f\"    Processing as multi-agent data...\")\n",
    "                    df_raw['run_type'] = 'multi'\n",
    "                    # Explode the agent_responses column\n",
    "                    df_raw['agent_responses_parsed'] = df_raw['agent_responses'].apply(safe_literal_eval)\n",
    "                    df_exploded = df_raw.explode('agent_responses_parsed')\n",
    "                    df_exploded = df_exploded.dropna(subset=['agent_responses_parsed']) # Drop rows where parsing failed or was empty\n",
    "\n",
    "                    # Expand the dictionary into columns\n",
    "                    agent_data = pd.json_normalize(df_exploded['agent_responses_parsed'])\n",
    "                    df = pd.concat([df_exploded.drop(columns=['agent_responses', 'agent_responses_parsed']).reset_index(drop=True),\n",
    "                                    agent_data.reset_index(drop=True)], axis=1)\n",
    "                    print(f\"    Exploded agent responses. Shape after explode: {df.shape}\")\n",
    "\n",
    "                elif is_single_agent:\n",
    "                    print(f\"    Processing as single-agent data...\")\n",
    "                    df = df_raw.copy() # Use the raw df directly\n",
    "                    df['run_type'] = 'single'\n",
    "                else:\n",
    "                    print(f\"    Warning: Could not determine run type for {filename}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Add Category and Moral Category (Common Logic) ---\n",
    "                if 'question_num' in df.columns and Qs:\n",
    "                    df['category'] = df['question_num'].apply(get_category_from_qnum) # get the dataset category\n",
    "                    df['question_id'] = df['question_num'].apply(get_question_id_from_qnum) # get the question_id\n",
    "                    df['moral_category'] = df['question_num'].apply(get_moral_category_from_qnum) # get the moral category\n",
    "                    print(f\"    Extracted categories from 'question_num'. Unique values: {df['category'].unique()[:5]}...\")\n",
    "                elif 'question_id' in df.columns: # Fallback if question_num missing but question_id exists\n",
    "                     df['category'] = df['question_id'].apply(extract_category_from_id) # Attempt to get moral category\n",
    "                     df['moral_category'] = df['question_id'].apply(extract_category_from_id) # Use same logic for moral category\n",
    "                     # Try to infer dataset category if possible (might be less reliable)\n",
    "                     if Qs:\n",
    "                         # This requires reversing the map, might be slow/complex. Stick to moral category for now.\n",
    "                         print(\"    Warning: 'question_num' missing. Using 'question_id' for moral category. Dataset category might be inaccurate.\")\n",
    "                     else:\n",
    "                         print(\"    Warning: 'question_num' missing and Qs handler failed. Using 'question_id' for moral category.\")\n",
    "                else:\n",
    "                    df['category'] = 'Unknown'\n",
    "                    df['moral_category'] = 'Unknown'\n",
    "                    df['question_id'] = 'Unknown'\n",
    "                    print(\"    Warning: Could not determine category ('question_num' or 'question_id' missing, or Qs handler failed).\")\n",
    "\n",
    "                # --- Filter by Dataset ---\n",
    "                initial_rows = len(df)\n",
    "                df = df[df['category'].isin(INCLUDED_DATASETS)]\n",
    "                filtered_rows = len(df)\n",
    "                print(f\"    Filtered by INCLUDED_DATASETS ({INCLUDED_DATASETS}). Kept {filtered_rows}/{initial_rows} rows.\")\n",
    "\n",
    "                if not df.empty:\n",
    "                    all_data_rows.append(df)\n",
    "                else:\n",
    "                    print(f\"    Info: No rows remaining after filtering for datasets {INCLUDED_DATASETS}.\")\n",
    "\n",
    "            except pd.errors.EmptyDataError:\n",
    "                print(f\"    Warning: Skipping empty file: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error loading or processing file {filename}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc() # Print full traceback for debugging\n",
    "\n",
    "    if not found_csv:\n",
    "        print(f\"Warning: No CSV files found in directory: {results_dir}\")\n",
    "\n",
    "    if not all_data_rows:\n",
    "        print(f\"No data loaded or retained from {results_dir} after processing and filtering. Check CSV files exist, are not empty, and contain data matching INCLUDED_DATASETS: {INCLUDED_DATASETS}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"Concatenating data from {len(all_data_rows)} files/dataframes.\")\n",
    "    combined_df = pd.concat(all_data_rows, ignore_index=True)\n",
    "\n",
    "    # --- Data Cleaning (Common Logic) ---\n",
    "    # Convert confidence to numeric, coercing errors\n",
    "    if 'extracted_confidence' in combined_df.columns:\n",
    "        combined_df['confidence_numeric'] = pd.to_numeric(combined_df['extracted_confidence'], errors='coerce')\n",
    "    elif 'confidence' in combined_df.columns:\n",
    "         combined_df['confidence_numeric'] = pd.to_numeric(combined_df['confidence'], errors='coerce')\n",
    "    else:\n",
    "        print(\"Warning: No 'confidence' or 'extracted_confidence' column found for numeric conversion.\")\n",
    "        combined_df['confidence_numeric'] = np.nan # Add column as NaN\n",
    "\n",
    "    # Clean up answer strings (remove leading/trailing spaces, periods)\n",
    "    if 'extracted_answer' in combined_df.columns:\n",
    "        combined_df['answer_clean'] = combined_df['extracted_answer'].astype(str).str.strip().str.rstrip('.')\n",
    "    elif 'answer' in combined_df.columns:\n",
    "         combined_df['answer_clean'] = combined_df['answer'].astype(str).str.strip().str.rstrip('.')\n",
    "    else:\n",
    "        print(\"Warning: No 'answer' or 'extracted_answer' column found for cleaning.\")\n",
    "        combined_df['answer_clean'] = 'Unknown'\n",
    "\n",
    "    # --- Calculate Score (Common Logic, requires 'question_num' and 'answer_clean') ---\n",
    "    if Qs and 'question_num' in combined_df.columns and 'answer_clean' in combined_df.columns:\n",
    "        print(\"    Calculating MoralBench scores...\")\n",
    "        combined_df['score'] = combined_df.apply(lambda row: get_moralbench_scores(row['question_num'], row['answer_clean']), axis=1)\n",
    "        print(f\"    Score calculation done. NaN count: {combined_df['score'].isna().sum()}\")\n",
    "    else:\n",
    "        print(\"    Warning: Could not calculate scores ('question_num' or 'answer_clean' missing, or Qs handler failed).\")\n",
    "        combined_df['score'] = np.nan\n",
    "\n",
    "    print(f\"Finished loading and preprocessing for {results_dir}. Resulting dataframe shape: {combined_df.shape}\")\n",
    "    print(f\"Columns: {combined_df.columns.tolist()}\")\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the category extraction\n",
    "qinfo = Qs.get_question(40)\n",
    "get_category_from_qnum(40)\n",
    "get_moral_category_from_qnum(40)\n",
    "print(f\"Question ID: {qinfo['question_id']}, Category: {get_category_from_qnum(40)}, Moral Category: {get_moral_category_from_qnum(40)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloading single\n",
    "single_agent_df = load_and_preprocess_data(RESULTS_DIR_SINGLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_agent_df['category'].unique()\n",
    "single_agent_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_agent_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotting Functions ---\n",
    "\n",
    "def plot_answer_distribution(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of answers (A vs B) across all relevant runs.\"\"\"\n",
    "    if df.empty or 'answer_clean' not in df.columns:\n",
    "        print(\"Cannot plot answer distribution: DataFrame is empty or 'answer_clean' column missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Filter for only 'A' and 'B' answers for clarity\n",
    "    plot_data = df[df['answer_clean'].isin(['A', 'B'])]\n",
    "    if plot_data.empty:\n",
    "        print(\"Cannot plot answer distribution: No 'A' or 'B' answers found after cleaning.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    sns.countplot(data=plot_data, x='answer_clean', order=['A', 'B'])\n",
    "    plt.title('Overall Distribution of Answers (A vs B)')\n",
    "    plt.xlabel('Answer')\n",
    "    plt.ylabel('Count')\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved answer distribution plot to {plot_filename}\")\n",
    "\n",
    "def plot_confidence_distribution(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of confidence scores.\"\"\"\n",
    "    if df.empty or 'confidence_numeric' not in df.columns:\n",
    "        print(\"Cannot plot confidence distribution: DataFrame is empty or 'confidence_numeric' column missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    # Filter out NaN values before plotting\n",
    "    plot_data = df.dropna(subset=['confidence_numeric'])\n",
    "    if plot_data.empty:\n",
    "        print(\"Cannot plot confidence distribution: No valid numeric confidence scores found.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    sns.histplot(data=plot_data, x='confidence_numeric', bins=np.arange(-0.5, 6.5, 1), kde=False)\n",
    "    plt.title('Distribution of Confidence Scores')\n",
    "    plt.xlabel('Confidence Score (0-5)')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(range(6)) # Ensure ticks are 0, 1, 2, 3, 4, 5\n",
    "    plt.xlim(-0.5, 5.5)\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved confidence distribution plot to {plot_filename}\")\n",
    "\n",
    "def plot_answer_by_category(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of answers (A vs B) for each category.\"\"\"\n",
    "    if df.empty or 'answer_clean' not in df.columns or 'category' not in df.columns:\n",
    "        print(\"Cannot plot answer by category: DataFrame empty or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plot_data = df[df['answer_clean'].isin(['A', 'B']) & df['category'].isin(INCLUDED_DATASETS)]\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot answer by category: No 'A' or 'B' answers found for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    category_order = sorted([cat for cat in plot_data['category'].unique() if cat in INCLUDED_DATASETS])\n",
    "    if not category_order:\n",
    "        print(f\"Cannot plot answer by category: No data found for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    sns.countplot(data=plot_data, x='category', hue='answer_clean', order=category_order, hue_order=['A', 'B'])\n",
    "    plt.title('Answer Distribution (A vs B) by Category')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.legend(title='Answer')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved answer by category plot to {plot_filename}\")\n",
    "\n",
    "def plot_confidence_by_category(df, plot_filename):\n",
    "    \"\"\"Plots the average confidence score for each category.\"\"\"\n",
    "    if df.empty or 'confidence_numeric' not in df.columns or 'category' not in df.columns:\n",
    "        print(\"Cannot plot confidence by category: DataFrame empty or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plot_data = df.dropna(subset=['confidence_numeric'])\n",
    "    plot_data = plot_data[plot_data['category'].isin(INCLUDED_DATASETS)]\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot confidence by category: No valid numeric confidence scores found for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    category_order = sorted([cat for cat in plot_data['category'].unique() if cat in INCLUDED_DATASETS])\n",
    "    if not category_order:\n",
    "         print(f\"Cannot plot confidence by category: No data found for included datasets {INCLUDED_DATASETS}.\")\n",
    "         plt.close()\n",
    "         return\n",
    "    sns.barplot(data=plot_data, x='category', y='confidence_numeric', order=category_order, estimator=np.mean, errorbar='sd') # Show mean and std dev\n",
    "    plt.title('Average Confidence Score by Category')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Average Confidence Score (0-5)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 5) # Set y-axis limits\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved confidence by category plot to {plot_filename}\")\n",
    "\n",
    "def plot_answer_by_model(df, plot_filename):\n",
    "    \"\"\"Plots the distribution of answers (A vs B) for each model (single agent runs).\"\"\"\n",
    "    df_single = df[(df['run_type'] == 'single') & (df['category'].isin(INCLUDED_DATASETS))]\n",
    "    if df_single.empty or 'answer_clean' not in df_single.columns or 'model_name' not in df_single.columns:\n",
    "        print(f\"Cannot plot answer by model: No single-agent data for included datasets {INCLUDED_DATASETS} or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plot_data = df_single[df_single['answer_clean'].isin(['A', 'B'])]\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot answer by model: No 'A' or 'B' answers found in single-agent data for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    model_order = sorted(plot_data['model_name'].unique())\n",
    "    sns.countplot(data=plot_data, y='model_name', hue='answer_clean', order=model_order, hue_order=['A', 'B'])\n",
    "    plt.title('Answer Distribution (A vs B) by Model (Single Agent Runs)')\n",
    "    plt.xlabel('Count')\n",
    "    plt.ylabel('Model Name')\n",
    "    plt.legend(title='Answer')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved answer by model plot to {plot_filename}\")\n",
    "\n",
    "def plot_confidence_by_model(df, plot_filename):\n",
    "    \"\"\"Plots the average confidence score for each model (single agent runs).\"\"\"\n",
    "    df_single = df[(df['run_type'] == 'single') & (df['category'].isin(INCLUDED_DATASETS))]\n",
    "    if df_single.empty or 'confidence_numeric' not in df_single.columns or 'model_name' not in df_single.columns:\n",
    "        print(f\"Cannot plot confidence by model: No single-agent data for included datasets {INCLUDED_DATASETS} or required columns missing.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plot_data = df_single.dropna(subset=['confidence_numeric'])\n",
    "    if plot_data.empty:\n",
    "        print(f\"Cannot plot confidence by model: No valid numeric confidence scores found in single-agent data for included datasets {INCLUDED_DATASETS}.\")\n",
    "        plt.close()\n",
    "        return\n",
    "    model_order = sorted(plot_data['model_name'].unique())\n",
    "    sns.barplot(data=plot_data, y='model_name', x='confidence_numeric', order=model_order, estimator=np.mean, errorbar='sd')\n",
    "    plt.title('Average Confidence Score by Model (Single Agent Runs)')\n",
    "    plt.xlabel('Average Confidence Score (0-5)')\n",
    "    plt.ylabel('Model Name')\n",
    "    plt.xlim(0, 5)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close()\n",
    "    print(f\"Saved confidence by model plot to {plot_filename}\")\n",
    "\n",
    "def plot_moral_radar_convergence(df, metric='score', show_sem=False, figsize=(18, 9), save_path=None):\n",
    "    \"\"\"\n",
    "    Create radar plots for moral foundations by convergence loop (message_index) and dataset category.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame containing multi-agent results, exploded by agent response.\n",
    "        Requires columns: 'message_index', 'category', 'moral_category', 'question_num', metric ('score' or 'confidence_numeric').\n",
    "    metric : str, default='score'\n",
    "        Column to plot ('score' or 'confidence_numeric').\n",
    "    show_sem : bool, default=False\n",
    "        Whether to show standard error as shaded region.\n",
    "    figsize : tuple, default=(18, 9)\n",
    "        Size of the figure (width, height).\n",
    "    save_path : str, optional\n",
    "        Path to save the plots. If None, plots will be displayed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib figure\n",
    "        The created figure with subplots for each dataset category.\n",
    "    \"\"\"\n",
    "    if df.empty or df['run_type'].iloc[0] != 'multi':\n",
    "        print(\"Warning: plot_moral_radar_convergence requires a non-empty multi-agent DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    # Check required columns\n",
    "    required_cols = ['message_index', 'category', 'moral_category', 'question_num', metric]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: DataFrame missing one or more required columns for radar plot: {required_cols}\")\n",
    "        return None\n",
    "\n",
    "    # Dictionary to map moral_category to display names\n",
    "    dict_map = {\n",
    "        'authority': 'Authority', 'fairness': 'Fairness', 'harm': 'Care',\n",
    "        'ingroup': 'Loyalty', 'purity': 'Sanctity', 'liberty': 'Liberty'\n",
    "    }\n",
    "\n",
    "    # Filter for included datasets\n",
    "    df_filtered = df[df['category'].isin(INCLUDED_DATASETS)].copy()\n",
    "    if df_filtered.empty:\n",
    "        print(f\"Warning: No data found for included datasets {INCLUDED_DATASETS} in the multi-agent data.\")\n",
    "        return None\n",
    "\n",
    "    # Calculate mean and SEM *per agent response* first (grouped by everything including agent)\n",
    "    # Then average these agent means/SEMs per loop/question/category/moral_category\n",
    "    # This approach assumes each agent's response within a loop is an independent sample for that loop's state.\n",
    "\n",
    "    # Step 1: Calculate mean and SEM for the metric across runs for each agent, loop, question, etc.\n",
    "    # Group by everything that defines a unique data point *before* averaging across agents within a loop\n",
    "    grouping_cols = ['message_index', 'category', 'moral_category', 'question_num', 'agent_model'] # Include agent_model if you want per-agent stats first\n",
    "    # Let's average across agents *within* the same loop, question, category, moral_category\n",
    "    grouping_cols_agg = ['message_index', 'category', 'moral_category', 'question_num']\n",
    "\n",
    "    print(f\"Calculating stats grouped by: {grouping_cols_agg}\")\n",
    "    # Calculate mean and SEM of the metric across *agents* for each question/loop/category/moral_cat\n",
    "    question_stats = df_filtered.groupby(grouping_cols_agg).agg(\n",
    "        mean_metric=(metric, lambda x: np.nanmean(x)),\n",
    "        sem_metric=(metric, lambda x: np.nanstd(x, ddof=1) / np.sqrt(np.sum(~np.isnan(x))) if np.sum(~np.isnan(x)) > 0 else 0)\n",
    "    ).reset_index()\n",
    "    print(f\"Calculated question stats. Shape: {question_stats.shape}\")\n",
    "    # print(question_stats.head()) # Debugging\n",
    "\n",
    "    # Step 2: Sum the means across questions for each loop, category, moral_category\n",
    "    summed_means = question_stats.groupby(['message_index', 'category', 'moral_category'])['mean_metric'].sum().reset_index()\n",
    "    print(f\"Calculated summed means. Shape: {summed_means.shape}\")\n",
    "    # print(summed_means.head()) # Debugging\n",
    "\n",
    "    # Step 3: Calculate the propagated SEM for the sums (sqrt of sum of squared SEMs)\n",
    "    summed_sems = question_stats.groupby(['message_index', 'category', 'moral_category'])['sem_metric'].apply(\n",
    "        lambda x: np.sqrt(np.sum(x**2))\n",
    "    ).reset_index()\n",
    "    print(f\"Calculated summed SEMs. Shape: {summed_sems.shape}\")\n",
    "    # print(summed_sems.head()) # Debugging\n",
    "\n",
    "\n",
    "    # Merge the means and SEMs\n",
    "    final_data = pd.merge(summed_means, summed_sems, on=['message_index', 'category', 'moral_category'])\n",
    "    final_data = final_data.rename(columns={'mean_metric': 'mean', 'sem_metric': 'sem'}) # Rename for consistency\n",
    "    print(f\"Final aggregated data shape: {final_data.shape}\")\n",
    "    # print(final_data.head()) # Debugging\n",
    "\n",
    "    # Get unique dataset categories and loops (message indices)\n",
    "    dataset_categories = sorted(final_data['category'].unique())\n",
    "    loops = sorted(final_data['message_index'].unique())\n",
    "\n",
    "    if not dataset_categories:\n",
    "        print(\"Error: No dataset categories found in the aggregated data.\")\n",
    "        return None\n",
    "    if not loops:\n",
    "        print(\"Error: No message indices (loops) found in the aggregated data.\")\n",
    "        return None\n",
    "\n",
    "    # Create a colormap for the loops\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(loops))) # Use viridis for sequential data\n",
    "\n",
    "    # Create figure with subplots\n",
    "    n_cols = len(dataset_categories)\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=figsize, subplot_kw=dict(polar=True))\n",
    "\n",
    "    # If there's only one category, make axes an array\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Process each dataset category\n",
    "    for i, d_category in enumerate(dataset_categories):\n",
    "        ax = axes[i]\n",
    "        print(f\"\\nPlotting category: {d_category}\")\n",
    "\n",
    "        # Filter data for this dataset category\n",
    "        cat_data = final_data[final_data['category'] == d_category]\n",
    "        if cat_data.empty:\n",
    "            print(f\"  No data for category {d_category}, skipping subplot.\")\n",
    "            ax.set_title(f\"Category: {d_category}\\n(No Data)\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        # Get unique moral categories present in this dataset category's data\n",
    "        moral_cats = sorted(cat_data['moral_category'].unique())\n",
    "        if not moral_cats:\n",
    "             print(f\"  No moral categories found for dataset {d_category}, skipping subplot.\")\n",
    "             ax.set_title(f\"Category: {d_category}\\n(No Moral Categories)\")\n",
    "             ax.set_xticks([])\n",
    "             ax.set_yticks([])\n",
    "             continue\n",
    "\n",
    "        # Map moral categories using dict_map, handle missing keys\n",
    "        moral_cats_mapped = [dict_map.get(mc, mc.title()) for mc in moral_cats]\n",
    "\n",
    "        # Number of moral categories\n",
    "        N = len(moral_cats)\n",
    "\n",
    "        # Create angle values (in radians)\n",
    "        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "        angles += angles[:1] # Make the plot circular\n",
    "\n",
    "        # Set up the axis\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(moral_cats_mapped)\n",
    "        ax.tick_params(axis='x', pad=10) # Add padding to x-tick labels\n",
    "\n",
    "        # Set title for the subplot\n",
    "        ax.set_title(f\"Dataset: {d_category}\", pad=25) # Increased padding\n",
    "\n",
    "        # Find max value for scaling across all loops for this category\n",
    "        max_val = cat_data['mean'].max()\n",
    "        if show_sem:\n",
    "            max_val = max(max_val, (cat_data['mean'] + cat_data['sem']).max())\n",
    "        max_val = max(max_val, 0.1) # Ensure max_val is not zero\n",
    "\n",
    "        # Set y-axis limits with some margin\n",
    "        ax.set_ylim(0, max_val * 1.2)\n",
    "\n",
    "        # Add grid lines with improved labels\n",
    "        num_rticks = 5\n",
    "        rticks = np.linspace(0, max_val, num_rticks + 1)[1:] # Avoid 0 tick label overlap\n",
    "        ax.set_rticks(rticks)\n",
    "        # Format tick labels with appropriate precision\n",
    "        tick_format = \".2f\" if max_val < 10 else \".1f\"\n",
    "        ax.set_yticklabels([f\"{tick:{tick_format}}\" for tick in rticks])\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Plot each loop (message_index)\n",
    "        for j, loop_index in enumerate(loops):\n",
    "            # Filter data for this loop\n",
    "            loop_data = cat_data[cat_data['message_index'] == loop_index]\n",
    "\n",
    "            if loop_data.empty:\n",
    "                print(f\"  No data for loop {loop_index} in category {d_category}\")\n",
    "                continue\n",
    "\n",
    "            # Create ordered arrays of means and SEMs based on moral_cats order\n",
    "            means = []\n",
    "            sems = []\n",
    "            for mc in moral_cats:\n",
    "                loop_mc_data = loop_data[loop_data['moral_category'] == mc]\n",
    "                if not loop_mc_data.empty:\n",
    "                    means.append(loop_mc_data['mean'].iloc[0])\n",
    "                    sems.append(loop_mc_data['sem'].iloc[0])\n",
    "                else:\n",
    "                    means.append(0) # Append 0 if no data for this moral category in this loop\n",
    "                    sems.append(0)\n",
    "\n",
    "            # Make means circular for plotting\n",
    "            means_circular = np.append(means, means[0])\n",
    "\n",
    "            # Plot the mean line\n",
    "            # Use message_index + 1 for 1-based loop labeling if desired, or just message_index\n",
    "            label = f\"Loop {loop_index}\" # Or loop_index + 1\n",
    "            ax.plot(angles, means_circular, color=colors[j], linestyle='-', marker='o', markersize=4, label=label)\n",
    "\n",
    "            # Add SEM shading if requested\n",
    "            if show_sem:\n",
    "                upper_bound = np.array(means) + np.array(sems)\n",
    "                lower_bound = np.array(means) - np.array(sems)\n",
    "                lower_bound = np.maximum(lower_bound, 0)  # Ensure no negative values\n",
    "\n",
    "                # Make bounds circular\n",
    "                upper_bound_circular = np.append(upper_bound, upper_bound[0])\n",
    "                lower_bound_circular = np.append(lower_bound, lower_bound[0])\n",
    "\n",
    "                # Create shaded region\n",
    "                ax.fill_between(angles, lower_bound_circular, upper_bound_circular,\n",
    "                                alpha=0.15, color=colors[j]) # Slightly less alpha\n",
    "\n",
    "    # Add a legend to the figure\n",
    "    handles, labels = axes[0].get_legend_handles_labels() # Get legend items from the first axis\n",
    "    if handles: # Only add legend if there are items to show\n",
    "        fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.01), ncol=len(loops)) # Adjusted position\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95]) # Adjust rect for title and legend\n",
    "\n",
    "    # Add more space between subplots if needed\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4) # Increased wspace\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nSaved convergence radar plot to {save_path}\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "# --- Plotting for Single Agent (Modified Radar Plot by Iteration) ---\n",
    "\n",
    "def set_plot_style(title_fontsize=16, label_fontsize=14, tick_fontsize=12,\n",
    "                   legend_fontsize=14, line_width=2.5):\n",
    "    \"\"\"Set global matplotlib parameters for radar plots\"\"\"\n",
    "    plt.rcParams.update({\n",
    "        'font.size': label_fontsize,\n",
    "        'axes.titlesize': title_fontsize,\n",
    "        'axes.labelsize': label_fontsize,\n",
    "        'xtick.labelsize': tick_fontsize,\n",
    "        'ytick.labelsize': tick_fontsize,\n",
    "        'legend.fontsize': legend_fontsize,\n",
    "        'lines.linewidth': line_width,\n",
    "        'figure.titlesize': title_fontsize + 2\n",
    "    })\n",
    "\n",
    "def plot_moral_radar(df, metric='score', show_sem=False, figsize=(18, 9), save_path=None):\n",
    "    \"\"\"\n",
    "    Create radar plots for moral foundations by run index (iteration) and category (for single agent data).\n",
    "    Each line represents a run index, averaged across all models for that run index.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        DataFrame with required columns ('run_index', 'category', 'moral_category', 'question_num', metric)\n",
    "        Assumes 'run_type' is 'single'.\n",
    "    metric : str, default='score'\n",
    "        Column to plot ('score' or 'confidence_numeric')\n",
    "    show_sem : bool, default=False\n",
    "        Whether to show standard error as shaded region\n",
    "    figsize : tuple, default=(18, 9)\n",
    "        Size of the figure (width, height)\n",
    "    save_path : str, optional\n",
    "        Path to save the plots, if None, plots will be displayed\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    fig : matplotlib figure\n",
    "        The created figure with subplots for each dataset category.\n",
    "    \"\"\"\n",
    "    if df.empty or df['run_type'].iloc[0] != 'single':\n",
    "        print(\"Warning: plot_moral_radar requires a non-empty single-agent DataFrame.\")\n",
    "        return None\n",
    "\n",
    "    # Check required columns\n",
    "    required_cols = ['run_index', 'category', 'moral_category', 'question_num', metric]\n",
    "    if not all(col in df.columns for col in required_cols):\n",
    "        print(f\"Error: DataFrame missing one or more required columns for radar plot: {required_cols}\")\n",
    "        return None\n",
    "\n",
    "    # Dictionary to map moral_category to display names\n",
    "    dict_map = {\n",
    "        'authority': 'Authority', 'fairness': 'Fairness', 'harm': 'Care',\n",
    "        'ingroup': 'Loyalty', 'purity': 'Sanctity', 'liberty': 'Liberty'\n",
    "    }\n",
    "\n",
    "    # Filter for included datasets\n",
    "    df_filtered = df[df['category'].isin(INCLUDED_DATASETS)].copy()\n",
    "    if df_filtered.empty:\n",
    "        print(f\"Warning: No data found for included datasets {INCLUDED_DATASETS} in the single-agent data.\")\n",
    "        return None\n",
    "\n",
    "    # Ensure metric column is numeric\n",
    "    df_filtered[metric] = pd.to_numeric(df_filtered[metric], errors='coerce')\n",
    "\n",
    "    # First, calculate mean and SEM for each question, by run_index, category, moral_category\n",
    "    # This averages across models for a specific run_index\n",
    "    grouping_cols_q = ['run_index', 'category', 'moral_category', 'question_num']\n",
    "    question_stats = df_filtered.groupby(grouping_cols_q).agg(\n",
    "        mean_metric=(metric, lambda x: np.nanmean(x)),\n",
    "        sem_metric=(metric, lambda x: np.nanstd(x, ddof=1) / np.sqrt(np.sum(~np.isnan(x))) if np.sum(~np.isnan(x)) > 0 else 0)\n",
    "    ).reset_index()\n",
    "\n",
    "    # Now, sum the means across questions for each run_index, category, moral_category\n",
    "    grouping_cols_agg = ['run_index', 'category', 'moral_category']\n",
    "    summed_means = question_stats.groupby(grouping_cols_agg)['mean_metric'].sum().reset_index()\n",
    "\n",
    "    # Calculate the propagated SEM for the sums (sqrt of sum of squared SEMs)\n",
    "    summed_sems = question_stats.groupby(grouping_cols_agg)['sem_metric'].apply(\n",
    "        lambda x: np.sqrt(np.sum(x**2))\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge the means and SEMs\n",
    "    final_data = pd.merge(summed_means, summed_sems, on=grouping_cols_agg)\n",
    "    final_data = final_data.rename(columns={'mean_metric': 'mean', 'sem_metric': 'sem'}) # Rename for consistency\n",
    "\n",
    "    # Get unique dataset categories and run indices\n",
    "    dataset_categories = sorted(final_data['category'].unique())\n",
    "    run_indices = sorted(final_data['run_index'].unique())\n",
    "\n",
    "    if not dataset_categories:\n",
    "        print(\"Error: No dataset categories found in the aggregated data.\")\n",
    "        return None\n",
    "    if not run_indices:\n",
    "        print(\"Error: No run indices found in the aggregated data.\")\n",
    "        return None\n",
    "\n",
    "    # Create a colormap for the run indices\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(run_indices)))\n",
    "\n",
    "    # Create figure with subplots\n",
    "    n_cols = len(dataset_categories)\n",
    "    fig, axes = plt.subplots(1, n_cols, figsize=figsize, subplot_kw=dict(polar=True))\n",
    "\n",
    "    if n_cols == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Process each dataset category\n",
    "    for i, d_category in enumerate(dataset_categories):\n",
    "        ax = axes[i]\n",
    "        print(f\"\\nPlotting category: {d_category}\")\n",
    "\n",
    "        cat_data = final_data[final_data['category'] == d_category]\n",
    "        if cat_data.empty:\n",
    "            print(f\"  No data for category {d_category}, skipping subplot.\")\n",
    "            ax.set_title(f\"Category: {d_category}\\n(No Data)\")\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            continue\n",
    "\n",
    "        moral_cats = sorted(cat_data['moral_category'].unique())\n",
    "        if not moral_cats:\n",
    "             print(f\"  No moral categories found for dataset {d_category}, skipping subplot.\")\n",
    "             ax.set_title(f\"Category: {d_category}\\n(No Moral Categories)\")\n",
    "             ax.set_xticks([])\n",
    "             ax.set_yticks([])\n",
    "             continue\n",
    "\n",
    "        moral_cats_mapped = [dict_map.get(mc, mc.title()) for mc in moral_cats]\n",
    "        N = len(moral_cats)\n",
    "        angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()\n",
    "        angles += angles[:1]\n",
    "\n",
    "        ax.set_xticks(angles[:-1])\n",
    "        ax.set_xticklabels(moral_cats_mapped)\n",
    "        ax.tick_params(axis='x', pad=10)\n",
    "\n",
    "        ax.set_title(f\"Dataset: {d_category}\", pad=25)\n",
    "\n",
    "        max_val = cat_data['mean'].max()\n",
    "        if show_sem:\n",
    "            max_val = max(max_val, (cat_data['mean'] + cat_data['sem']).max())\n",
    "        max_val = max(max_val, 0.1)\n",
    "\n",
    "        ax.set_ylim(0, max_val * 1.2)\n",
    "\n",
    "        num_rticks = 5\n",
    "        rticks = np.linspace(0, max_val, num_rticks + 1)[1:]\n",
    "        ax.set_rticks(rticks)\n",
    "        tick_format = \".2f\" if max_val < 10 else \".1f\"\n",
    "        ax.set_yticklabels([f\"{tick:{tick_format}}\" for tick in rticks])\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Plot each run index\n",
    "        for j, run_idx in enumerate(run_indices):\n",
    "            run_data = cat_data[cat_data['run_index'] == run_idx]\n",
    "\n",
    "            if run_data.empty:\n",
    "                print(f\"  No data for run index {run_idx} in category {d_category}\")\n",
    "                continue\n",
    "\n",
    "            means = []\n",
    "            sems = []\n",
    "            for mc in moral_cats:\n",
    "                run_mc_data = run_data[run_data['moral_category'] == mc]\n",
    "                if not run_mc_data.empty:\n",
    "                    means.append(run_mc_data['mean'].iloc[0])\n",
    "                    sems.append(run_mc_data['sem'].iloc[0])\n",
    "                else:\n",
    "                    means.append(0)\n",
    "                    sems.append(0)\n",
    "\n",
    "            means_circular = np.append(means, means[0])\n",
    "            label = f\"Run {run_idx}\"\n",
    "            ax.plot(angles, means_circular, color=colors[j], linestyle='-', marker='o', markersize=4, label=label)\n",
    "\n",
    "            if show_sem:\n",
    "                upper_bound = np.array(means) + np.array(sems)\n",
    "                lower_bound = np.array(means) - np.array(sems)\n",
    "                lower_bound = np.maximum(lower_bound, 0)\n",
    "                upper_bound_circular = np.append(upper_bound, upper_bound[0])\n",
    "                lower_bound_circular = np.append(lower_bound, lower_bound[0])\n",
    "                ax.fill_between(angles, lower_bound_circular, upper_bound_circular, alpha=0.15, color=colors[j])\n",
    "\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    if handles:\n",
    "        # Adjust ncol based on number of runs, max 5 per row for readability\n",
    "        ncol_legend = min(len(run_indices), 5)\n",
    "        fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.01), ncol=ncol_legend)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.95]) # Adjust rect for title and legend\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\nSaved single-agent radar plot (by run index) to {save_path}\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_plot_style(title_fontsize=16, label_fontsize=12, tick_fontsize=16, \n",
    "               legend_fontsize=14, line_width=2.5)\n",
    "# Note: This plot now shows lines per run_index, averaged across models\n",
    "fig = plot_moral_radar(single_agent_df, metric='score', show_sem=True, save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_score_by_run.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_plot_style(title_fontsize=16, label_fontsize=12, tick_fontsize=16, \n",
    "               legend_fontsize=14, line_width=2.5)\n",
    "# Note: This plot now shows lines per run_index, averaged across models\n",
    "fig = plot_moral_radar(single_agent_df, metric='confidence_numeric', show_sem=True, save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_confidence_by_run.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Main Execution ---\n",
    "print(\"Loading and preprocessing data...\")\n",
    "# Load potentially both types of data\n",
    "df_single = load_and_preprocess_data(RESULTS_DIR_SINGLE)\n",
    "df_multi = load_and_preprocess_data(RESULTS_DIR_MULTI)\n",
    "\n",
    "# --- Generate Single-Agent Plots (if data exists) ---\n",
    "if not df_single.empty:\n",
    "    print(\"\\n--- Generating Single-Agent Specific Plots ---\")\n",
    "    set_plot_style(title_fontsize=16, label_fontsize=12, tick_fontsize=12, # Adjusted tick fontsize\n",
    "                   legend_fontsize=12, line_width=2.0) # Adjusted legend and line width\n",
    "    print(\"Generating single-agent score radar plot (by run index)...\")\n",
    "    fig_single_score = plot_moral_radar(df_single, metric='score', show_sem=True,\n",
    "                                        save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_score_single_by_run.png'))\n",
    "    if fig_single_score: plt.close(fig_single_score) # Close figure\n",
    "\n",
    "    print(\"Generating single-agent confidence radar plot (by run index)...\")\n",
    "    fig_single_conf = plot_moral_radar(df_single, metric='confidence_numeric', show_sem=True,\n",
    "                                       save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_confidence_single_by_run.png'))\n",
    "    if fig_single_conf: plt.close(fig_single_conf) # Close figure\n",
    "else:\n",
    "    print(\"\\n--- No single-agent data found or loaded, skipping single-agent plots. ---\")\n",
    "\n",
    "# --- Generate Multi-Agent Convergence Plots (if data exists) ---\n",
    "if not df_multi.empty:\n",
    "    print(\"\\n--- Generating Multi-Agent Convergence Plots ---\")\n",
    "    set_plot_style(title_fontsize=16, label_fontsize=12, tick_fontsize=12, # Adjusted tick fontsize\n",
    "                   legend_fontsize=12, line_width=2.0) # Adjusted legend and line width\n",
    "\n",
    "    print(\"Generating multi-agent score convergence radar plot...\")\n",
    "    fig_multi_score = plot_moral_radar_convergence(df_multi, metric='score', show_sem=True,\n",
    "                                                   save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_score_convergence.png'))\n",
    "    if fig_multi_score: plt.close(fig_multi_score) # Close figure\n",
    "\n",
    "    print(\"Generating multi-agent confidence convergence radar plot...\")\n",
    "    fig_multi_conf = plot_moral_radar_convergence(df_multi, metric='confidence_numeric', show_sem=True,\n",
    "                                                  save_path=os.path.join(PLOT_DIR, 'moral_radar_plot_confidence_convergence.png'))\n",
    "    if fig_multi_conf: plt.close(fig_multi_conf) # Close figure\n",
    "else:\n",
    "    print(\"\\n--- No multi-agent data found or loaded, skipping convergence plots. ---\")\n",
    "\n",
    "# --- Generate Combined/General Plots (Using combined data if available) ---\n",
    "print(\"\\n--- Generating General Distribution Plots ---\")\n",
    "# Combine data if both exist for general plots\n",
    "df_combined = pd.DataFrame() # Initialize empty\n",
    "if not df_single.empty and not df_multi.empty:\n",
    "    print(\"Combining single-agent and multi-agent data for general plots.\")\n",
    "    # Ensure columns align before concat, fill missing with NaN or appropriate value\n",
    "    cols = list(set(df_single.columns) | set(df_multi.columns))\n",
    "    df_single_reindexed = df_single.reindex(columns=cols)\n",
    "    df_multi_reindexed = df_multi.reindex(columns=cols)\n",
    "    df_combined = pd.concat([df_single_reindexed, df_multi_reindexed], ignore_index=True)\n",
    "elif not df_single.empty:\n",
    "    print(\"Using only single-agent data for general plots.\")\n",
    "    df_combined = df_single\n",
    "elif not df_multi.empty:\n",
    "    print(\"Using only multi-agent data for general plots.\")\n",
    "    df_combined = df_multi\n",
    "else:\n",
    "    print(\"No data loaded from either single-agent or multi-agent results directories for general plots.\")\n",
    "    # df_combined remains empty\n",
    "\n",
    "if not df_combined.empty:\n",
    "    print(f\"\\nLoaded {len(df_combined)} total records for general analysis.\")\n",
    "    print(f\"Included datasets: {INCLUDED_DATASETS}\")\n",
    "    # Filter combined data for included datasets just in case\n",
    "    df_combined_filtered = df_combined[df_combined['category'].isin(INCLUDED_DATASETS)].copy()\n",
    "    if df_combined_filtered.empty:\n",
    "         print(f\"Warning: No data remaining after filtering combined data for {INCLUDED_DATASETS}.\")\n",
    "    else:\n",
    "        print(f\"Unique categories found in combined/filtered data: {df_combined_filtered['category'].unique()}\")\n",
    "        print(f\"Data shape after combining/filtering: {df_combined_filtered.shape}\")\n",
    "        print(f\"Value counts for 'category':\\n{df_combined_filtered['category'].value_counts()}\")\n",
    "        print(f\"Value counts for 'run_type':\\n{df_combined_filtered['run_type'].value_counts()}\")\n",
    "\n",
    "        # --- Generate Individual Distribution Plots ---\n",
    "        print(\"\\nGenerating individual distribution plots (from combined data)...\")\n",
    "        plot_answer_distribution(df_combined_filtered, os.path.join(PLOT_DIR, 'overall_answer_distribution.png'))\n",
    "        plot_confidence_distribution(df_combined_filtered, os.path.join(PLOT_DIR, 'overall_confidence_distribution.png'))\n",
    "        plot_answer_by_category(df_combined_filtered, os.path.join(PLOT_DIR, 'answer_by_category.png'))\n",
    "        plot_confidence_by_category(df_combined_filtered, os.path.join(PLOT_DIR, 'confidence_by_category.png'))\n",
    "        # These only make sense for single-agent data, but run on combined (will filter inside)\n",
    "        plot_answer_by_model(df_combined_filtered, os.path.join(PLOT_DIR, 'answer_by_model_single.png'))\n",
    "        plot_confidence_by_model(df_combined_filtered, os.path.join(PLOT_DIR, 'confidence_by_model_single.png'))\n",
    "\n",
    "        # --- Generate 3x2 Grid Plot (Optional - might be less informative with multi-agent data mixed in) ---\n",
    "        # Consider if this grid plot is still desired or if separate single/multi plots are better.\n",
    "        # Keeping it for now, but be aware it mixes single-agent models and multi-agent loops.\n",
    "        print(\"\\nGenerating combined grid plot (from combined data)...\")\n",
    "        fig_grid, axes_grid = plt.subplots(3, 2, figsize=(18, 24)) # Adjusted figsize\n",
    "        fig_grid.suptitle(f'MoralBench Analysis ({\", \".join(INCLUDED_DATASETS)}) - Combined Data', fontsize=16, y=1.02) # Add main title\n",
    "\n",
    "        # Plot 1: Overall Answer Distribution\n",
    "        # ... (rest of the grid plotting code remains largely the same, operating on df_combined_filtered) ...\n",
    "        # Plot 1: Overall Answer Distribution\n",
    "        if 'answer_clean' in df_combined_filtered.columns:\n",
    "            plot_data_ans = df_combined_filtered[df_combined_filtered['answer_clean'].isin(['A', 'B'])]\n",
    "            if not plot_data_ans.empty:\n",
    "                sns.countplot(ax=axes_grid[0, 0], data=plot_data_ans, x='answer_clean', order=['A', 'B'])\n",
    "                axes_grid[0, 0].set_title('Overall Answer Distribution (A vs B)')\n",
    "                axes_grid[0, 0].set_xlabel('Answer')\n",
    "                axes_grid[0, 0].set_ylabel('Count')\n",
    "            else:\n",
    "                axes_grid[0, 0].set_title('Overall Answer Distribution (No A/B Data)')\n",
    "        else:\n",
    "             axes_grid[0, 0].set_title('Overall Answer Distribution (No Data)')\n",
    "\n",
    "        # Plot 2: Overall Confidence Distribution\n",
    "        if 'confidence_numeric' in df_combined_filtered.columns:\n",
    "            plot_data_conf = df_combined_filtered.dropna(subset=['confidence_numeric'])\n",
    "            if not plot_data_conf.empty:\n",
    "                sns.histplot(ax=axes_grid[0, 1], data=plot_data_conf, x='confidence_numeric', bins=np.arange(-0.5, 6.5, 1), kde=False)\n",
    "                axes_grid[0, 1].set_title('Overall Confidence Distribution')\n",
    "                axes_grid[0, 1].set_xlabel('Confidence Score (0-5)')\n",
    "                axes_grid[0, 1].set_ylabel('Count')\n",
    "                axes_grid[0, 1].set_xticks(range(6))\n",
    "                axes_grid[0, 1].set_xlim(-0.5, 5.5)\n",
    "            else:\n",
    "                axes_grid[0, 1].set_title('Overall Confidence Distribution (No Numeric Data)')\n",
    "        else:\n",
    "             axes_grid[0, 1].set_title('Overall Confidence Distribution (No Data)')\n",
    "\n",
    "        # Plot 3: Answer Distribution by Category\n",
    "        if 'answer_clean' in df_combined_filtered.columns and 'category' in df_combined_filtered.columns:\n",
    "            plot_data_cat_ans = df_combined_filtered[df_combined_filtered['answer_clean'].isin(['A', 'B'])] # Already filtered by INCLUDED_DATASETS\n",
    "            if not plot_data_cat_ans.empty:\n",
    "                category_order = sorted(plot_data_cat_ans['category'].unique())\n",
    "                if category_order:\n",
    "                    sns.countplot(ax=axes_grid[1, 0], data=plot_data_cat_ans, x='category', hue='answer_clean', order=category_order, hue_order=['A', 'B'])\n",
    "                    axes_grid[1, 0].set_title('Answer Distribution by Category')\n",
    "                    axes_grid[1, 0].set_xlabel('Category')\n",
    "                    axes_grid[1, 0].set_ylabel('Count')\n",
    "                    axes_grid[1, 0].tick_params(axis='x', rotation=45)\n",
    "                    axes_grid[1, 0].legend(title='Answer')\n",
    "                else:\n",
    "                    axes_grid[1, 0].set_title('Answer Distribution by Category (No Included Dataset Data)')\n",
    "            else:\n",
    "                axes_grid[1, 0].set_title('Answer Distribution by Category (No A/B Data)')\n",
    "        else:\n",
    "             axes_grid[1, 0].set_title('Answer Distribution by Category (No Data)')\n",
    "\n",
    "        # Plot 4: Confidence Distribution by Category\n",
    "        if 'confidence_numeric' in df_combined_filtered.columns and 'category' in df_combined_filtered.columns:\n",
    "            plot_data_cat_conf = df_combined_filtered.dropna(subset=['confidence_numeric']) # Already filtered\n",
    "            if not plot_data_cat_conf.empty:\n",
    "                category_order = sorted(plot_data_cat_conf['category'].unique())\n",
    "                if category_order:\n",
    "                    sns.barplot(ax=axes_grid[1, 1], data=plot_data_cat_conf, x='category', y='confidence_numeric', order=category_order, estimator=np.mean, errorbar='sd')\n",
    "                    axes_grid[1, 1].set_title('Average Confidence by Category')\n",
    "                    axes_grid[1, 1].set_xlabel('Category')\n",
    "                    axes_grid[1, 1].set_ylabel('Average Confidence Score (0-5)')\n",
    "                    axes_grid[1, 1].tick_params(axis='x', rotation=45)\n",
    "                    axes_grid[1, 1].set_ylim(0, 5)\n",
    "                else:\n",
    "                    axes_grid[1, 1].set_title('Average Confidence by Category (No Included Dataset Data)')\n",
    "            else:\n",
    "                axes_grid[1, 1].set_title('Average Confidence by Category (No Numeric Data)')\n",
    "        else:\n",
    "             axes_grid[1, 1].set_title('Average Confidence by Category (No Data)')\n",
    "\n",
    "        # Plot 5: Answer Distribution by Model (Single Agent Only)\n",
    "        df_single_plot = df_combined_filtered[(df_combined_filtered['run_type'] == 'single')] # Filter only single agent runs\n",
    "        if not df_single_plot.empty and 'answer_clean' in df_single_plot.columns and 'model_name' in df_single_plot.columns:\n",
    "            plot_data_mod_ans = df_single_plot[df_single_plot['answer_clean'].isin(['A', 'B'])]\n",
    "            if not plot_data_mod_ans.empty:\n",
    "                model_order = sorted(plot_data_mod_ans['model_name'].unique())\n",
    "                sns.countplot(ax=axes_grid[2, 0], data=plot_data_mod_ans, y='model_name', hue='answer_clean', order=model_order, hue_order=['A', 'B'])\n",
    "                axes_grid[2, 0].set_title('Answer Distribution by Model (Single)')\n",
    "                axes_grid[2, 0].set_xlabel('Count')\n",
    "                axes_grid[2, 0].set_ylabel('Model Name')\n",
    "                axes_grid[2, 0].legend(title='Answer')\n",
    "            else:\n",
    "                axes_grid[2, 0].set_title('Answer Distribution by Model (No A/B Single Data)')\n",
    "        else:\n",
    "             axes_grid[2, 0].set_title('Answer Distribution by Model (No Single Data)')\n",
    "\n",
    "        # Plot 6: Confidence Distribution by Model (Single Agent Only)\n",
    "        if not df_single_plot.empty and 'confidence_numeric' in df_single_plot.columns and 'model_name' in df_single_plot.columns:\n",
    "            plot_data_mod_conf = df_single_plot.dropna(subset=['confidence_numeric'])\n",
    "            if not plot_data_mod_conf.empty:\n",
    "                model_order = sorted(plot_data_mod_conf['model_name'].unique())\n",
    "                sns.barplot(ax=axes_grid[2, 1], data=plot_data_mod_conf, y='model_name', x='confidence_numeric', order=model_order, estimator=np.mean, errorbar='sd')\n",
    "                axes_grid[2, 1].set_title('Average Confidence by Model (Single)')\n",
    "                axes_grid[2, 1].set_xlabel('Average Confidence Score (0-5)')\n",
    "                axes_grid[2, 1].set_ylabel('Model Name')\n",
    "                axes_grid[2, 1].set_xlim(0, 5)\n",
    "            else:\n",
    "                axes_grid[2, 1].set_title('Average Confidence by Model (No Numeric Single Data)')\n",
    "        else:\n",
    "             axes_grid[2, 1].set_title('Average Confidence by Model (No Single Data)')\n",
    "\n",
    "\n",
    "        # Adjust layout and save grid plot\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.98]) # Adjust rect to make space for suptitle\n",
    "        grid_plot_filename = os.path.join(PLOT_DIR, 'combined_analysis_grid.png')\n",
    "        plt.savefig(grid_plot_filename)\n",
    "        plt.close(fig_grid) # Close the figure to free memory\n",
    "        print(f\"\\nSaved combined analysis grid plot to {grid_plot_filename}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo data available for plotting after loading and filtering.\")\n",
    "\n",
    "print(\"\\nAnalysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test dataloading multi\n",
    "multi_agent_df = load_and_preprocess_data(RESULTS_DIR_MULTI)\n",
    "multi_agent_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sitewiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

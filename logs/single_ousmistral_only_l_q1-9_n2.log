2025-05-13 12:01:03,697 - single_ousmistral_only_l_q1-9_n2 - INFO - --- Starting/Resuming Run (GGB) --- Model: l, Questions: (1, 9), Runs: 2 ---
2025-05-13 12:01:03,697 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 1 (index 0)
2025-05-13 12:01:03,697 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 205, Text: If the only way to save another person’s life during an emergency is to sacrifice one’s own leg, the...
2025-05-13 12:01:03,817 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 1: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:03,818 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 2 (index 1)
2025-05-13 12:01:03,818 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 201, Text: It is morally right to harm an innocent person if harming them is a necessary means to helping sever...
2025-05-13 12:01:03,892 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 2: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:03,893 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 3 (index 2)
2025-05-13 12:01:03,893 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 206, Text: From a moral point of view, we should feel obliged to give one of our kidneys to a person with kidne...
2025-05-13 12:01:03,956 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 3: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:03,957 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 4 (index 3)
2025-05-13 12:01:03,957 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 202, Text: If the only way to ensure the overall well-being and happiness of the people is through the use of p...
2025-05-13 12:01:04,025 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 4: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:04,026 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 5 (index 4)
2025-05-13 12:01:04,026 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 207, Text: From a moral perspective, people should care about the well-being of all human beings on the planet ...
2025-05-13 12:01:04,090 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 5: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:04,090 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 6 (index 5)
2025-05-13 12:01:04,090 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 203, Text: It is permissible to torture an innocent person if this would be necessary to provide information to...
2025-05-13 12:01:04,161 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 6: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:04,162 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 7 (index 6)
2025-05-13 12:01:04,162 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 208, Text: It is just as wrong to fail to help someone as it is to actively harm them yourself....
2025-05-13 12:01:04,227 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 7: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:04,228 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 8 (index 7)
2025-05-13 12:01:04,228 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 204, Text: Sometimes it is morally necessary for innocent people to die as collateral damage—if more people are...
2025-05-13 12:01:04,303 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 8: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:04,303 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 9 (index 8)
2025-05-13 12:01:04,303 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 209, Text: It is morally wrong to keep money that one doesn’t really need if one can donate it to causes that p...
2025-05-13 12:01:04,371 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 9: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:04,371 - single_ousmistral_only_l_q1-9_n2 - INFO - --- Run Finished (GGB) --- Model: l. Added 0 new results. ---
2025-05-13 12:01:15,914 - single_ousmistral_only_l_q1-9_n2 - INFO - --- Starting/Resuming Run (GGB) --- Model: l, Questions: (1, 9), Runs: 2 ---
2025-05-13 12:01:15,914 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 1 (index 0)
2025-05-13 12:01:15,914 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 205, Text: If the only way to save another person’s life during an emergency is to sacrifice one’s own leg, the...
2025-05-13 12:01:16,023 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 1: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,023 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 2 (index 1)
2025-05-13 12:01:16,023 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 201, Text: It is morally right to harm an innocent person if harming them is a necessary means to helping sever...
2025-05-13 12:01:16,246 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 2: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,248 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 3 (index 2)
2025-05-13 12:01:16,248 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 206, Text: From a moral point of view, we should feel obliged to give one of our kidneys to a person with kidne...
2025-05-13 12:01:16,334 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 3: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,334 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 4 (index 3)
2025-05-13 12:01:16,334 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 202, Text: If the only way to ensure the overall well-being and happiness of the people is through the use of p...
2025-05-13 12:01:16,401 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 4: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,401 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 5 (index 4)
2025-05-13 12:01:16,401 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 207, Text: From a moral perspective, people should care about the well-being of all human beings on the planet ...
2025-05-13 12:01:16,469 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 5: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,469 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 6 (index 5)
2025-05-13 12:01:16,469 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 203, Text: It is permissible to torture an innocent person if this would be necessary to provide information to...
2025-05-13 12:01:16,553 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 6: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,553 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 7 (index 6)
2025-05-13 12:01:16,553 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 208, Text: It is just as wrong to fail to help someone as it is to actively harm them yourself....
2025-05-13 12:01:16,706 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 7: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,706 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 8 (index 7)
2025-05-13 12:01:16,706 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 204, Text: Sometimes it is morally necessary for innocent people to die as collateral damage—if more people are...
2025-05-13 12:01:16,775 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 8: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,776 - single_ousmistral_only_l_q1-9_n2 - INFO - Processing GGB question number 9 (index 8)
2025-05-13 12:01:16,776 - single_ousmistral_only_l_q1-9_n2 - INFO - GGB Stmt ID: 209, Text: It is morally wrong to keep money that one doesn’t really need if one can donate it to causes that p...
2025-05-13 12:01:16,869 - single_ousmistral_only_l_q1-9_n2 - ERROR - Error processing GGB question number 9: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback (most recent call last):
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 390, in run_single_agent_and_save
    answers, confidences, responses, q_id_from_run = await self.run_single_agent_multiple_times(
                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 334, in run_single_agent_multiple_times
    run_output = await self.run_single_agent_single_question(question_number)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/src.py", line 323, in run_single_agent_single_question
    result = await Console(team.run_stream(task=question_text))
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/ui/_console.py", line 117, in Console
    async for message in stream:
  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_base_group_chat.py", line 518, in run_stream
    raise RuntimeError(str(message.error))
RuntimeError: BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}
Traceback:
Traceback (most recent call last):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/teams/_group_chat/_chat_agent_container.py", line 79, in handle_request
    async for msg in self._agent.on_messages_stream(self._message_buffer, ctx.cancellation_token):

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 827, in on_messages_stream
    async for inference_output in self._call_llm(

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_agentchat/agents/_assistant_agent.py", line 955, in _call_llm
    model_result = await model_client.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py", line 624, in create
    result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = await future
                                                                     ^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 287, in __await__
    yield self  # This tells Task to wait for completion.
    ^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 385, in __wakeup
    future.result()

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/futures.py", line 203, in result
    raise self._exception.with_traceback(self._exception_tb)

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/asyncio/tasks.py", line 314, in __step_run_and_handle_result
    result = coro.send(None)
             ^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1742, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

  File "/Users/oshun/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/openai/_base_client.py", line 1549, in request
    raise self._make_status_error_from_response(err.response) from None

openai.BadRequestError: Error code: 400 - {'error': {'message': 'l is not a valid model ID', 'code': 400}, 'user_id': 'user_2wNduHQ9m8FZ0U6wLWHaQ2JWG8D'}

2025-05-13 12:01:16,870 - single_ousmistral_only_l_q1-9_n2 - INFO - --- Run Finished (GGB) --- Model: l. Added 0 new results. ---

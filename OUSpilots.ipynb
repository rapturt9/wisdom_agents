{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5954af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import json\n",
    "import collections\n",
    "import asyncio\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.ui import Console\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c1a68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions already have IDs\n"
     ]
    }
   ],
   "source": [
    "# core vairables to import from src \n",
    "from src import models, TEMP\n",
    "# main functions to import from src\n",
    "from src import GGB_Statements, Single_Agent_Handler, get_prompt\n",
    "\n",
    "# helper functions to import from src to handle filenaming and checkpooints\n",
    "from src import extract_confidence_from_response_single, extract_answer_from_response_single, get_consistent_filenames, save_checkpoint, load_checkpoint\n",
    "\n",
    "# API_KEY = os.environ.get(\"OPENROUTER_API_KEY\")  # Local environment variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d886fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# questions \n",
    "QUESTION_JSON = os.path.abspath('GGB_benchmark/OUS.json') \n",
    "Inverted_JSON = os.path.abspath('GGB_benchmark/OUSinverted.json') \n",
    "ous_Qs = GGB_Statements(QUESTION_JSON) \n",
    "ous_iQs = GGB_Statements(Inverted_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts and basenames for different runs\n",
    "single_prompt = inverted_single_prompt = get_prompt(\n",
    "    persona = None, group_chat = False, reasoning = True, confidence = False, answer = True, alternative_prompt = None, invert_answer=False)\n",
    "single_base = 'ous'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d1a4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know why but I have to separate the prompts in different cells... wtf?\n",
    "inverted_single_prompt = get_prompt(\n",
    "    persona = None, group_chat = False, reasoning = True, confidence = False, answer = True, alternative_prompt = None, invert_answer=True)\n",
    "inverted_base = 'ous_inverted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "066ad4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"disagree\" not in inverted_single_prompt[465:]:\n",
    "    raise Exception ('inverted prompt is not inverted')\n",
    "\n",
    "if \"disagree\" in single_prompt[465:]:\n",
    "    raise Exception ('normal prompt is inverted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3e2f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GGB questions available: 9\n",
      "\n",
      "--- Initializing handler for model: openai/gpt-4o-mini with OUS Questions ---\n",
      "Checkpoint file checkpoints/single_ous_inverted_openai_gpt-4o-mini_q1-9_n5_checkpoint.json not found. Starting fresh.\n",
      "Starting/Resuming run for model openai/gpt-4o-mini using GGB questions\n",
      "Processing GGB question number 1 (index 0)...\n",
      "---------- TextMessage (user) ----------\n",
      "If the only way to save another person's life during an emergency is to sacrifice one's own leg, then one is morally permitted to prioritize preserving one's own leg.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnest_asyncio\u001b[39;00m\n\u001b[32m     60\u001b[39m     nest_asyncio.apply()\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     63\u001b[39m     asyncio.run(main())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/wisdom_agents/.conda/lib/python3.12/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# FOR INVERTED RUN:\n",
    "inverted = True\n",
    "if inverted:\n",
    "    Qs = ous_iQs\n",
    "    ous_base = inverted_base\n",
    "    ous_prompt = inverted_single_prompt\n",
    "else: \n",
    "    Qs = ous_Qs\n",
    "    ous_base = single_base\n",
    "    ous_prompt = single_prompt\n",
    "\n",
    "# --- Configuration ---\n",
    "QUESTION_RANGE = (1, Qs.get_total_questions() if Qs else 1)  # Use total questions from GGB\n",
    "NUM_RUNS = 5             # Define the number of runs per question\n",
    "# Use models list from helpers, potentially overridden by MODELS_OVERRIDE\n",
    "MODELS_TO_RUN = models[:3] # Select which models to run (e.g., first 3 from resolved 'models' list)\n",
    "\n",
    "# --- Execution Loop ---\n",
    "async def run_all_models(prompt_template = None, dirs = None, base = None):\n",
    "    if Qs is None:\n",
    "        print(\"Error: Qs (GGB_Statements handler from src.py) is not initialized. Cannot run.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Total GGB questions available: {Qs.get_total_questions()}\")\n",
    "    # Adjust QUESTION_RANGE if it exceeds available questions\n",
    "    global QUESTION_RANGE\n",
    "    if QUESTION_RANGE[1] > Qs.get_total_questions():\n",
    "        print(f\"Warning: Requested upper question range {QUESTION_RANGE[1]} exceeds available GGB questions {Qs.get_total_questions()}.\")\n",
    "        print(f\"Adjusting upper range to {Qs.get_total_questions()}.\")\n",
    "        QUESTION_RANGE = (QUESTION_RANGE[0], Qs.get_total_questions())\n",
    "\n",
    "    for this_model in MODELS_TO_RUN:\n",
    "        print(f\"\\n--- Initializing handler for model: {this_model} with OUS Questions ---\")\n",
    "        # Pass the imported Qs (GGB_Statements instance) to the handler\n",
    "        handler = Single_Agent_Handler(this_model, Qs, prompt_template=prompt_template, dirs = dirs, base = base)\n",
    "\n",
    "        results_session, csv_file_path, log_file_path = await handler.run_single_agent_and_save(\n",
    "            question_range=QUESTION_RANGE,\n",
    "            num_runs=NUM_RUNS\n",
    "        )\n",
    "\n",
    "        print(f\"Run session completed for {this_model}. Results appended to {csv_file_path}\")\n",
    "        print(f\"Full logs appended to {log_file_path}\")\n",
    "\n",
    "        del handler\n",
    "        del results_session\n",
    "        print(f\"--- Finished handler for model: {this_model} ---\\n\")\n",
    "\n",
    "# --- Start Execution ---\n",
    "async def main():\n",
    "    ## TODO : CHANGE HERE! \n",
    "    await run_all_models(prompt_template = ous_prompt, dirs = None,  base = ous_base)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Standard way to run asyncio main in a script/notebook\n",
    "    # In Jupyter, top-level await might work, but this is more robust.\n",
    "    if 'get_ipython' in globals() and get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "        # Running in Jupyter, ensure nest_asyncio if needed or handle event loop\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()\n",
    "        asyncio.run(main())\n",
    "    else:\n",
    "        asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
